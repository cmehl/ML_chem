{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning applied to 0D reactors\n",
    "\n",
    "In this notebook, we will train neural networks to replace the CVODE solver of CANTERA. We will use the databases generated in the *0D_database_generation.ipynb* notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import cantera as ct\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(\"notebook\")\n",
    "\n",
    "from chem_ai.cantera_runs import compute_nn_cantera_0D_homo\n",
    "from chem_ai.utils import get_molar_mass_atomic_matrix\n",
    "from chem_ai.utils import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the default pytorch precision to double. It slows down a little bit the training but it is the usual standard for CFD reacting flows applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We identify the device (CPU or GPU) available on the machine. This will be used by pytorch to identify the device on which to train and use the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda:0')\n",
    "  print('Running on the GPU')\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "  print('Running on the CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary\n",
    "\n",
    "### Loading the required data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the folder including the desired database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"./case_0D_highT\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the parameters stored in the json file of the dabatase folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(folder, \"dtb_params.json\"), \"r\") as file:\n",
    "    dtb_params = json.load(file)\n",
    "\n",
    "fuel = dtb_params[\"fuel\"]\n",
    "mech_file = dtb_params[\"mech_file\"]\n",
    "log_transform = dtb_params[\"log_transform\"]\n",
    "threshold = dtb_params[\"threshold\"]\n",
    "p = dtb_params[\"p\"]\n",
    "dt = dtb_params[\"dt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the scalers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xscaler = joblib.load(os.path.join(folder, \"processed_database\", \"Xscaler.pkl\"))\n",
    "Yscaler = joblib.load(os.path.join(folder, \"processed_database\", \"Yscaler.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the training and validation databases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(os.path.join(folder, \"processed_database\",\"X_train.csv\"))\n",
    "X_val = pd.read_csv(os.path.join(folder, \"processed_database\",\"X_val.csv\"))\n",
    "Y_train = pd.read_csv(os.path.join(folder, \"processed_database\",\"Y_train.csv\"))\n",
    "Y_val = pd.read_csv(os.path.join(folder, \"processed_database\",\"Y_val.csv\"))\n",
    "\n",
    "Xcols = X_train.columns\n",
    "Ycols = Y_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of input and output dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in = X_train.shape[1]\n",
    "n_out = Y_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elements conservation matrix\n",
    "\n",
    "In combustion, elements (usually C, H, O, N) are preserved when a mixture undergoes chemical reactions, as there are no nuclear reactions. Therefore, the initial mass of elements of a mixture is conserved at the next time step and so on. For a 0D reactors (no mixing), elements mass fractions are constant for a given simulation.\n",
    "\n",
    "For a given element $j \\in {C, H, O, N}$, the mass fraction of this elements can be expressed as:\n",
    "\n",
    "$$\n",
    "Y_e^j = \\sum_{k=1}^{N_S} \\frac{M_j}{M_k} n_k^j Y_k\n",
    "$$\n",
    "\n",
    "where $M_j$ and $M_k$ are the molar masses of element $j$ and species $k$ respectively. $n_k^j$ is the number of atoms $j$ in species $k$. This equation can also be written in matrix form:\n",
    "\n",
    "$$\n",
    "Y_e = \\mathcal{A} Y\n",
    "$$\n",
    "\n",
    "where $Y_e \\in \\mathbb{R}^4$ is the vector of elements mass fractions and $Y \\in \\mathbb{R}^{N_S}$ the vector of species mass fractions. The matrix $\\mathcal{A} \\in \\mathbb{R}^{4 \\times N_S}$ is defined be the following coefficients:\n",
    "\n",
    "$$\n",
    "\\mathcal{A}_{jk} = \\frac{M_j}{M_k} n_k^j\n",
    "$$\n",
    "\n",
    "The matrix $\\mathcal{A}$ can be computed using the function *get_molar_mass_atomic_matrix* given in *utils.py*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gas = ct.Solution(mech_file)\n",
    "A_element = get_molar_mass_atomic_matrix(gas.species_names, fuel, True)\n",
    "print(A_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix will be helpful to analyze the conservation of elements in the training loop and at inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data loaded (scalers, training/validation sets, etc...) are in numpy format. In order to use them in a Pytorch training loop, we need to convert them to *torch* tensors. Those tensors are very similar to numpy arrays, with similar functions.\n",
    "\n",
    "We first transform training and validation datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train.values, dtype=torch.float64)\n",
    "Y_train = torch.tensor(Y_train.values, dtype=torch.float64)\n",
    "X_val = torch.tensor(X_val.values, dtype=torch.float64)\n",
    "Y_val = torch.tensor(Y_val.values, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to deal with the scaler, we decide to extract the mean and standard deviation and write the formula directly when necessary. These quantities are here converted to torch tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xscaler_mean = torch.from_numpy(Xscaler.mean.values)\n",
    "Xscaler_std = torch.from_numpy(Xscaler.std.values)\n",
    "\n",
    "Yscaler_mean = torch.from_numpy(Yscaler.mean.values)\n",
    "Yscaler_std = torch.from_numpy(Yscaler.std.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conservation matrix $A$ also needs to be converted as it will be used during the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_element = torch.tensor(A_element, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another aspect is that the data needs to be on the correct device, as pytorch will look for the data on it. As CPU and GPU memory is not shared, we will have to manually move the data to the GPU if necessary. We do it here for training/validation data, scalers and conservation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to(device)\n",
    "Y_train = Y_train.to(device)\n",
    "X_val = X_val.to(device)\n",
    "Y_val = Y_val.to(device)\n",
    "\n",
    "Xscaler_mean = Xscaler_mean.to(device)\n",
    "Xscaler_std = Xscaler_std.to(device)\n",
    "\n",
    "Yscaler_mean = Yscaler_mean.to(device)\n",
    "Yscaler_std = Yscaler_std.to(device)\n",
    "\n",
    "A_element = A_element.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now can generate the model. In this work, we will consider a simple Multi Layer Perceptron (MLP). We generate the model using Pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChemNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(n_in, 60)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.hidden2 = nn.Linear(60, 60)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.output = nn.Linear(60, n_out)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.hidden1(x))\n",
    "        x = self.act2(self.hidden2(x))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is then instantiated and transferred to the GPU if present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChemNN()\n",
    "print(model)\n",
    "\n",
    "#Put model on GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define hyperparameters of the training loop. The following choices need to be made:\n",
    "\n",
    "+ **n_epochs**: number of passes of entire training dataset through the algorithm.\n",
    "+ **batch_size**: size of the chunks passed to the algorithm at each parameters update.\n",
    "+ **loss_fn**: loss function. In this we choose the Mean Square Loss (MSE), which is adapted to the regression problem. Assuming that the output of the ANN is $Y_k^n$ (preprocessed mass fractions) and the true value is $Y_k^{n,*}$, the loss reads:\n",
    "\n",
    "$$ \n",
    "\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{N_S} \\sum_{k=1}^{N_S} \\left( Y_{k,i}^n - Y_{k,i}^{n,*} \\right)^2\n",
    "$$\n",
    "\n",
    "where $N$ is the number of data points.\n",
    "\n",
    "+ **optimizer**: optimization method. We use here the standard Adam method with initial learning rate $lr$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 300\n",
    "batch_size = 256\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform now the main model training loop. In Pytorch the training loop needs to be written but offers flexibility in the way we can compute training monitoring quantities. In this loop, we decide to monitor conservation metrics, i.e. the sum of species mass fractions and the elements mass fractions variation.\n",
    "\n",
    "*Exercice 1:* Complete the training loop by computing monitoring metrics. Note that these metrics are computed every 10 epochs in order to limit the computational overhead.\n",
    "\n",
    "1. Compute the validation loss.\n",
    "\n",
    "2. From the conservation of mass we have $\\sum_{k=1}^{N_S} Y_k = 1$. Compute the mean, min and max of $\\sum_{k=1}^{N_S} Y_k$ over the entire validation dataset. \n",
    "\n",
    "3. Let us note $Y^{in}$ the mass fraction of species at the input of the ANN and $Y^{out}$ at the output (same for elemental mass fractions which are written $Y_e^{in}$ and $Y_e^{out}$). Elements conservation imposes $Y_e^{in}=Y_e^{out}$ which means $\\mathcal{A}Y^{in}=\\mathcal{A}Y^{out}$. We will consider here the quantity $\\delta Y_e = \\left( \\mathcal{A}Y^{out} - \\mathcal{A}Y^{in} \\right) / \\mathcal{A}Y^{in} \\in \\mathbb{R}^4$. Compute the mean, min and max of this quantity over the entire validation dataset.\n",
    "\n",
    "We give the following hints:\n",
    "\n",
    "+ Do not forget that input of output data of the ANN is preprocessed. For the scaling, you can use directly the mean and std values and the formula, as these vectors have been put to the GPU to that purpose.\n",
    "\n",
    "+ Metric arrays are defined as numpy arrays on CPU to be later plotted on matplotlib. In order to transfer a GPU torch tensor *tensor_torch* to a CPU numpy array you can use: *tensor_torch.detach().cpu().numpy()*.\n",
    "\n",
    "+ Matrix multiplication in pytorch can be done using the *torch.matmul* function and transpose by using *torch.transpose*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_training_loop(X_train, X_val, Y_train, Y_val, loss_fn, optimizer, n_epochs, model, log_transform, need_ini_vals):\n",
    "\n",
    "    # Array to store the loss and validation loss\n",
    "    loss_list = np.empty(n_epochs)\n",
    "    val_loss_list = np.empty(n_epochs//10)\n",
    "\n",
    "    # Array to store sum of mass fractions: mean, min and max\n",
    "    stats_sum_yk = np.empty((n_epochs//10,3))\n",
    "\n",
    "    # Array to store elements conservation: mean, min and max\n",
    "    stats_A_elements = np.empty((n_epochs//10,4,3))\n",
    "\n",
    "    epochs = np.arange(n_epochs)\n",
    "    epochs_small = epochs[::10]\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # Training parameters\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "\n",
    "            Xbatch = X_train[i:i+batch_size]\n",
    "            y_pred = model(Xbatch)\n",
    "            ybatch = Y_train[i:i+batch_size]\n",
    "            if need_ini_vals: #used for soft elements constraint later\n",
    "                loss = loss_fn(y_pred, ybatch, Xbatch[:,1:])\n",
    "            else:\n",
    "                loss = loss_fn(y_pred, ybatch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        loss_list[epoch] = loss\n",
    "\n",
    "        # Computing validation loss and mass conservation metric (only every 10 epochs as it is expensive)\n",
    "        if epoch%10==0:\n",
    "            model.eval()  # evaluation mode\n",
    "            with torch.no_grad():\n",
    "\n",
    "                # VALIDATION LOSS\n",
    "                y_val_pred = model(X_val)\n",
    "                if need_ini_vals:\n",
    "                    val_loss = loss_fn(y_val_pred, Y_val, X_val[:,1:])\n",
    "                else:\n",
    "                    val_loss = loss_fn(y_val_pred, Y_val)\n",
    "\n",
    "                # SUM OF MASS FRACTION\n",
    "                #Inverse scale done by hand to stay with Torch arrays\n",
    "                yk = Yscaler_mean + (Yscaler_std + 1e-7)*y_val_pred\n",
    "                if log_transform:\n",
    "                    yk = torch.exp(yk)\n",
    "                sum_yk = yk.sum(axis=1)\n",
    "                sum_yk = sum_yk.detach().cpu().numpy()\n",
    "                stats_sum_yk[epoch//10,0] = sum_yk.mean() \n",
    "                stats_sum_yk[epoch//10,1] = sum_yk.min()\n",
    "                stats_sum_yk[epoch//10,2] = sum_yk.max()\n",
    "\n",
    "                # ELEMENTS CONSERVATION\n",
    "                yval_in = Xscaler_mean[1:] + (Xscaler_std[1:] + 1e-7)*X_val[:,1:]\n",
    "                if log_transform:\n",
    "                    yval_in = torch.exp(yval_in)\n",
    "                ye_in = torch.matmul(A_element, torch.transpose(yval_in, 0, 1))\n",
    "                ye_out = torch.matmul(A_element, torch.transpose(yk, 0, 1))\n",
    "                delta_ye = (ye_out - ye_in)/(ye_in+1e-10)\n",
    "                delta_ye = delta_ye.detach().cpu().numpy()\n",
    "                stats_A_elements[epoch//10, :, 0] = delta_ye.mean(axis=1)\n",
    "                stats_A_elements[epoch//10, :, 1] = delta_ye.min(axis=1)\n",
    "                stats_A_elements[epoch//10, :, 2] = delta_ye.max(axis=1)\n",
    "\n",
    "            model.train()   # Back to training mode\n",
    "            val_loss_list[epoch//10] = val_loss\n",
    "\n",
    "        print(f\"Finished epoch {epoch}\")\n",
    "        print(f\"    >> Loss: {loss}\")\n",
    "        if epoch%10==0:\n",
    "            print(f\"    >> Validation loss: {val_loss}\")\n",
    "\n",
    "    return epochs, epochs_small, loss_list, val_loss_list, stats_sum_yk, stats_A_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs, epochs_small, loss_list, val_loss_list, stats_sum_yk, stats_A_elements = main_training_loop(X_train, X_val, Y_train, Y_val, loss_fn, optimizer, n_epochs, model, log_transform, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a function to analyze the training. We plot:\n",
    "\n",
    "+ The training and validation losses\n",
    "\n",
    "+ The evolution of $\\sum_{k=1}^{N_S} Y_k$ (mean, min and max).\n",
    "\n",
    "+ The elements conservation by plotting $100\\times\\delta Y_e$ for each element (C, H, O and N). The factor $100$ enables to get an error in \\%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses_conservation(epochs, epochs_small, loss_list, val_loss_list, stats_sum_yk, stats_A_elements):\n",
    "\n",
    "    # LOSSES\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(epochs, loss_list, color=\"k\", label=\"Training\")\n",
    "    ax.plot(epochs_small, val_loss_list, color=\"r\", label = \"Validation\")\n",
    "\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "\n",
    "    # MASS CONSERVATION\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(epochs_small, stats_sum_yk[:,0], color=\"k\")\n",
    "    ax.plot(epochs_small, stats_sum_yk[:,1], color=\"k\", ls=\"--\")\n",
    "    ax.plot(epochs_small, stats_sum_yk[:,2], color=\"k\", ls=\"--\")\n",
    "\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(r\"$\\sum_k \\ Y_k$\")\n",
    "\n",
    "    # ELEMENTS CONSERVATION\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2)\n",
    "\n",
    "    # C\n",
    "    ax1.plot(epochs_small, 100*stats_A_elements[:,0,0], color=\"k\")\n",
    "    ax1.plot(epochs_small, 100*stats_A_elements[:,0,1], color=\"k\", ls=\"--\")\n",
    "    ax1.plot(epochs_small, 100*stats_A_elements[:,0,2], color=\"k\", ls=\"--\")\n",
    "\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(r\"$\\Delta Y_C$ $(\\%$)\")\n",
    "\n",
    "    # H\n",
    "    ax2.plot(epochs_small, 100*stats_A_elements[:,1,0], color=\"k\")\n",
    "    ax2.plot(epochs_small, 100*stats_A_elements[:,1,1], color=\"k\", ls=\"--\")\n",
    "    ax2.plot(epochs_small, 100*stats_A_elements[:,1,2], color=\"k\", ls=\"--\")\n",
    "\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(r\"$\\Delta Y_H$ $(\\%)$\")\n",
    "\n",
    "    # O\n",
    "    ax3.plot(epochs_small, 100*stats_A_elements[:,2,0], color=\"k\")\n",
    "    ax3.plot(epochs_small, 100*stats_A_elements[:,2,1], color=\"k\", ls=\"--\")\n",
    "    ax3.plot(epochs_small, 100*stats_A_elements[:,2,2], color=\"k\", ls=\"--\")\n",
    "\n",
    "    ax3.set_xlabel(\"Epoch\")\n",
    "    ax3.set_ylabel(r\"$\\Delta Y_O$ $(\\%)$\")\n",
    "\n",
    "    # N\n",
    "    ax4.plot(epochs_small, 100*stats_A_elements[:,3,0], color=\"k\")\n",
    "    ax4.plot(epochs_small, 100*stats_A_elements[:,3,1], color=\"k\", ls=\"--\")\n",
    "    ax4.plot(epochs_small, 100*stats_A_elements[:,3,2], color=\"k\", ls=\"--\")\n",
    "\n",
    "    ax4.set_xlabel(\"Epoch\")\n",
    "    ax4.set_ylabel(r\"$\\Delta Y_N$ $(\\%)$\")\n",
    "\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses_conservation(epochs, epochs_small, loss_list, val_loss_list, stats_sum_yk, stats_A_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the Pytorch model in the case folder for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(folder,\"pytorch_mlp.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN model test\n",
    "\n",
    "Now that we have generated the model we would like to test it on unseen data. For this, we will use the test initial conditions which were stored during data generation. The methodology is as follows:\n",
    "\n",
    "1. We get the test conditions and simulate CANTERA flames with (i) the CVODE solver and (ii) the generated ANN.\n",
    "2. We define appropriate metric and assess the accuracy of the ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing simulations with CANTERA and NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the test initial conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim_test = pd.read_csv(os.path.join(folder, \"sim_test.csv\"))\n",
    "\n",
    "n_sim = df_sim_test.shape[0]\n",
    "print(f\"There are {n_sim} test simulations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to compute 0D reactors for each conditions in *df_sim_test*. We use here the function *compute_nn_cantera_0D_homo* which is given. It takes as input initial conditions $T_0, \\phi$, the ANN model (with associated scalers), the time step (used for the ANN) and the database parameters. It outputs two dataframes containing the exact simulation and the ANN simulation. These results are concatenated in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_test_results = []\n",
    "\n",
    "for i, row in df_sim_test.iterrows():\n",
    "\n",
    "    phi_ini = row['Phi']\n",
    "    temperature_ini = row['T0']\n",
    "\n",
    "    print(f\"Performing test computation for phi={phi_ini}; T0={temperature_ini}\")\n",
    "\n",
    "    df_exact, df_nn = compute_nn_cantera_0D_homo(device, model, Xscaler, Yscaler, phi_ini, temperature_ini, dt, dtb_params)\n",
    "\n",
    "    list_test_results.append((df_exact, df_nn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are what dataframes look like for a given simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_sim = 1\n",
    "list_test_results[i_sim][0].head()   # Exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_test_results[i_sim][1].head()   # ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the results for a given simulation *i_sim*. We can look at temperature and species mass fractions for instance. We can also analyze conservation metrics. If we learned in logarithmic space, it is interesting also to plot in log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results_sim(i_sim, list_test_results, spec_to_plot):\n",
    "\n",
    "    df_exact = list_test_results[i_sim][0]\n",
    "    df_nn = list_test_results[i_sim][1]\n",
    "\n",
    "    # Temperature \n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(df_exact['Time'], df_exact['Temperature'], color='k')\n",
    "    ax.plot(df_nn['Time'], df_nn['Temperature'], color='b')\n",
    "    ax.set_xlabel(\"Time [s]\")\n",
    "    ax.set_ylabel(\"T [K]\")\n",
    "\n",
    "    # Species (normal)\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(df_exact['Time'], df_exact[spec_to_plot], color='k')\n",
    "    ax.plot(df_nn['Time'], df_nn[spec_to_plot], color='b')\n",
    "    ax.set_xlabel(\"Time [s]\")\n",
    "    ax.set_ylabel(f\"{spec_to_plot} [-]\")\n",
    "\n",
    "    # Species (log)\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(df_exact['Time'], np.log(df_exact[spec_to_plot]), color='k')\n",
    "    ax.plot(df_nn['Time'], np.log(df_nn[spec_to_plot]), color='b')\n",
    "    ax.set_xlabel(\"Time [s]\")\n",
    "    ax.set_ylabel(f\"{spec_to_plot} [-]\")\n",
    "\n",
    "    # Sum of Yk\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(df_nn['Time'], df_nn['SumYk'], color='b')\n",
    "    ax.set_xlabel(\"Time [s]\")\n",
    "    ax.set_ylabel(\"$\\sum Y_k$ [-]\")\n",
    "\n",
    "    # Elements\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2)\n",
    "    ax1.plot(df_nn['Time'], df_nn['Y_C'], color='b')\n",
    "    ax2.plot(df_nn['Time'], df_nn['Y_H'], color='b')\n",
    "    ax3.plot(df_nn['Time'], df_nn['Y_O'], color='b')\n",
    "    ax4.plot(df_nn['Time'], df_nn['Y_N'], color='b')\n",
    "    ax1.set_ylabel(\"$Y_C$\")\n",
    "    ax2.set_ylabel(\"$Y_H$\")\n",
    "    ax3.set_ylabel(\"$Y_O$\")\n",
    "    ax4.set_ylabel(\"$Y_N$\")\n",
    "    ax3.set_xlabel(\"Time [s]\")\n",
    "    ax4.set_xlabel(\"Time [s]\")\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_sim = 40\n",
    "spec_to_plot = \"H2O2\"\n",
    "plot_results_sim(i_sim, list_test_results, spec_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the ANN run can also check the conservation of mass and elements:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing error statistics\n",
    "\n",
    "We need a metric to assess the accuracy of the ANN over the entire test simulations. To do that, we will define normalized fitness functions for each simulation, and average the values over the simulation. As previously, we note $Y_k^n$ the processed mass fractions (including potential log and scaling) from the ANN, and $Y_k^{n,*}$ the exact values. The error on species mass fractions is then for given initial conditions:\n",
    "\n",
    "$$\n",
    "\\mathcal{M}_k(T_0,\\phi) = \\frac{1}{N_{iter}} \\sum_{i=1}^{N_{iter}} \\left| \\frac{Y_k^n - Y_k^{n,*}}{Y_k^{n,*}} \\right|\n",
    "$$\n",
    "\n",
    "where $N_{iter}$ is the number of iterations of the considered simulation (it may vary from one simulation to another as it is controlled by a stopping criterion).\n",
    "\n",
    "Although it is not a direct output of the model, we can still compute error on the temperature. We write $T^n$ and $T^{n,*}$ the normalized predicted and exact temperatures, respestively (note that log is never applied on temeprature). The error is:\n",
    "\n",
    "$$\n",
    "\\mathcal{M}_T(T_0,\\phi) = \\frac{1}{N_{iter}} \\sum_{i=1}^{N_{iter}} \\left| \\frac{T^n - T^{n,*}}{T^{n,*}} \\right|\n",
    "$$\n",
    "\n",
    "Finally, we can define a global error for each simulation as the mean of all errors (they can be compared as everything is normalized):\n",
    "\n",
    "$$\n",
    "\\mathcal{M}(T_0,\\phi) = \\frac{\\mathcal{M}_T(T_0,\\phi) + \\sum_{k=1}^{N_S} \\mathcal{M}_k(T_0,\\phi)}{N_S+1}\n",
    "$$\n",
    "\n",
    "*Exercice 2:* we define a function *compute_fitness* to calculate these errors. Complete the calculation of the errors in this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fitness(df_exact, df_nn):\n",
    "\n",
    "    # Results will be stored in data_errors array.\n",
    "    # The first column corresponds to errors on temperature\n",
    "    # The next n_out columns correspond to errors on species mass fractions\n",
    "    # The last column corresponds to the mean error\n",
    "    data_errors = np.empty([n_sim, n_out+2]) \n",
    "\n",
    "    for i_sim in range(n_sim):\n",
    "\n",
    "        df_exact = list_test_results[i_sim][0]\n",
    "        df_nn = list_test_results[i_sim][1]\n",
    "\n",
    "        # Removing undesired variables\n",
    "        df_exact = df_exact.drop('Time', axis=1)\n",
    "        df_nn = df_nn.drop([\"Time\",\"SumYk\", \"Y_C\", \"Y_H\", \"Y_O\", \"Y_N\"], axis=1)\n",
    "\n",
    "        # Applying log\n",
    "        if log_transform:\n",
    "\n",
    "            df_exact[df_exact < threshold] = threshold\n",
    "            df_nn[df_nn < threshold] = threshold\n",
    "\n",
    "            df_exact.iloc[:, 1:] = np.log(df_exact.iloc[:, 1:])\n",
    "            df_nn.iloc[:, 1:] = np.log(df_nn.iloc[:, 1:])\n",
    "\n",
    "        # Scaling\n",
    "        data_exact_scaled = (df_exact.values-Xscaler.mean.values)/(Xscaler.std.values+1.0e-7)\n",
    "        data_nn_scaled = (df_nn.values-Xscaler.mean.values)/(Xscaler.std.values+1.0e-7)\n",
    "\n",
    "        diff_exact_nn = np.abs((data_nn_scaled-data_exact_scaled)/data_exact_scaled)\n",
    "\n",
    "        diff_exact_nn = diff_exact_nn.mean(axis=0)\n",
    "\n",
    "        M = diff_exact_nn.mean()\n",
    "\n",
    "        print(f\"Simulation {i_sim} error M = {M}\")\n",
    "\n",
    "        data_errors[i_sim, :n_out+1] = diff_exact_nn\n",
    "        data_errors[i_sim, n_out+1] = M\n",
    "\n",
    "\n",
    "    return data_errors\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_errors = compute_fitness(df_exact, df_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize errors, we can draw a boxplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting gas species for labels\n",
    "gas = ct.Solution(mech_file)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.boxplot(data_errors, ax=ax)\n",
    "\n",
    "custom_labels = [\"T\"] + gas.species_names + [\"Total\"]\n",
    "ax.set_xticklabels(custom_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercice 3*: Find the simulation with the highest error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_sim_max = data_errors[:,-1].argmax()\n",
    "print(f\"Simulation with largest error: {i_sim_max}\")\n",
    "print(f\"Error is: {data_errors[:,-1][i_sim_max]} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can average the errors of all simulations to get a global error which can be used to get an idea of the overall error of the ANN:\n",
    "\n",
    "$$\n",
    "\\mathcal{M}_{avg} = \\frac{1}{N_{test}} \\sum_{i=1}^{N_{test}} \\mathcal{M}(T_{0,i},\\phi_i)\n",
    "$$\n",
    "\n",
    "where $N_{test}$ is the number of test simulations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_vect = data_errors[:,-1]\n",
    "\n",
    "print(f\"Averaged on set of test simulations, error is M={M_vect.mean()} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results must be taken with caution, as the fitness function is not perfect. Amonsgt other potentiel issues we can note:\n",
    "\n",
    "+ If the profile are slightly shifted (for example if the ignition delay is mispredicted), it will lead to large errors. A solution could be to add the ignition delay in teh fitness and compute profiles errors in a progress variable space.\n",
    "+ When profiles are close to $0$, it can lead to large errors (for example if we predict $10^{-6}$ instead of $10^{-7}$ for a major species). This will artificially increase errors.\n",
    "\n",
    "Other fitness functions are possible, feel free to implement other ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enforcing physical information: soft constraints "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the strategy employed above, the ANN model freely predicts the new chemical states without being forced to satisfy any constraints. As already analyzed above, several constraints must be satisfied. First, mass fractions should sum up to $1$:\n",
    "\n",
    "$$\n",
    "\\sum_{k=1}^{N_S} Y_k = 1\n",
    "$$\n",
    "\n",
    "Additionally, elements must be conserved. This amounts to say that the elements mass fractions must be equal for the ANN inputs and outputs:\n",
    "\n",
    "$$\n",
    "Y_e^{in} = Y_e^{out}\n",
    "$$\n",
    "\n",
    "One way to lead the ANN towards verifying constraints is to add penalization terms in the loss functions. Those are then called **soft constraints**. In the present case, we will consider two loss functions. The first one tries to impose mass conservation:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{mass} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{N_S} \\sum_{k=1}^{N_S} \\left( Y_{k,i}^n - Y_{k,i}^{n,*} \\right)^2 + \\alpha_{mass} \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\sum_{k=1}^{N_S} Y_k - 1 \\right)^2\n",
    "$$\n",
    "\n",
    "where $\\alpha_{mass}$ is a hyperparameter to control the constraint weight in the loss.\n",
    "\n",
    "Another loss can be defined by adding a penalty term for elements conservation:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{elt} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{N_S} \\sum_{k=1}^{N_S} \\left( Y_{k,i}^n - Y_{k,i}^{n,*} \\right)^2 + \\alpha_{elt} \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{4} \\sum_{j=1}^{4} \\left( Y_{e,j}^{out} - Y_{e,j}^{in} \\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercice 4:* complete the loss function for mass conservation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sumYkLoss(nn.Module):\n",
    "    def __init__(self, alpha):\n",
    "        super(sumYkLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, pred, targets):\n",
    "\n",
    "        yk = Yscaler_mean + pred * (Yscaler_std+1.0e-7)\n",
    "        if log_transform:\n",
    "            yk = torch.exp(yk)\n",
    "        sum_yk = yk.sum(axis=1)\n",
    "\n",
    "        return torch.mean((pred - targets) ** 2) + self.alpha * torch.mean((sum_yk-1.0)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercice 5:* complete the loss function for elements conservation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElementLoss(nn.Module):\n",
    "    def __init__(self, alpha):\n",
    "        super(ElementLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, pred, targets, yk_scaled_in):\n",
    "\n",
    "        # Elements input\n",
    "        yk_in = Xscaler_mean[1:] + yk_scaled_in * (Xscaler_std[1:]+1.0e-7)\n",
    "        if log_transform:\n",
    "            yk_in = torch.exp(yk_in)\n",
    "        ye_in = torch.matmul(A_element, torch.transpose(yk_in, 0, 1))\n",
    "\n",
    "        # Elements output\n",
    "        yk = Yscaler_mean + pred * (Yscaler_std+1.0e-7)\n",
    "        if log_transform:\n",
    "            yk = torch.exp(yk)\n",
    "        ye_out = torch.matmul(A_element, torch.transpose(yk, 0, 1))\n",
    "\n",
    "        return torch.mean((pred - targets) ** 2) + self.alpha * torch.mean((ye_out - ye_in) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin from the model already trained without constraints. You can also choose to train a new model from scratch. We can redo the training loop and setting one of the new losses: (feel free to test both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "alpha = 100.0\n",
    "\n",
    "# loss_fn = sumYkLoss(alpha)\n",
    "# need_ini_val = False\n",
    "\n",
    "loss_fn = ElementLoss(alpha)\n",
    "need_ini_val = True\n",
    "\n",
    "epochs, epochs_small, loss_list, val_loss_list, stats_sum_yk, stats_A_elements = main_training_loop(X_train, X_val, Y_train, Y_val, loss_fn, optimizer, n_epochs, model, log_transform, need_ini_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(folder,\"pytorch_mlp_soft.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot again the metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses_conservation(epochs, epochs_small, loss_list, val_loss_list, stats_sum_yk, stats_A_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can recompute the test simulations using the new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_test_results = []\n",
    "\n",
    "for i, row in df_sim_test.iterrows():\n",
    "\n",
    "    phi_ini = row['Phi']\n",
    "    temperature_ini = row['T0']\n",
    "\n",
    "    print(f\"Performing test computation for phi={phi_ini}; T0={temperature_ini}\")\n",
    "\n",
    "    df_exact, df_nn = compute_nn_cantera_0D_homo(device, model, Xscaler, Yscaler, phi_ini, temperature_ini, dt, dtb_params)\n",
    "\n",
    "    list_test_results.append((df_exact, df_nn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can again plot results for a given simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_sim = 40\n",
    "spec_to_plot = \"H2O2\"\n",
    "plot_results_sim(i_sim, list_test_results, spec_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_errors_soft_cst = compute_fitness(df_exact, df_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting of the errors distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting gas species for labels\n",
    "gas = ct.Solution(mech_file)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.boxplot(data_errors_soft_cst, ax=ax)\n",
    "\n",
    "custom_labels = [\"T\"] + gas.species_names + [\"Total\"]\n",
    "ax.set_xticklabels(custom_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_vect = data_errors_soft_cst[:,-1]\n",
    "\n",
    "print(f\"Averaged on set of test simulations, error is M={M_vect.mean()} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_sim_max = data_errors_soft_cst[:,-1].argmax()\n",
    "print(f\"Simulation with largest error: {i_sim_max}\")\n",
    "print(f\"Error is: {data_errors_soft_cst[:,-1][i_sim_max]} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enforcing physical information: hard constraints "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that adding soft constraints is not sufficient to absolutely guarantee the conservation of mass and elements. An alternative way is to use hard constraints, where constraints are directly encoded in the neural network. In this case, conservation of elements is guaranteed at inference.\n",
    "\n",
    "**Remark:** the method that will be exposed here is not compatible yet with logarithm transform due to issues with back-propagation convergence; and also to the fact that the thresholding removes some mass. For this reason, we encourage you to re-run this notebook using a second database, without log transform. In order to get interesting results, it is then necessary to limit the ignition zone where log was necessary. To do so, you can increase the initial temperatures of the reactors. We can choose $T_0 \\in [2100, 2200]$ K for example.\n",
    "\n",
    "In this section, we will focus on elements conservation only, as it leads also to mass conservation. As discussed earlier we have:\n",
    "\n",
    "$$\n",
    "\\mathcal{A}Y^{in} = \\mathcal{A}Y^{out}\n",
    "$$\n",
    "\n",
    "The idea is to add a final layer (without any trainable parameter) which will perform a projection on a space were elements are conserved. We will note $Y'$ the vector before applying this layer. The idea followed here (not the only solution) is to balance the mass of elements using a pre-defined set of \"balancing\" species, containing at least of instance of each element. As we deal with combustion, we choose here: $CO_2$, $H_2O$, $O_2$ and $N_2$ (for hydrogen $CO_2$ is removed). The vector $Y'$ is corrected as follows:\n",
    "\n",
    "$$\n",
    "Y^{out} = Y' + \\epsilon\n",
    "$$\n",
    "\n",
    "where $\\epsilon$ is a correction term computed to balance elements. We have then:\n",
    "\n",
    "$$\n",
    "\\mathcal{A} \\epsilon = \\mathcal{A} \\left(Y^{in} -Y' \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "This system has $N_S$ unknowns (components of $\\epsilon$) but only 4 equations (or 3 for $H_2$) and is therefore indeterminated. We use here the selected balancing species, and define:\n",
    "\n",
    "$$\n",
    "\\epsilon' = \\left( \\epsilon_{CO_2}, \\epsilon_{H_2O}, \\epsilon_{O_2}, \\epsilon_{N_2} \\right)\n",
    "$$\n",
    "\n",
    "The new system is then:\n",
    "\n",
    "$$\n",
    "\\mathcal{A} \\epsilon' = \\mathcal{A}' \\left(Y^{in} -Y' \\right)\n",
    "$$\n",
    "\n",
    "where $\\mathcal{A}'$ is a sub-matrix of $\\mathcal{A}$ containing only the columns associated to balancing species. Finally we can solve the linear system:\n",
    "\n",
    "$$\n",
    "\\epsilon' = \\mathcal{A}'^{-1} \\mathcal{A} \\left(Y^{in} -Y' \\right)\n",
    "$$\n",
    "\n",
    "And we have: $Y^{out} = Y' + \\epsilon'$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will precompute the matrix $\\mathcal{A}'^{-1}$, based on the already computed $\\mathcal{A}$ matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_species = len(gas.species_names)\n",
    "\n",
    "# For H2, we need here to remove carbon\n",
    "if fuel==\"H2\":\n",
    "    A_element_final = A_element[1:,:]\n",
    "else:\n",
    "    A_element_final = A_element\n",
    "\n",
    "\n",
    "# We define balancing species (without CO2 if H2 is considered)\n",
    "if fuel==\"H2\":\n",
    "    balancing_species = [\"H2O\", \"O2\", \"N2\"]\n",
    "    mass_per_atom_array = np.array([1.008, 15.999, 14.007])\n",
    "else:\n",
    "    balancing_species = [\"CO2\", \"H2O\", \"O2\", \"N2\"]\n",
    "    mass_per_atom_array = np.array([12.011, 1.008, 15.999])\n",
    "\n",
    "# We transpose the matrix, as it will be needed in this form in the ann model (because X are of shape (n_samples, N_S))\n",
    "A_element_t = torch.transpose(A_element_final, 0, 1)\n",
    "        \n",
    "# We construct the matrix A' by selecting balancing species only\n",
    "A_reduced = A_element_final[:,[gas.species_names.index(spec) for spec in balancing_species]]\n",
    "        \n",
    "# We invert A'\n",
    "A_reduced_inv = torch.linalg.inv(A_reduced)\n",
    "    \n",
    "# We want to automatically add 0 for non-balancing species, we therefore add liens of 0 in the matrix\n",
    "A_inv_final = torch.zeros((nb_species, len(balancing_species)))\n",
    "for i, spec in enumerate(balancing_species):\n",
    "    A_inv_final[gas.species_names.index(spec),:] = A_reduced_inv[i,:]\n",
    "    \n",
    "# We will also need the transpose\n",
    "A_inv_final_t = torch.transpose(A_inv_final, 0, 1)\n",
    "\n",
    "# Sending to GPU\n",
    "A_element_t = A_element_t.to(device)\n",
    "A_inv_final_t = A_inv_final_t.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a new Pytorch model.\n",
    "\n",
    "*Exercice 6:* Complete the *forward* method of the newly defined model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChemNN_Element(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(n_in, 60)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.hidden2 = nn.Linear(60, 60)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.output = nn.Linear(60, n_out)\n",
    " \n",
    "    def forward(self, input):\n",
    "        \n",
    "        x = self.act1(self.hidden1(input))\n",
    "        x = self.act2(self.hidden2(x))\n",
    "        x = self.output(x)\n",
    "\n",
    "        # Unscale previous layer\n",
    "        x_unscaled = Yscaler_mean + (Yscaler_std + 1e-7)*x\n",
    "        if log_transform:\n",
    "            x_unscaled = torch.exp(x_unscaled)\n",
    "\n",
    "        # Unscale input\n",
    "        input_unscaled = Xscaler_mean[1:] + (Xscaler_std[1:] + 1e-7)*input[:,1:]\n",
    "        if log_transform:\n",
    "            input_unscaled = torch.exp(input_unscaled)\n",
    "\n",
    "        # Getting Yj's (atomic mass fractions) and computing missing masses\n",
    "        Y_el_initial = torch.matmul(input_unscaled, A_element_t)\n",
    "        Y_el_p = torch.matmul(x_unscaled, A_element_t)\n",
    "        delta_Y_el = Y_el_initial - Y_el_p\n",
    "\n",
    "        # Computing epsilon_k's (correction to balancing species: \"CO2\", \"H2O\", \"O2\", \"N2\")\n",
    "        epsilon_k = torch.matmul(delta_Y_el, A_inv_final_t)\n",
    "\n",
    "        # Updating Yk\n",
    "        outputs = x_unscaled + epsilon_k  \n",
    "\n",
    "        # Log & scale\n",
    "        if log_transform:\n",
    "            outputs[outputs < threshold] = threshold\n",
    "            outputs = torch.log(outputs)\n",
    "        outputs = (outputs - Yscaler_mean)/(Yscaler_std+1.0e-7)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate the new model. We can also read parameters from the already trained baseline model is we want to. In this case, we need to make sure that the model structure (except final layer) is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cst = ChemNN_Element()\n",
    "\n",
    "# Load previous model parameters\n",
    "trained_model_file = os.path.join(folder, \"pytorch_mlp.pt\")\n",
    "model_cst.load_state_dict(torch.load(trained_model_file))\n",
    "print(model_cst)\n",
    "\n",
    "model_cst = model_cst.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then proceed with the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model_cst.parameters(), lr=0.001)\n",
    "\n",
    "epochs, epochs_small, loss_list, val_loss_list, stats_sum_yk, stats_A_elements = main_training_loop(X_train, X_val, Y_train, Y_val, loss_fn, optimizer, n_epochs, model_cst, log_transform, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(folder,\"pytorch_mlp_hard.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we plot the losses and the conservation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses_conservation(epochs, epochs_small, loss_list, val_loss_list, stats_sum_yk, stats_A_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the test simulations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_test_results = []\n",
    "\n",
    "for i, row in df_sim_test.iterrows():\n",
    "\n",
    "    phi_ini = row['Phi']\n",
    "    temperature_ini = row['T0']\n",
    "\n",
    "    print(f\"Performing test computation for phi={phi_ini}; T0={temperature_ini}\")\n",
    "\n",
    "    df_exact, df_nn = compute_nn_cantera_0D_homo(device, model_cst, Xscaler, Yscaler, phi_ini, temperature_ini, dt, dtb_params)\n",
    "\n",
    "    list_test_results.append((df_exact, df_nn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot results for a given simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_sim = 40\n",
    "spec_to_plot = \"H2O2\"\n",
    "plot_results_sim(i_sim, list_test_results, spec_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_errors_hard_cst = compute_fitness(df_exact, df_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting gas species for labels\n",
    "gas = ct.Solution(mech_file)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.boxplot(data_errors_hard_cst, ax=ax)\n",
    "\n",
    "custom_labels = [\"T\"] + gas.species_names + [\"Total\"]\n",
    "ax.set_xticklabels(custom_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_vect = data_errors_hard_cst[:,-1]\n",
    "\n",
    "print(f\"Averaged on set of test simulations, error is M={M_vect.mean()} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_sim_max = data_errors_hard_cst[:,-1].argmax()\n",
    "print(f\"Simulation with largest error: {i_sim_max}\")\n",
    "print(f\"Error is: {data_errors_hard_cst[:,-1][i_sim_max]} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To go further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the *0D_database_generation.ipynb* and the present notebook, you can play around and do some tests on your own. Here are some ideas:\n",
    "\n",
    "+ Variation of the sampling space: extension of the $T_0$ and/or $\\phi$ range, change of the pressure.\n",
    "\n",
    "+ Change of the sampling method, using the *dt_cvode* option.\n",
    "\n",
    "+ Applying methodology with another (bigger) chemical mechanism. The mechanism *mech_ch4_lu30.yaml* is provided, it is a 30 species mechanism for methane ($CH_4$) combustion.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
