{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning applied to 0D reactors\n",
    "\n",
    "In this notebook, we will train neural networks to replace the CVODE solver of CANTERA. We will use the databases generated in the *0D_database_generation.ipynb* notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_colab = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google colab preparation\n",
    "\n",
    "These lines are here to enable Colab running of the tools. We need to perform a git clone in order to have access to python scripts. This needs to be done at each runtime as the clone is lost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if use_colab:\n",
    "    !git clone -b cost_course_exercices https://github.com/cmehl/ML_chem.git\n",
    "    \n",
    "    !pip install cantera\n",
    "\n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Create a folder in the root directory\n",
    "    if not os.path.isdir(\"/content/drive/MyDrive/ML_chem_data\"):\n",
    "        !mkdir -p \"/content/drive/MyDrive/ML_chem_data\"\n",
    "    else:\n",
    "        print(\"Folder /content/drive/MyDrive/ML_chem_data already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import cantera as ct\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(\"notebook\")\n",
    "\n",
    "if use_colab:\n",
    "    from ML_chem.chem_ai.cantera_runs import compute_nn_cantera_0D_homo\n",
    "    from ML_chem.chem_ai.utils import get_molar_mass_atomic_matrix\n",
    "    from ML_chem.chem_ai.utils import StandardScaler\n",
    "else:\n",
    "    from chem_ai.cantera_runs import compute_nn_cantera_0D_homo\n",
    "    from chem_ai.utils import get_molar_mass_atomic_matrix\n",
    "    from chem_ai.utils import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the default pytorch precision to double. It slows down a little bit the training but it is the usual standard for CFD reacting flows applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We identify the device (CPU or GPU) available on the machine. This will be used by pytorch to identify the device on which to train and use the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda:0')\n",
    "  print('Running on the GPU')\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "  print('Running on the CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary\n",
    "\n",
    "### Loading the required data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the folder including the desired database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"./case_0D_highT\" \n",
    "\n",
    "if use_colab:\n",
    "    folder = \"/content/drive/MyDrive/ML_chem_data/case_0D_highT\"\n",
    "else:\n",
    "    folder = \"./case_0D_highT\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the parameters stored in the json file of the dabatase folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(folder, \"dtb_params.json\"), \"r\") as file:\n",
    "    dtb_params = json.load(file)\n",
    "\n",
    "fuel = dtb_params[\"fuel\"]\n",
    "mech_file = dtb_params[\"mech_file\"]\n",
    "log_transform = dtb_params[\"log_transform\"]\n",
    "threshold = dtb_params[\"threshold\"]\n",
    "p = dtb_params[\"p\"]\n",
    "dt = dtb_params[\"dt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the scalers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xscaler = joblib.load(os.path.join(folder, \"processed_database\", \"Xscaler.pkl\"))\n",
    "Yscaler = joblib.load(os.path.join(folder, \"processed_database\", \"Yscaler.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the training and validation databases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(os.path.join(folder, \"processed_database\",\"X_train.csv\"))\n",
    "X_val = pd.read_csv(os.path.join(folder, \"processed_database\",\"X_val.csv\"))\n",
    "Y_train = pd.read_csv(os.path.join(folder, \"processed_database\",\"Y_train.csv\"))\n",
    "Y_val = pd.read_csv(os.path.join(folder, \"processed_database\",\"Y_val.csv\"))\n",
    "\n",
    "Xcols = X_train.columns\n",
    "Ycols = Y_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of input and output dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in = X_train.shape[1]\n",
    "n_out = Y_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elements conservation matrix\n",
    "\n",
    "In combustion, elements (usually C, H, O, N) are preserved when a mixture undergoes chemical reactions, as there are no nuclear reactions. Therefore, the initial mass of elements of a mixture is conserved at the next time step and so on. For a 0D reactors (no mixing), elements mass fractions are constant for a given simulation.\n",
    "\n",
    "For a given element $j \\in {C, H, O, N}$, the mass fraction of this elements can be expressed as:\n",
    "\n",
    "$$\n",
    "Y_e^j = \\sum_{k=1}^{N_S} \\frac{M_j}{M_k} n_k^j Y_k\n",
    "$$\n",
    "\n",
    "where $M_j$ and $M_k$ are the molar masses of element $j$ and species $k$ respectively. $n_k^j$ is the number of atoms $j$ in species $k$. This equation can also be written in matrix form:\n",
    "\n",
    "$$\n",
    "Y_e = \\mathcal{A} Y\n",
    "$$\n",
    "\n",
    "where $Y_e \\in \\mathbb{R}^4$ is the vector of elements mass fractions and $Y \\in \\mathbb{R}^{N_S}$ the vector of species mass fractions. The matrix $\\mathcal{A} \\in \\mathbb{R}^{4 \\times N_S}$ is defined be the following coefficients:\n",
    "\n",
    "$$\n",
    "\\mathcal{A}_{jk} = \\frac{M_j}{M_k} n_k^j\n",
    "$$\n",
    "\n",
    "The matrix $\\mathcal{A}$ can be computed using the function *get_molar_mass_atomic_matrix* given in *utils.py*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [1.         1.         0.         0.         0.05926971 0.11190674\n",
      "  0.         0.0305399  0.05926971]\n",
      " [0.         0.         1.         1.         0.94073029 0.88809326\n",
      "  0.         0.9694601  0.94073029]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  1.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "gas = ct.Solution(mech_file)\n",
    "A_element = get_molar_mass_atomic_matrix(gas.species_names, fuel, True)\n",
    "print(A_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix will be helpful to analyze the conservation of elements in the training loop and at inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data loaded (scalers, training/validation sets, etc...) are in numpy format. In order to use them in a Pytorch training loop, we need to convert them to *torch* tensors. Those tensors are very similar to numpy arrays, with similar functions.\n",
    "\n",
    "We first transform training and validation datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train.values, dtype=torch.float64)\n",
    "Y_train = torch.tensor(Y_train.values, dtype=torch.float64)\n",
    "X_val = torch.tensor(X_val.values, dtype=torch.float64)\n",
    "Y_val = torch.tensor(Y_val.values, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to deal with the scaler, we decide to extract the mean and standard deviation and write the formula directly when necessary. These quantities are here converted to torch tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xscaler_mean = torch.from_numpy(Xscaler.mean.values)\n",
    "Xscaler_std = torch.from_numpy(Xscaler.std.values)\n",
    "\n",
    "Yscaler_mean = torch.from_numpy(Yscaler.mean.values)\n",
    "Yscaler_std = torch.from_numpy(Yscaler.std.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conservation matrix $A$ also needs to be converted as it will be used during the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_element = torch.tensor(A_element, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another aspect is that the data needs to be on the correct device, as pytorch will look for the data on it. As CPU and GPU memory is not shared, we will have to manually move the data to the GPU if necessary. We do it here for training/validation data, scalers and conservation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to(device)\n",
    "Y_train = Y_train.to(device)\n",
    "X_val = X_val.to(device)\n",
    "Y_val = Y_val.to(device)\n",
    "\n",
    "Xscaler_mean = Xscaler_mean.to(device)\n",
    "Xscaler_std = Xscaler_std.to(device)\n",
    "\n",
    "Yscaler_mean = Yscaler_mean.to(device)\n",
    "Yscaler_std = Yscaler_std.to(device)\n",
    "\n",
    "A_element = A_element.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now can generate the model. In this work, we will consider a simple Multi Layer Perceptron (MLP). We generate the model using Pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChemNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(n_in, 60)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.hidden2 = nn.Linear(60, 60)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.output = nn.Linear(60, n_out)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.hidden1(x))\n",
    "        x = self.act2(self.hidden2(x))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is then instantiated and transferred to the GPU if present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChemNN(\n",
      "  (hidden1): Linear(in_features=10, out_features=60, bias=True)\n",
      "  (act1): ReLU()\n",
      "  (hidden2): Linear(in_features=60, out_features=60, bias=True)\n",
      "  (act2): ReLU()\n",
      "  (output): Linear(in_features=60, out_features=9, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ChemNN()\n",
    "print(model)\n",
    "\n",
    "#Put model on GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define hyperparameters of the training loop. The following choices need to be made:\n",
    "\n",
    "+ **n_epochs**: number of passes of entire training dataset through the algorithm.\n",
    "+ **batch_size**: size of the chunks passed to the algorithm at each parameters update.\n",
    "+ **loss_fn**: loss function. In this we choose the Mean Square Loss (MSE), which is adapted to the regression problem. Assuming that the output of the ANN is $Y_k^n$ (preprocessed mass fractions) and the true value is $Y_k^{n,*}$, the loss reads:\n",
    "\n",
    "$$ \n",
    "\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{N_S} \\sum_{k=1}^{N_S} \\left( Y_{k,i}^n - Y_{k,i}^{n,*} \\right)^2\n",
    "$$\n",
    "\n",
    "where $N$ is the number of data points.\n",
    "\n",
    "+ **optimizer**: optimization method. We use here the standard Adam method with initial learning rate $lr$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 300\n",
    "batch_size = 256\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform now the main model training loop. In Pytorch the training loop needs to be written but offers flexibility in the way we can compute training monitoring quantities. In this loop, we decide to monitor conservation metrics, i.e. the sum of species mass fractions and the elements mass fractions variation.\n",
    "\n",
    "*Exercice 1:* Complete the training loop by computing monitoring metrics. Note that these metrics are computed every 10 epochs in order to limit the computational overhead.\n",
    "\n",
    "1. Compute the validation loss.\n",
    "\n",
    "2. From the conservation of mass we have $\\sum_{k=1}^{N_S} Y_k = 1$. Compute the mean, min and max of $\\sum_{k=1}^{N_S} Y_k$ over the entire validation dataset. \n",
    "\n",
    "3. Let us note $Y^{in}$ the mass fraction of species at the input of the ANN and $Y^{out}$ at the output (same for elemental mass fractions which are written $Y_e^{in}$ and $Y_e^{out}$). Elements conservation imposes $Y_e^{in}=Y_e^{out}$ which means $\\mathcal{A}Y^{in}=\\mathcal{A}Y^{out}$. We will consider here the quantity $\\delta Y_e = \\left( \\mathcal{A}Y^{out} - \\mathcal{A}Y^{in} \\right) / \\mathcal{A}Y^{in} \\in \\mathbb{R}^4$. Compute the mean, min and max of this quantity over the entire validation dataset.\n",
    "\n",
    "We give the following hints:\n",
    "\n",
    "+ Do not forget that input of output data of the ANN is preprocessed. For the scaling, you can use directly the mean and std values and the formula, as these vectors have been put to the GPU to that purpose.\n",
    "\n",
    "+ Metric arrays are defined as numpy arrays on CPU to be later plotted on matplotlib. In order to transfer a GPU torch tensor *tensor_torch* to a CPU numpy array you can use: *tensor_torch.detach().cpu().numpy()*.\n",
    "\n",
    "+ Matrix multiplication in pytorch can be done using the *torch.matmul* function and transpose by using *torch.transpose*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_training_loop(X_train, X_val, Y_train, Y_val, loss_fn, optimizer, n_epochs, model, log_transform, need_ini_vals):\n",
    "\n",
    "    # Array to store the loss and validation loss\n",
    "    loss_list = np.empty(n_epochs)\n",
    "    val_loss_list = np.empty(n_epochs//10)\n",
    "\n",
    "    # Array to store sum of mass fractions: mean, min and max\n",
    "    stats_sum_yk = np.empty((n_epochs//10,3))\n",
    "\n",
    "    # Array to store elements conservation: mean, min and max\n",
    "    stats_A_elements = np.empty((n_epochs//10,4,3))\n",
    "\n",
    "    epochs = np.arange(n_epochs)\n",
    "    epochs_small = epochs[::10]\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # Training parameters\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "\n",
    "            Xbatch = X_train[i:i+batch_size]\n",
    "            y_pred = model(Xbatch)\n",
    "            ybatch = Y_train[i:i+batch_size]\n",
    "            if need_ini_vals: #used for soft elements constraint later\n",
    "                loss = loss_fn(y_pred, ybatch, Xbatch[:,1:])\n",
    "            else:\n",
    "                loss = loss_fn(y_pred, ybatch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        loss_list[epoch] = loss\n",
    "\n",
    "        # Computing validation loss and mass conservation metric (only every 10 epochs as it is expensive)\n",
    "        if epoch%10==0:\n",
    "            model.eval()  # evaluation mode\n",
    "            with torch.no_grad():\n",
    "\n",
    "                # VALIDATION LOSS\n",
    "                y_val_pred = model(X_val)\n",
    "                if need_ini_vals:\n",
    "                    val_loss = loss_fn(y_val_pred, Y_val, X_val[:,1:])\n",
    "                else:\n",
    "                    val_loss = loss_fn(y_val_pred, Y_val)\n",
    "\n",
    "                # SUM OF MASS FRACTION\n",
    "                #Inverse scale done by hand to stay with Torch arrays\n",
    "                yk = Yscaler_mean + (Yscaler_std + 1e-7)*y_val_pred\n",
    "                if log_transform:\n",
    "                    yk = torch.exp(yk)\n",
    "                sum_yk = yk.sum(axis=1)\n",
    "                sum_yk = sum_yk.detach().cpu().numpy()\n",
    "                stats_sum_yk[epoch//10,0] = sum_yk.mean() \n",
    "                stats_sum_yk[epoch//10,1] = sum_yk.min()\n",
    "                stats_sum_yk[epoch//10,2] = sum_yk.max()\n",
    "\n",
    "                # ELEMENTS CONSERVATION\n",
    "                yval_in = Xscaler_mean[1:] + (Xscaler_std[1:] + 1e-7)*X_val[:,1:]\n",
    "                if log_transform:\n",
    "                    yval_in = torch.exp(yval_in)\n",
    "                ye_in = torch.matmul(A_element, torch.transpose(yval_in, 0, 1))\n",
    "                ye_out = torch.matmul(A_element, torch.transpose(yk, 0, 1))\n",
    "                delta_ye = (ye_out - ye_in)/(ye_in+1e-10)\n",
    "                delta_ye = delta_ye.detach().cpu().numpy()\n",
    "                stats_A_elements[epoch//10, :, 0] = delta_ye.mean(axis=1)\n",
    "                stats_A_elements[epoch//10, :, 1] = delta_ye.min(axis=1)\n",
    "                stats_A_elements[epoch//10, :, 2] = delta_ye.max(axis=1)\n",
    "\n",
    "            model.train()   # Back to training mode\n",
    "            val_loss_list[epoch//10] = val_loss\n",
    "\n",
    "        print(f\"Finished epoch {epoch}\")\n",
    "        print(f\"    >> Loss: {loss}\")\n",
    "        if epoch%10==0:\n",
    "            print(f\"    >> Validation loss: {val_loss}\")\n",
    "\n",
    "    return epochs, epochs_small, loss_list, val_loss_list, stats_sum_yk, stats_A_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 0\n",
      "    >> Loss: 0.0894298542500097\n",
      "    >> Validation loss: 0.09441843228899548\n",
      "Finished epoch 1\n",
      "    >> Loss: 0.06972018495865212\n",
      "Finished epoch 2\n",
      "    >> Loss: 0.04847530189745515\n",
      "Finished epoch 3\n",
      "    >> Loss: 0.026190520474987065\n",
      "Finished epoch 4\n",
      "    >> Loss: 0.00945195044124659\n",
      "Finished epoch 5\n",
      "    >> Loss: 0.0060686568672627976\n",
      "Finished epoch 6\n",
      "    >> Loss: 0.0067234045183874255\n",
      "Finished epoch 7\n",
      "    >> Loss: 0.008169697845827205\n",
      "Finished epoch 8\n",
      "    >> Loss: 0.008819224451275792\n",
      "Finished epoch 9\n",
      "    >> Loss: 0.00892224325496501\n",
      "Finished epoch 10\n",
      "    >> Loss: 0.008892534025459935\n",
      "    >> Validation loss: 0.0060448017945164126\n",
      "Finished epoch 11\n",
      "    >> Loss: 0.008057582787430982\n",
      "Finished epoch 12\n",
      "    >> Loss: 0.007080270852146038\n",
      "Finished epoch 13\n",
      "    >> Loss: 0.006065854227500902\n",
      "Finished epoch 14\n",
      "    >> Loss: 0.005182193507849727\n",
      "Finished epoch 15\n",
      "    >> Loss: 0.00478761732572669\n",
      "Finished epoch 16\n",
      "    >> Loss: 0.00435388716000197\n",
      "Finished epoch 17\n",
      "    >> Loss: 0.003920940229424143\n",
      "Finished epoch 18\n",
      "    >> Loss: 0.0036682410941728787\n",
      "Finished epoch 19\n",
      "    >> Loss: 0.0034516411837627503\n",
      "Finished epoch 20\n",
      "    >> Loss: 0.0033048924241694103\n",
      "    >> Validation loss: 0.003261129715752003\n",
      "Finished epoch 21\n",
      "    >> Loss: 0.003164121843061005\n",
      "Finished epoch 22\n",
      "    >> Loss: 0.003105717504788186\n",
      "Finished epoch 23\n",
      "    >> Loss: 0.0030093885321703135\n",
      "Finished epoch 24\n",
      "    >> Loss: 0.002955629696272878\n",
      "Finished epoch 25\n",
      "    >> Loss: 0.0028943093093701314\n",
      "Finished epoch 26\n",
      "    >> Loss: 0.00277906181213476\n",
      "Finished epoch 27\n",
      "    >> Loss: 0.002649422643207505\n",
      "Finished epoch 28\n",
      "    >> Loss: 0.0025368628565694954\n",
      "Finished epoch 29\n",
      "    >> Loss: 0.0024266176953146703\n",
      "Finished epoch 30\n",
      "    >> Loss: 0.0023672550606984367\n",
      "    >> Validation loss: 0.002441436126483549\n",
      "Finished epoch 31\n",
      "    >> Loss: 0.0023026346641751303\n",
      "Finished epoch 32\n",
      "    >> Loss: 0.0022627466806635728\n",
      "Finished epoch 33\n",
      "    >> Loss: 0.0022143656718733983\n",
      "Finished epoch 34\n",
      "    >> Loss: 0.0022146302075161323\n",
      "Finished epoch 35\n",
      "    >> Loss: 0.002173614528815369\n",
      "Finished epoch 36\n",
      "    >> Loss: 0.0021380204603441157\n",
      "Finished epoch 37\n",
      "    >> Loss: 0.0021029218317472485\n",
      "Finished epoch 38\n",
      "    >> Loss: 0.0020799400174592174\n",
      "Finished epoch 39\n",
      "    >> Loss: 0.002039283519355131\n",
      "Finished epoch 40\n",
      "    >> Loss: 0.0019883433652209273\n",
      "    >> Validation loss: 0.0016481132644576197\n",
      "Finished epoch 41\n",
      "    >> Loss: 0.001935860988370506\n",
      "Finished epoch 42\n",
      "    >> Loss: 0.0018883635294060942\n",
      "Finished epoch 43\n",
      "    >> Loss: 0.0018002417164126076\n",
      "Finished epoch 44\n",
      "    >> Loss: 0.0017239097147502347\n",
      "Finished epoch 45\n",
      "    >> Loss: 0.0016380850039217276\n",
      "Finished epoch 46\n",
      "    >> Loss: 0.0014834165108659118\n",
      "Finished epoch 47\n",
      "    >> Loss: 0.001340355670875331\n",
      "Finished epoch 48\n",
      "    >> Loss: 0.0012173894352697189\n",
      "Finished epoch 49\n",
      "    >> Loss: 0.0010632054889997938\n",
      "Finished epoch 50\n",
      "    >> Loss: 0.0009112132687254792\n",
      "    >> Validation loss: 0.0008377838976973603\n",
      "Finished epoch 51\n",
      "    >> Loss: 0.0007886928149588694\n",
      "Finished epoch 52\n",
      "    >> Loss: 0.0006839537530967713\n",
      "Finished epoch 53\n",
      "    >> Loss: 0.0005908101846996137\n",
      "Finished epoch 54\n",
      "    >> Loss: 0.0005034889197380186\n",
      "Finished epoch 55\n",
      "    >> Loss: 0.0004080663418260332\n",
      "Finished epoch 56\n",
      "    >> Loss: 0.0003512964632284838\n",
      "Finished epoch 57\n",
      "    >> Loss: 0.0002721914786514141\n",
      "Finished epoch 58\n",
      "    >> Loss: 0.00023700587363259515\n",
      "Finished epoch 59\n",
      "    >> Loss: 0.00021431437351445014\n",
      "Finished epoch 60\n",
      "    >> Loss: 0.00022366563226886753\n",
      "    >> Validation loss: 0.0002595542198081914\n",
      "Finished epoch 61\n",
      "    >> Loss: 0.00024932841741373774\n",
      "Finished epoch 62\n",
      "    >> Loss: 0.0002555897777333455\n",
      "Finished epoch 63\n",
      "    >> Loss: 0.0002820660665905111\n",
      "Finished epoch 64\n",
      "    >> Loss: 0.0002773915843678576\n",
      "Finished epoch 65\n",
      "    >> Loss: 0.0003205170542707905\n",
      "Finished epoch 66\n",
      "    >> Loss: 0.0002667392046111528\n",
      "Finished epoch 67\n",
      "    >> Loss: 0.00030658422197668843\n",
      "Finished epoch 68\n",
      "    >> Loss: 0.00031954999290716575\n",
      "Finished epoch 69\n",
      "    >> Loss: 0.0002871061392427445\n",
      "Finished epoch 70\n",
      "    >> Loss: 0.0002571149161776159\n",
      "    >> Validation loss: 0.0002239594487017999\n",
      "Finished epoch 71\n",
      "    >> Loss: 0.00023677576979133392\n",
      "Finished epoch 72\n",
      "    >> Loss: 0.0003091879811336706\n",
      "Finished epoch 73\n",
      "    >> Loss: 0.0002602834151311706\n",
      "Finished epoch 74\n",
      "    >> Loss: 0.0003109321421962344\n",
      "Finished epoch 75\n",
      "    >> Loss: 0.0003433337100505671\n",
      "Finished epoch 76\n",
      "    >> Loss: 0.00034796476143804784\n",
      "Finished epoch 77\n",
      "    >> Loss: 0.00029297264722437404\n",
      "Finished epoch 78\n",
      "    >> Loss: 0.0003303664049431038\n",
      "Finished epoch 79\n",
      "    >> Loss: 0.00042350315576270264\n",
      "Finished epoch 80\n",
      "    >> Loss: 0.00031214309238410213\n",
      "    >> Validation loss: 0.00017648376502404353\n",
      "Finished epoch 81\n",
      "    >> Loss: 0.00048061389393061856\n",
      "Finished epoch 82\n",
      "    >> Loss: 0.00036859242587134415\n",
      "Finished epoch 83\n",
      "    >> Loss: 0.00047779556539799523\n",
      "Finished epoch 84\n",
      "    >> Loss: 0.00027575880476636897\n",
      "Finished epoch 85\n",
      "    >> Loss: 0.00026086473098888143\n",
      "Finished epoch 86\n",
      "    >> Loss: 0.000367032055830718\n",
      "Finished epoch 87\n",
      "    >> Loss: 0.0003503272474253206\n",
      "Finished epoch 88\n",
      "    >> Loss: 0.0003554148919711199\n",
      "Finished epoch 89\n",
      "    >> Loss: 0.0002687563266021597\n",
      "Finished epoch 90\n",
      "    >> Loss: 0.00036981987781878643\n",
      "    >> Validation loss: 0.00023311829773865983\n",
      "Finished epoch 91\n",
      "    >> Loss: 0.00038655913367334884\n",
      "Finished epoch 92\n",
      "    >> Loss: 0.0002753283208822918\n",
      "Finished epoch 93\n",
      "    >> Loss: 0.00035816912229673733\n",
      "Finished epoch 94\n",
      "    >> Loss: 0.00034123010406116294\n",
      "Finished epoch 95\n",
      "    >> Loss: 0.0003537361770692609\n",
      "Finished epoch 96\n",
      "    >> Loss: 0.00030048613099422207\n",
      "Finished epoch 97\n",
      "    >> Loss: 0.0003148532088605566\n",
      "Finished epoch 98\n",
      "    >> Loss: 0.0003669808770045981\n",
      "Finished epoch 99\n",
      "    >> Loss: 0.0003944758483740745\n",
      "Finished epoch 100\n",
      "    >> Loss: 0.000287917158909736\n",
      "    >> Validation loss: 0.00022745453499045933\n",
      "Finished epoch 101\n",
      "    >> Loss: 0.00029484187180694473\n",
      "Finished epoch 102\n",
      "    >> Loss: 0.0003192471507072544\n",
      "Finished epoch 103\n",
      "    >> Loss: 0.00038904053562078633\n",
      "Finished epoch 104\n",
      "    >> Loss: 0.00032040957730838736\n",
      "Finished epoch 105\n",
      "    >> Loss: 0.0002753054252084462\n",
      "Finished epoch 106\n",
      "    >> Loss: 0.0002850671911904216\n",
      "Finished epoch 107\n",
      "    >> Loss: 0.0003252431609921448\n",
      "Finished epoch 108\n",
      "    >> Loss: 0.0003264593315850808\n",
      "Finished epoch 109\n",
      "    >> Loss: 0.00035090810739850717\n",
      "Finished epoch 110\n",
      "    >> Loss: 0.0002553527767344246\n",
      "    >> Validation loss: 0.00020683249049997327\n",
      "Finished epoch 111\n",
      "    >> Loss: 0.0002581458725050451\n",
      "Finished epoch 112\n",
      "    >> Loss: 0.00030985944012738875\n",
      "Finished epoch 113\n",
      "    >> Loss: 0.0003538910081182848\n",
      "Finished epoch 114\n",
      "    >> Loss: 0.00031000672789518855\n",
      "Finished epoch 115\n",
      "    >> Loss: 0.0002960943705807688\n",
      "Finished epoch 116\n",
      "    >> Loss: 0.0003781328999747912\n",
      "Finished epoch 117\n",
      "    >> Loss: 0.00032251506382667197\n",
      "Finished epoch 118\n",
      "    >> Loss: 0.00027870066438572227\n",
      "Finished epoch 119\n",
      "    >> Loss: 0.0004472522281415513\n",
      "Finished epoch 120\n",
      "    >> Loss: 0.0001994265870550147\n",
      "    >> Validation loss: 0.00020372629147327834\n",
      "Finished epoch 121\n",
      "    >> Loss: 0.00020594873231669195\n",
      "Finished epoch 122\n",
      "    >> Loss: 0.0002774209701971906\n",
      "Finished epoch 123\n",
      "    >> Loss: 0.0003550874429205551\n",
      "Finished epoch 124\n",
      "    >> Loss: 0.00031637238465303846\n",
      "Finished epoch 125\n",
      "    >> Loss: 0.00039096408484483325\n",
      "Finished epoch 126\n",
      "    >> Loss: 0.0004267991961176122\n",
      "Finished epoch 127\n",
      "    >> Loss: 0.00023938880814677088\n",
      "Finished epoch 128\n",
      "    >> Loss: 0.00028641121497664026\n",
      "Finished epoch 129\n",
      "    >> Loss: 0.0002905394365901132\n",
      "Finished epoch 130\n",
      "    >> Loss: 0.00038294777135988836\n",
      "    >> Validation loss: 0.000259180914185459\n",
      "Finished epoch 131\n",
      "    >> Loss: 0.00034401227694097323\n",
      "Finished epoch 132\n",
      "    >> Loss: 0.00027143187176409124\n",
      "Finished epoch 133\n",
      "    >> Loss: 0.00028813861361674803\n",
      "Finished epoch 134\n",
      "    >> Loss: 0.0002105062450306487\n",
      "Finished epoch 135\n",
      "    >> Loss: 0.00017156917944094257\n",
      "Finished epoch 136\n",
      "    >> Loss: 0.00012006217988289367\n",
      "Finished epoch 137\n",
      "    >> Loss: 0.00010100621197416797\n",
      "Finished epoch 138\n",
      "    >> Loss: 0.00010331987538236808\n",
      "Finished epoch 139\n",
      "    >> Loss: 0.0001153511803016246\n",
      "Finished epoch 140\n",
      "    >> Loss: 0.00013769835067724536\n",
      "    >> Validation loss: 8.051544023681563e-05\n",
      "Finished epoch 141\n",
      "    >> Loss: 0.0001300942836222164\n",
      "Finished epoch 142\n",
      "    >> Loss: 0.00015704422720908535\n",
      "Finished epoch 143\n",
      "    >> Loss: 0.00012757447132881222\n",
      "Finished epoch 144\n",
      "    >> Loss: 0.0001428085886329129\n",
      "Finished epoch 145\n",
      "    >> Loss: 0.0001231523458273859\n",
      "Finished epoch 146\n",
      "    >> Loss: 0.00016439218943179384\n",
      "Finished epoch 147\n",
      "    >> Loss: 0.00013104640130539623\n",
      "Finished epoch 148\n",
      "    >> Loss: 0.00012401237675486795\n",
      "Finished epoch 149\n",
      "    >> Loss: 0.00011592056890323748\n",
      "Finished epoch 150\n",
      "    >> Loss: 0.00012688260071453238\n",
      "    >> Validation loss: 8.689752060765818e-05\n",
      "Finished epoch 151\n",
      "    >> Loss: 0.00010658363117074695\n",
      "Finished epoch 152\n",
      "    >> Loss: 0.00012736327131603108\n",
      "Finished epoch 153\n",
      "    >> Loss: 0.0001168867503142003\n",
      "Finished epoch 154\n",
      "    >> Loss: 9.349284067263966e-05\n",
      "Finished epoch 155\n",
      "    >> Loss: 0.00014966320495585518\n",
      "Finished epoch 156\n",
      "    >> Loss: 0.00014370321086974822\n",
      "Finished epoch 157\n",
      "    >> Loss: 0.00010853509222964089\n",
      "Finished epoch 158\n",
      "    >> Loss: 0.00010568741989623996\n",
      "Finished epoch 159\n",
      "    >> Loss: 0.00014863817734630136\n",
      "Finished epoch 160\n",
      "    >> Loss: 0.00010854785209134204\n",
      "    >> Validation loss: 7.005936064549204e-05\n",
      "Finished epoch 161\n",
      "    >> Loss: 0.00011671065428542231\n",
      "Finished epoch 162\n",
      "    >> Loss: 0.00012010920117189972\n",
      "Finished epoch 163\n",
      "    >> Loss: 0.00011763902446051585\n",
      "Finished epoch 164\n",
      "    >> Loss: 0.0001115817398695616\n",
      "Finished epoch 165\n",
      "    >> Loss: 0.0001149416072697371\n",
      "Finished epoch 166\n",
      "    >> Loss: 0.00010677173479231874\n",
      "Finished epoch 167\n",
      "    >> Loss: 0.00011044921299204196\n",
      "Finished epoch 168\n",
      "    >> Loss: 0.00011739758950556372\n",
      "Finished epoch 169\n",
      "    >> Loss: 8.974455531984249e-05\n",
      "Finished epoch 170\n",
      "    >> Loss: 0.00012844462490934342\n",
      "    >> Validation loss: 8.710389176097767e-05\n",
      "Finished epoch 171\n",
      "    >> Loss: 0.00010068144698614743\n",
      "Finished epoch 172\n",
      "    >> Loss: 0.00011566698139239866\n",
      "Finished epoch 173\n",
      "    >> Loss: 0.00012636816761213564\n",
      "Finished epoch 174\n",
      "    >> Loss: 0.00011844991253089258\n",
      "Finished epoch 175\n",
      "    >> Loss: 0.00011801067029936024\n",
      "Finished epoch 176\n",
      "    >> Loss: 0.00011249628492671826\n",
      "Finished epoch 177\n",
      "    >> Loss: 0.00010853316376991782\n",
      "Finished epoch 178\n",
      "    >> Loss: 8.967528844725786e-05\n",
      "Finished epoch 179\n",
      "    >> Loss: 0.00010441796684425918\n",
      "Finished epoch 180\n",
      "    >> Loss: 8.944290605546705e-05\n",
      "    >> Validation loss: 5.821177426131019e-05\n",
      "Finished epoch 181\n",
      "    >> Loss: 0.0001268930736979459\n",
      "Finished epoch 182\n",
      "    >> Loss: 6.97581488002434e-05\n",
      "Finished epoch 183\n",
      "    >> Loss: 0.00010245601570564963\n",
      "Finished epoch 184\n",
      "    >> Loss: 9.0836251512519e-05\n",
      "Finished epoch 185\n",
      "    >> Loss: 9.4326517375436e-05\n",
      "Finished epoch 186\n",
      "    >> Loss: 9.075100609752088e-05\n",
      "Finished epoch 187\n",
      "    >> Loss: 8.777706856941115e-05\n",
      "Finished epoch 188\n",
      "    >> Loss: 0.00011396726416319185\n",
      "Finished epoch 189\n",
      "    >> Loss: 8.107635309284538e-05\n",
      "Finished epoch 190\n",
      "    >> Loss: 0.00010326067634970676\n",
      "    >> Validation loss: 7.250864139156383e-05\n",
      "Finished epoch 191\n",
      "    >> Loss: 7.366208667130654e-05\n",
      "Finished epoch 192\n",
      "    >> Loss: 9.951634865321632e-05\n",
      "Finished epoch 193\n",
      "    >> Loss: 7.030984923012942e-05\n",
      "Finished epoch 194\n",
      "    >> Loss: 9.038498985363357e-05\n",
      "Finished epoch 195\n",
      "    >> Loss: 7.640726874587031e-05\n",
      "Finished epoch 196\n",
      "    >> Loss: 9.284154322112429e-05\n",
      "Finished epoch 197\n",
      "    >> Loss: 7.41283783766241e-05\n",
      "Finished epoch 198\n",
      "    >> Loss: 8.942327561402577e-05\n",
      "Finished epoch 199\n",
      "    >> Loss: 7.963434669033882e-05\n",
      "Finished epoch 200\n",
      "    >> Loss: 8.552833976910777e-05\n",
      "    >> Validation loss: 5.955853375050125e-05\n",
      "Finished epoch 201\n",
      "    >> Loss: 8.433643242811845e-05\n",
      "Finished epoch 202\n",
      "    >> Loss: 7.088641851859565e-05\n",
      "Finished epoch 203\n",
      "    >> Loss: 0.00010100903403684666\n",
      "Finished epoch 204\n",
      "    >> Loss: 6.493829187879127e-05\n",
      "Finished epoch 205\n",
      "    >> Loss: 9.123279329677218e-05\n",
      "Finished epoch 206\n",
      "    >> Loss: 6.61242045410246e-05\n",
      "Finished epoch 207\n",
      "    >> Loss: 8.718174461115007e-05\n",
      "Finished epoch 208\n",
      "    >> Loss: 7.321762326785298e-05\n",
      "Finished epoch 209\n",
      "    >> Loss: 6.908472834573239e-05\n",
      "Finished epoch 210\n",
      "    >> Loss: 9.252568993825197e-05\n",
      "    >> Validation loss: 6.556333802468143e-05\n",
      "Finished epoch 211\n",
      "    >> Loss: 7.643595478841387e-05\n",
      "Finished epoch 212\n",
      "    >> Loss: 8.728147004818021e-05\n",
      "Finished epoch 213\n",
      "    >> Loss: 6.995011277271733e-05\n",
      "Finished epoch 214\n",
      "    >> Loss: 9.532641372526172e-05\n",
      "Finished epoch 215\n",
      "    >> Loss: 6.577260139976116e-05\n",
      "Finished epoch 216\n",
      "    >> Loss: 8.258535629192516e-05\n",
      "Finished epoch 217\n",
      "    >> Loss: 6.815646081684626e-05\n",
      "Finished epoch 218\n",
      "    >> Loss: 8.550157516481432e-05\n",
      "Finished epoch 219\n",
      "    >> Loss: 6.152531147835808e-05\n",
      "Finished epoch 220\n",
      "    >> Loss: 8.759298185006867e-05\n",
      "    >> Validation loss: 7.041099939735407e-05\n",
      "Finished epoch 221\n",
      "    >> Loss: 5.9894182333631754e-05\n",
      "Finished epoch 222\n",
      "    >> Loss: 8.64620427935545e-05\n",
      "Finished epoch 223\n",
      "    >> Loss: 5.897948406351688e-05\n",
      "Finished epoch 224\n",
      "    >> Loss: 9.051603402964997e-05\n",
      "Finished epoch 225\n",
      "    >> Loss: 6.100222044007879e-05\n",
      "Finished epoch 226\n",
      "    >> Loss: 8.316950329060756e-05\n",
      "Finished epoch 227\n",
      "    >> Loss: 5.450874269286262e-05\n",
      "Finished epoch 228\n",
      "    >> Loss: 8.133983219719058e-05\n",
      "Finished epoch 229\n",
      "    >> Loss: 7.612641491583472e-05\n",
      "Finished epoch 230\n",
      "    >> Loss: 6.91036616119564e-05\n",
      "    >> Validation loss: 4.891544319617404e-05\n",
      "Finished epoch 231\n",
      "    >> Loss: 7.858414269802526e-05\n",
      "Finished epoch 232\n",
      "    >> Loss: 5.963949857237516e-05\n",
      "Finished epoch 233\n",
      "    >> Loss: 7.692586087265937e-05\n",
      "Finished epoch 234\n",
      "    >> Loss: 6.954397175011831e-05\n",
      "Finished epoch 235\n",
      "    >> Loss: 6.653912138744244e-05\n",
      "Finished epoch 236\n",
      "    >> Loss: 8.540853392243357e-05\n",
      "Finished epoch 237\n",
      "    >> Loss: 5.14758183914005e-05\n",
      "Finished epoch 238\n",
      "    >> Loss: 9.181831040586305e-05\n",
      "Finished epoch 239\n",
      "    >> Loss: 5.540758236677822e-05\n",
      "Finished epoch 240\n",
      "    >> Loss: 7.830533016950428e-05\n",
      "    >> Validation loss: 6.57934544581218e-05\n",
      "Finished epoch 241\n",
      "    >> Loss: 5.7032973371278536e-05\n",
      "Finished epoch 242\n",
      "    >> Loss: 7.977525456957163e-05\n",
      "Finished epoch 243\n",
      "    >> Loss: 5.94246045232561e-05\n",
      "Finished epoch 244\n",
      "    >> Loss: 7.855627486920785e-05\n",
      "Finished epoch 245\n",
      "    >> Loss: 5.9787979619423255e-05\n",
      "Finished epoch 246\n",
      "    >> Loss: 7.848289723023822e-05\n",
      "Finished epoch 247\n",
      "    >> Loss: 9.235137703919114e-05\n",
      "Finished epoch 248\n",
      "    >> Loss: 8.46907359691602e-05\n",
      "Finished epoch 249\n",
      "    >> Loss: 5.9047708160700415e-05\n",
      "Finished epoch 250\n",
      "    >> Loss: 7.114735043961686e-05\n",
      "    >> Validation loss: 5.676218168736891e-05\n",
      "Finished epoch 251\n",
      "    >> Loss: 6.148053842421958e-05\n",
      "Finished epoch 252\n",
      "    >> Loss: 7.03979580500233e-05\n",
      "Finished epoch 253\n",
      "    >> Loss: 6.635592314921909e-05\n",
      "Finished epoch 254\n",
      "    >> Loss: 6.418884478172765e-05\n",
      "Finished epoch 255\n",
      "    >> Loss: 6.28613646233465e-05\n",
      "Finished epoch 256\n",
      "    >> Loss: 7.097317982456392e-05\n",
      "Finished epoch 257\n",
      "    >> Loss: 6.242894753288852e-05\n",
      "Finished epoch 258\n",
      "    >> Loss: 6.596246924030819e-05\n",
      "Finished epoch 259\n",
      "    >> Loss: 6.84184202005474e-05\n",
      "Finished epoch 260\n",
      "    >> Loss: 6.866375474334336e-05\n",
      "    >> Validation loss: 5.693590715903689e-05\n",
      "Finished epoch 261\n",
      "    >> Loss: 6.885064702650515e-05\n",
      "Finished epoch 262\n",
      "    >> Loss: 6.375963447937754e-05\n",
      "Finished epoch 263\n",
      "    >> Loss: 7.227553489835157e-05\n",
      "Finished epoch 264\n",
      "    >> Loss: 7.717540555745057e-05\n",
      "Finished epoch 265\n",
      "    >> Loss: 6.331047108162808e-05\n",
      "Finished epoch 266\n",
      "    >> Loss: 8.07160647232128e-05\n",
      "Finished epoch 267\n",
      "    >> Loss: 6.30207496286893e-05\n",
      "Finished epoch 268\n",
      "    >> Loss: 5.517383239051225e-05\n",
      "Finished epoch 269\n",
      "    >> Loss: 0.00010037854846978458\n",
      "Finished epoch 270\n",
      "    >> Loss: 5.956586215986897e-05\n",
      "    >> Validation loss: 4.121826691617779e-05\n",
      "Finished epoch 271\n",
      "    >> Loss: 8.914292482768275e-05\n",
      "Finished epoch 272\n",
      "    >> Loss: 6.789993617659007e-05\n",
      "Finished epoch 273\n",
      "    >> Loss: 5.666696466941705e-05\n",
      "Finished epoch 274\n",
      "    >> Loss: 7.964782464278482e-05\n",
      "Finished epoch 275\n",
      "    >> Loss: 6.459924904374602e-05\n",
      "Finished epoch 276\n",
      "    >> Loss: 7.442444555764033e-05\n",
      "Finished epoch 277\n",
      "    >> Loss: 5.5149747790408314e-05\n",
      "Finished epoch 278\n",
      "    >> Loss: 9.03837788288081e-05\n",
      "Finished epoch 279\n",
      "    >> Loss: 5.76794251500891e-05\n",
      "Finished epoch 280\n",
      "    >> Loss: 8.140792521639879e-05\n",
      "    >> Validation loss: 5.517232382065883e-05\n",
      "Finished epoch 281\n",
      "    >> Loss: 6.091146059849977e-05\n",
      "Finished epoch 282\n",
      "    >> Loss: 5.824228072484296e-05\n",
      "Finished epoch 283\n",
      "    >> Loss: 6.027438578050719e-05\n",
      "Finished epoch 284\n",
      "    >> Loss: 5.62531035876693e-05\n",
      "Finished epoch 285\n",
      "    >> Loss: 5.89067104845757e-05\n",
      "Finished epoch 286\n",
      "    >> Loss: 4.679163267363031e-05\n",
      "Finished epoch 287\n",
      "    >> Loss: 5.2619788314872255e-05\n",
      "Finished epoch 288\n",
      "    >> Loss: 4.182309411934199e-05\n",
      "Finished epoch 289\n",
      "    >> Loss: 6.214361184833361e-05\n",
      "Finished epoch 290\n",
      "    >> Loss: 3.6431854360941e-05\n",
      "    >> Validation loss: 2.7367999355482513e-05\n",
      "Finished epoch 291\n",
      "    >> Loss: 6.1028725282015265e-05\n",
      "Finished epoch 292\n",
      "    >> Loss: 3.674829060376053e-05\n",
      "Finished epoch 293\n",
      "    >> Loss: 6.110016154575496e-05\n",
      "Finished epoch 294\n",
      "    >> Loss: 3.782567253795878e-05\n",
      "Finished epoch 295\n",
      "    >> Loss: 5.1823608455852476e-05\n",
      "Finished epoch 296\n",
      "    >> Loss: 4.381639910754153e-05\n",
      "Finished epoch 297\n",
      "    >> Loss: 4.181189129173998e-05\n",
      "Finished epoch 298\n",
      "    >> Loss: 4.41014120886742e-05\n",
      "Finished epoch 299\n",
      "    >> Loss: 4.746687451326068e-05\n",
      " TRAINING DURATION: 18.643759327000225\n"
     ]
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "epochs, epochs_small, loss_list, val_loss_list, stats_sum_yk, stats_A_elements = main_training_loop(X_train, X_val, Y_train, Y_val, loss_fn, optimizer, n_epochs, model, log_transform, False)\n",
    "end_time = time.perf_counter()\n",
    "print(f\" TRAINING DURATION: {end_time-start_time} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a function to analyze the training. We plot:\n",
    "\n",
    "+ The training and validation losses\n",
    "\n",
    "+ The evolution of $\\sum_{k=1}^{N_S} Y_k$ (mean, min and max).\n",
    "\n",
    "+ The elements conservation by plotting $100\\times\\delta Y_e$ for each element (C, H, O and N). The factor $100$ enables to get an error in \\%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses_conservation(epochs, epochs_small, loss_list, val_loss_list, stats_sum_yk, stats_A_elements):\n",
    "\n",
    "    # LOSSES\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(epochs, loss_list, color=\"k\", label=\"Training\")\n",
    "    ax.plot(epochs_small, val_loss_list, color=\"r\", label = \"Validation\")\n",
    "\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "\n",
    "    # MASS CONSERVATION\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(epochs_small, stats_sum_yk[:,0], color=\"k\")\n",
    "    ax.plot(epochs_small, stats_sum_yk[:,1], color=\"k\", ls=\"--\")\n",
    "    ax.plot(epochs_small, stats_sum_yk[:,2], color=\"k\", ls=\"--\")\n",
    "\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(r\"$\\sum_k \\ Y_k$\")\n",
    "\n",
    "    # ELEMENTS CONSERVATION\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2)\n",
    "\n",
    "    # C\n",
    "    ax1.plot(epochs_small, 100*stats_A_elements[:,0,0], color=\"k\")\n",
    "    ax1.plot(epochs_small, 100*stats_A_elements[:,0,1], color=\"k\", ls=\"--\")\n",
    "    ax1.plot(epochs_small, 100*stats_A_elements[:,0,2], color=\"k\", ls=\"--\")\n",
    "\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(r\"$\\Delta Y_C$ $(\\%$)\")\n",
    "\n",
    "    # H\n",
    "    ax2.plot(epochs_small, 100*stats_A_elements[:,1,0], color=\"k\")\n",
    "    ax2.plot(epochs_small, 100*stats_A_elements[:,1,1], color=\"k\", ls=\"--\")\n",
    "    ax2.plot(epochs_small, 100*stats_A_elements[:,1,2], color=\"k\", ls=\"--\")\n",
    "\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(r\"$\\Delta Y_H$ $(\\%)$\")\n",
    "\n",
    "    # O\n",
    "    ax3.plot(epochs_small, 100*stats_A_elements[:,2,0], color=\"k\")\n",
    "    ax3.plot(epochs_small, 100*stats_A_elements[:,2,1], color=\"k\", ls=\"--\")\n",
    "    ax3.plot(epochs_small, 100*stats_A_elements[:,2,2], color=\"k\", ls=\"--\")\n",
    "\n",
    "    ax3.set_xlabel(\"Epoch\")\n",
    "    ax3.set_ylabel(r\"$\\Delta Y_O$ $(\\%)$\")\n",
    "\n",
    "    # N\n",
    "    ax4.plot(epochs_small, 100*stats_A_elements[:,3,0], color=\"k\")\n",
    "    ax4.plot(epochs_small, 100*stats_A_elements[:,3,1], color=\"k\", ls=\"--\")\n",
    "    ax4.plot(epochs_small, 100*stats_A_elements[:,3,2], color=\"k\", ls=\"--\")\n",
    "\n",
    "    ax4.set_xlabel(\"Epoch\")\n",
    "    ax4.set_ylabel(r\"$\\Delta Y_N$ $(\\%)$\")\n",
    "\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses_conservation(epochs, epochs_small, loss_list, val_loss_list, stats_sum_yk, stats_A_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the Pytorch model in the case folder for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(folder,\"pytorch_mlp.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN model test\n",
    "\n",
    "Now that we have generated the model we would like to test it on unseen data. For this, we will use the test initial conditions which were stored during data generation. The methodology is as follows:\n",
    "\n",
    "1. We get the test conditions and simulate CANTERA flames with (i) the CVODE solver and (ii) the generated ANN.\n",
    "2. We define appropriate metric and assess the accuracy of the ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing simulations with CANTERA and NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the test initial conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim_test = pd.read_csv(os.path.join(folder, \"sim_test.csv\"))\n",
    "\n",
    "n_sim = df_sim_test.shape[0]\n",
    "print(f\"There are {n_sim} test simulations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to compute 0D reactors for each conditions in *df_sim_test*. We use here the function *compute_nn_cantera_0D_homo* which is given. It takes as input initial conditions $T_0, \\phi$, the ANN model (with associated scalers), the time step (used for the ANN) and the database parameters. It outputs two dataframes containing the exact simulation and the ANN simulation. These results are concatenated in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_test_results = []\n",
    "\n",
    "fails = 0\n",
    "for i, row in df_sim_test.iterrows():\n",
    "\n",
    "    phi_ini = row['Phi']\n",
    "    temperature_ini = row['T0']\n",
    "\n",
    "    print(f\"Performing test computation for phi={phi_ini}; T0={temperature_ini}\")\n",
    "\n",
    "    df_exact, df_nn, fail = compute_nn_cantera_0D_homo(device, model, Xscaler, Yscaler, phi_ini, temperature_ini, dt, dtb_params, A_element.detach().cpu().numpy())\n",
    "\n",
    "    fails += fail\n",
    "\n",
    "    list_test_results.append((df_exact, df_nn))\n",
    "\n",
    "\n",
    "print(f\"Total number of simulations which crashed: {fails}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are what dataframes look like for a given simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_sim = 1\n",
    "list_test_results[i_sim][0].head()   # Exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_test_results[i_sim][1].head()   # ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the results for a given simulation *i_sim*. We can look at temperature and species mass fractions for instance. We can also analyze conservation metrics. If we learned in logarithmic space, it is interesting also to plot in log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results_sim(i_sim, list_test_results, spec_to_plot):\n",
    "\n",
    "    df_exact = list_test_results[i_sim][0]\n",
    "    df_nn = list_test_results[i_sim][1]\n",
    "\n",
    "    # Temperature \n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(df_exact['Time'], df_exact['Temperature'], color='k')\n",
    "    ax.plot(df_nn['Time'], df_nn['Temperature'], color='b')\n",
    "    ax.set_xlabel(\"Time [s]\")\n",
    "    ax.set_ylabel(\"T [K]\")\n",
    "\n",
    "    # Species (normal)\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(df_exact['Time'], df_exact[spec_to_plot], color='k')\n",
    "    ax.plot(df_nn['Time'], df_nn[spec_to_plot], color='b')\n",
    "    ax.set_xlabel(\"Time [s]\")\n",
    "    ax.set_ylabel(f\"{spec_to_plot} [-]\")\n",
    "\n",
    "    # Species (log)\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(df_exact['Time'], np.log(df_exact[spec_to_plot]), color='k')\n",
    "    ax.plot(df_nn['Time'], np.log(df_nn[spec_to_plot]), color='b')\n",
    "    ax.set_xlabel(\"Time [s]\")\n",
    "    ax.set_ylabel(f\"{spec_to_plot} [-]\")\n",
    "\n",
    "    # Sum of Yk\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(df_nn['Time'], df_nn['SumYk'], color='b')\n",
    "    ax.set_xlabel(\"Time [s]\")\n",
    "    ax.set_ylabel(\"$\\sum Y_k$ [-]\")\n",
    "\n",
    "    # Elements\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2)\n",
    "    ax1.plot(df_nn['Time'], df_nn['Y_C'], color='b')\n",
    "    ax2.plot(df_nn['Time'], df_nn['Y_H'], color='b')\n",
    "    ax3.plot(df_nn['Time'], df_nn['Y_O'], color='b')\n",
    "    ax4.plot(df_nn['Time'], df_nn['Y_N'], color='b')\n",
    "    ax1.set_ylabel(\"$Y_C$\")\n",
    "    ax2.set_ylabel(\"$Y_H$\")\n",
    "    ax3.set_ylabel(\"$Y_O$\")\n",
    "    ax4.set_ylabel(\"$Y_N$\")\n",
    "    ax3.set_xlabel(\"Time [s]\")\n",
    "    ax4.set_xlabel(\"Time [s]\")\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_sim = 40\n",
    "spec_to_plot = \"H2O2\"\n",
    "plot_results_sim(i_sim, list_test_results, spec_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the ANN run can also check the conservation of mass and elements:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing error statistics\n",
    "\n",
    "We need a metric to assess the accuracy of the ANN over the entire test simulations. To do that, we will define normalized fitness functions for each simulation, and average the values over the simulation. As previously, we note $Y_k^n$ the processed mass fractions (including potential log and scaling) from the ANN, and $Y_k^{n,*}$ the exact values. The error on species mass fractions is then for given initial conditions:\n",
    "\n",
    "$$\n",
    "\\mathcal{M}_k(T_0,\\phi) = \\frac{1}{N_{iter}} \\sum_{i=1}^{N_{iter}} \\left| \\frac{Y_k^n - Y_k^{n,*}}{Y_k^{n,*}} \\right|\n",
    "$$\n",
    "\n",
    "where $N_{iter}$ is the number of iterations of the considered simulation (it may vary from one simulation to another as it is controlled by a stopping criterion).\n",
    "\n",
    "Although it is not a direct output of the model, we can still compute error on the temperature. We write $T^n$ and $T^{n,*}$ the normalized predicted and exact temperatures, respestively (note that log is never applied on temeprature). The error is:\n",
    "\n",
    "$$\n",
    "\\mathcal{M}_T(T_0,\\phi) = \\frac{1}{N_{iter}} \\sum_{i=1}^{N_{iter}} \\left| \\frac{T^n - T^{n,*}}{T^{n,*}} \\right|\n",
    "$$\n",
    "\n",
    "Finally, we can define a global error for each simulation as the mean of all errors (they can be compared as everything is normalized):\n",
    "\n",
    "$$\n",
    "\\mathcal{M}(T_0,\\phi) = \\frac{\\mathcal{M}_T(T_0,\\phi) + \\sum_{k=1}^{N_S} \\mathcal{M}_k(T_0,\\phi)}{N_S+1}\n",
    "$$\n",
    "\n",
    "*Exercice 2:* we define a function *compute_fitness* to calculate these errors. Complete the calculation of the errors in this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fitness(df_exact, df_nn):\n",
    "\n",
    "    # Results will be stored in data_errors array.\n",
    "    # The first column corresponds to errors on temperature\n",
    "    # The next n_out columns correspond to errors on species mass fractions\n",
    "    # The last column corresponds to the mean error\n",
    "    data_errors = np.empty([n_sim, n_out+2]) \n",
    "\n",
    "    for i_sim in range(n_sim):\n",
    "\n",
    "        df_exact = list_test_results[i_sim][0]\n",
    "        df_nn = list_test_results[i_sim][1]\n",
    "\n",
    "        # Removing undesired variables\n",
    "        df_exact = df_exact.drop('Time', axis=1)\n",
    "        df_nn = df_nn.drop([\"Time\",\"SumYk\", \"Y_C\", \"Y_H\", \"Y_O\", \"Y_N\"], axis=1)\n",
    "\n",
    "        # Applying log\n",
    "        if log_transform:\n",
    "\n",
    "            df_exact[df_exact < threshold] = threshold\n",
    "            df_nn[df_nn < threshold] = threshold\n",
    "\n",
    "            df_exact.iloc[:, 1:] = np.log(df_exact.iloc[:, 1:])\n",
    "            df_nn.iloc[:, 1:] = np.log(df_nn.iloc[:, 1:])\n",
    "\n",
    "        # Scaling\n",
    "        data_exact_scaled = (df_exact.values-Xscaler.mean.values)/(Xscaler.std.values+1.0e-7)\n",
    "        data_nn_scaled = (df_nn.values-Xscaler.mean.values)/(Xscaler.std.values+1.0e-7)\n",
    "\n",
    "        diff_exact_nn = np.abs((data_nn_scaled-data_exact_scaled)/data_exact_scaled)\n",
    "\n",
    "        diff_exact_nn = diff_exact_nn.mean(axis=0)\n",
    "\n",
    "        M = diff_exact_nn.mean()\n",
    "\n",
    "        print(f\"Simulation {i_sim} error M = {M}\")\n",
    "\n",
    "        data_errors[i_sim, :n_out+1] = diff_exact_nn\n",
    "        data_errors[i_sim, n_out+1] = M\n",
    "\n",
    "\n",
    "    return data_errors\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_errors = compute_fitness(df_exact, df_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize errors, we can draw a boxplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting gas species for labels\n",
    "gas = ct.Solution(mech_file)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.boxplot(data_errors, ax=ax)\n",
    "\n",
    "custom_labels = [\"T\"] + gas.species_names + [\"Total\"]\n",
    "ax.set_xticklabels(custom_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercice 3*: Find the simulation with the highest error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_sim_max = data_errors[:,-1].argmax()\n",
    "print(f\"Simulation with largest error: {i_sim_max}\")\n",
    "print(f\"Error is: {data_errors[:,-1][i_sim_max]} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can average the errors of all simulations to get a global error which can be used to get an idea of the overall error of the ANN:\n",
    "\n",
    "$$\n",
    "\\mathcal{M}_{avg} = \\frac{1}{N_{test}} \\sum_{i=1}^{N_{test}} \\mathcal{M}(T_{0,i},\\phi_i)\n",
    "$$\n",
    "\n",
    "where $N_{test}$ is the number of test simulations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_vect = data_errors[:,-1]\n",
    "\n",
    "print(f\"Averaged on set of test simulations, error is M={M_vect.mean()} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results must be taken with caution, as the fitness function is not perfect. Amonsgt other potentiel issues we can note:\n",
    "\n",
    "+ If the profile are slightly shifted (for example if the ignition delay is mispredicted), it will lead to large errors. A solution could be to add the ignition delay in teh fitness and compute profiles errors in a progress variable space.\n",
    "+ When profiles are close to $0$, it can lead to large errors (for example if we predict $10^{-6}$ instead of $10^{-7}$ for a major species). This will artificially increase errors.\n",
    "\n",
    "Other fitness functions are possible, feel free to implement other ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enforcing physical information: soft constraints "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the strategy employed above, the ANN model freely predicts the new chemical states without being forced to satisfy any constraints. As already analyzed above, several constraints must be satisfied. First, mass fractions should sum up to $1$:\n",
    "\n",
    "$$\n",
    "\\sum_{k=1}^{N_S} Y_k = 1\n",
    "$$\n",
    "\n",
    "Additionally, elements must be conserved. This amounts to say that the elements mass fractions must be equal for the ANN inputs and outputs:\n",
    "\n",
    "$$\n",
    "Y_e^{in} = Y_e^{out}\n",
    "$$\n",
    "\n",
    "One way to lead the ANN towards verifying constraints is to add penalization terms in the loss functions. Those are then called **soft constraints**. In the present case, we will consider two loss functions. The first one tries to impose mass conservation:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{mass} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{N_S} \\sum_{k=1}^{N_S} \\left( Y_{k,i}^n - Y_{k,i}^{n,*} \\right)^2 + \\alpha_{mass} \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\sum_{k=1}^{N_S} Y_k - 1 \\right)^2\n",
    "$$\n",
    "\n",
    "where $\\alpha_{mass}$ is a hyperparameter to control the constraint weight in the loss.\n",
    "\n",
    "Another loss can be defined by adding a penalty term for elements conservation:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{elt} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{N_S} \\sum_{k=1}^{N_S} \\left( Y_{k,i}^n - Y_{k,i}^{n,*} \\right)^2 + \\alpha_{elt} \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{4} \\sum_{j=1}^{4} \\left( Y_{e,j}^{out} - Y_{e,j}^{in} \\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercice 4:* complete the loss function for mass conservation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sumYkLoss(nn.Module):\n",
    "    def __init__(self, alpha):\n",
    "        super(sumYkLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, pred, targets):\n",
    "\n",
    "        yk = Yscaler_mean + pred * (Yscaler_std+1.0e-7)\n",
    "        if log_transform:\n",
    "            yk = torch.exp(yk)\n",
    "        sum_yk = yk.sum(axis=1)\n",
    "\n",
    "        return torch.mean((pred - targets) ** 2) + self.alpha * torch.mean((sum_yk-1.0)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercice 5:* complete the loss function for elements conservation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElementLoss(nn.Module):\n",
    "    def __init__(self, alpha):\n",
    "        super(ElementLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, pred, targets, yk_scaled_in):\n",
    "\n",
    "        # Elements input\n",
    "        yk_in = Xscaler_mean[1:] + yk_scaled_in * (Xscaler_std[1:]+1.0e-7)\n",
    "        if log_transform:\n",
    "            yk_in = torch.exp(yk_in)\n",
    "        ye_in = torch.matmul(A_element, torch.transpose(yk_in, 0, 1))\n",
    "\n",
    "        # Elements output\n",
    "        yk = Yscaler_mean + pred * (Yscaler_std+1.0e-7)\n",
    "        if log_transform:\n",
    "            yk = torch.exp(yk)\n",
    "        ye_out = torch.matmul(A_element, torch.transpose(yk, 0, 1))\n",
    "\n",
    "        return torch.mean((pred - targets) ** 2) + self.alpha * torch.mean((ye_out - ye_in) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin from the model already trained without constraints. You can also choose to train a new model from scratch. We can redo the training loop and setting one of the new losses: (feel free to test both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "alpha = 100.0\n",
    "\n",
    "# loss_fn = sumYkLoss(alpha)\n",
    "# need_ini_val = False\n",
    "\n",
    "loss_fn = ElementLoss(alpha)\n",
    "need_ini_val = True\n",
    "\n",
    "epochs, epochs_small, loss_list, val_loss_list, stats_sum_yk, stats_A_elements = main_training_loop(X_train, X_val, Y_train, Y_val, loss_fn, optimizer, n_epochs, model, log_transform, need_ini_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(folder,\"pytorch_mlp_soft.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot again the metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses_conservation(epochs, epochs_small, loss_list, val_loss_list, stats_sum_yk, stats_A_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can recompute the test simulations using the new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_test_results = []\n",
    "\n",
    "fails = 0\n",
    "for i, row in df_sim_test.iterrows():\n",
    "\n",
    "    phi_ini = row['Phi']\n",
    "    temperature_ini = row['T0']\n",
    "\n",
    "    print(f\"Performing test computation for phi={phi_ini}; T0={temperature_ini}\")\n",
    "\n",
    "    df_exact, df_nn, fail = compute_nn_cantera_0D_homo(device, model, Xscaler, Yscaler, phi_ini, temperature_ini, dt, dtb_params, A_element.detach().cpu().numpy())\n",
    "\n",
    "    fails += fail\n",
    "\n",
    "    list_test_results.append((df_exact, df_nn))\n",
    "\n",
    "\n",
    "print(f\"Total number of simulations which crashed: {fails}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can again plot results for a given simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_sim = 40\n",
    "spec_to_plot = \"H2O2\"\n",
    "plot_results_sim(i_sim, list_test_results, spec_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_errors_soft_cst = compute_fitness(df_exact, df_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting of the errors distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting gas species for labels\n",
    "gas = ct.Solution(mech_file)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.boxplot(data_errors_soft_cst, ax=ax)\n",
    "\n",
    "custom_labels = [\"T\"] + gas.species_names + [\"Total\"]\n",
    "ax.set_xticklabels(custom_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_vect = data_errors_soft_cst[:,-1]\n",
    "\n",
    "print(f\"Averaged on set of test simulations, error is M={M_vect.mean()} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_sim_max = data_errors_soft_cst[:,-1].argmax()\n",
    "print(f\"Simulation with largest error: {i_sim_max}\")\n",
    "print(f\"Error is: {data_errors_soft_cst[:,-1][i_sim_max]} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enforcing physical information: hard constraints "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that adding soft constraints is not sufficient to absolutely guarantee the conservation of mass and elements. An alternative way is to use hard constraints, where constraints are directly encoded in the neural network. In this case, conservation of elements is guaranteed at inference.\n",
    "\n",
    "**Remark:** the method that will be exposed here is not compatible yet with logarithm transform due to issues with back-propagation convergence; and also to the fact that the thresholding removes some mass. For this reason, we encourage you to re-run this notebook using a second database, without log transform. In order to get interesting results, it is then necessary to limit the ignition zone where log was necessary. To do so, you can increase the initial temperatures of the reactors. We can choose $T_0 \\in [2100, 2200]$ K for example.\n",
    "\n",
    "In this section, we will focus on elements conservation only, as it leads also to mass conservation. As discussed earlier we have:\n",
    "\n",
    "$$\n",
    "\\mathcal{A}Y^{in} = \\mathcal{A}Y^{out}\n",
    "$$\n",
    "\n",
    "The idea is to add a final layer (without any trainable parameter) which will perform a projection on a space were elements are conserved. We will note $Y'$ the vector before applying this layer. The idea followed here (not the only solution) is to balance the mass of elements using a pre-defined set of \"balancing\" species, containing at least one instance of each element. As we deal with combustion, we choose here: $CO_2$, $H_2O$, $O_2$ and $N_2$ (for hydrogen $CO_2$ is removed). The vector $Y'$ is corrected as follows:\n",
    "\n",
    "$$\n",
    "Y^{out} = Y' + \\epsilon\n",
    "$$\n",
    "\n",
    "where $\\epsilon$ is a correction term computed to balance elements. We have then:\n",
    "\n",
    "$$\n",
    "\\mathcal{A} \\epsilon = \\mathcal{A} \\left(Y^{in} -Y' \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "This system has $N_S$ unknowns (components of $\\epsilon$) but only 4 equations (or 3 for $H_2$) and is therefore indeterminated. We use here the selected balancing species, and define:\n",
    "\n",
    "$$\n",
    "\\epsilon' = \\left( \\epsilon_{CO_2}, \\epsilon_{H_2O}, \\epsilon_{O_2}, \\epsilon_{N_2} \\right)\n",
    "$$\n",
    "\n",
    "The new system is then:\n",
    "\n",
    "$$\n",
    "\\mathcal{A} \\epsilon' = \\mathcal{A}' \\left(Y^{in} -Y' \\right)\n",
    "$$\n",
    "\n",
    "where $\\mathcal{A}'$ is a sub-matrix of $\\mathcal{A}$ containing only the columns associated to balancing species. Finally we can solve the linear system:\n",
    "\n",
    "$$\n",
    "\\epsilon' = \\mathcal{A}'^{-1} \\mathcal{A} \\left(Y^{in} -Y' \\right)\n",
    "$$\n",
    "\n",
    "And we have: $Y^{out} = Y' + \\epsilon'$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will precompute the matrix $\\mathcal{A}'^{-1}$, based on the already computed $\\mathcal{A}$ matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_species = len(gas.species_names)\n",
    "\n",
    "# For H2, we need here to remove carbon\n",
    "if fuel==\"H2\":\n",
    "    A_element_final = A_element[1:,:]\n",
    "else:\n",
    "    A_element_final = A_element\n",
    "\n",
    "\n",
    "# We define balancing species (without CO2 if H2 is considered)\n",
    "if fuel==\"H2\":\n",
    "    balancing_species = [\"H2O\", \"O2\", \"N2\"]\n",
    "    mass_per_atom_array = np.array([1.008, 15.999, 14.007])\n",
    "else:\n",
    "    balancing_species = [\"CO2\", \"H2O\", \"O2\", \"N2\"]\n",
    "    mass_per_atom_array = np.array([12.011, 1.008, 15.999])\n",
    "\n",
    "# We transpose the matrix, as it will be needed in this form in the ann model (because X are of shape (n_samples, N_S))\n",
    "A_element_t = torch.transpose(A_element_final, 0, 1)\n",
    "        \n",
    "# We construct the matrix A' by selecting balancing species only\n",
    "A_reduced = A_element_final[:,[gas.species_names.index(spec) for spec in balancing_species]]\n",
    "        \n",
    "# We invert A'\n",
    "A_reduced_inv = torch.linalg.inv(A_reduced)\n",
    "    \n",
    "# We want to automatically add 0 for non-balancing species, we therefore add liens of 0 in the matrix\n",
    "A_inv_final = torch.zeros((nb_species, len(balancing_species)))\n",
    "for i, spec in enumerate(balancing_species):\n",
    "    A_inv_final[gas.species_names.index(spec),:] = A_reduced_inv[i,:]\n",
    "    \n",
    "# We will also need the transpose\n",
    "A_inv_final_t = torch.transpose(A_inv_final, 0, 1)\n",
    "\n",
    "# Sending to GPU\n",
    "A_element_t = A_element_t.to(device)\n",
    "A_inv_final_t = A_inv_final_t.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a new Pytorch model.\n",
    "\n",
    "*Exercice 6:* Complete the *forward* method of the newly defined model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChemNN_Element(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(n_in, 60)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.hidden2 = nn.Linear(60, 60)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.output = nn.Linear(60, n_out)\n",
    " \n",
    "    def forward(self, input):\n",
    "        \n",
    "        x = self.act1(self.hidden1(input))\n",
    "        x = self.act2(self.hidden2(x))\n",
    "        x = self.output(x)\n",
    "\n",
    "        # Unscale previous layer\n",
    "        x_unscaled = Yscaler_mean + (Yscaler_std + 1e-7)*x\n",
    "        if log_transform:\n",
    "            x_unscaled = torch.exp(x_unscaled)\n",
    "\n",
    "        # Unscale input\n",
    "        input_unscaled = Xscaler_mean[1:] + (Xscaler_std[1:] + 1e-7)*input[:,1:]\n",
    "        if log_transform:\n",
    "            input_unscaled = torch.exp(input_unscaled)\n",
    "\n",
    "        # Getting Yj's (atomic mass fractions) and computing missing masses\n",
    "        Y_el_initial = torch.matmul(input_unscaled, A_element_t)\n",
    "        Y_el_p = torch.matmul(x_unscaled, A_element_t)\n",
    "        delta_Y_el = Y_el_initial - Y_el_p\n",
    "\n",
    "        # Computing epsilon_k's (correction to balancing species: \"CO2\", \"H2O\", \"O2\", \"N2\")\n",
    "        epsilon_k = torch.matmul(delta_Y_el, A_inv_final_t)\n",
    "\n",
    "        # Updating Yk\n",
    "        outputs = x_unscaled + epsilon_k  \n",
    "\n",
    "        # Log & scale\n",
    "        if log_transform:\n",
    "            outputs[outputs < threshold] = threshold\n",
    "            outputs = torch.log(outputs)\n",
    "        outputs = (outputs - Yscaler_mean)/(Yscaler_std+1.0e-7)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate the new model. We can also read parameters from the already trained baseline model is we want to. In this case, we need to make sure that the model structure (except final layer) is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cst = ChemNN_Element()\n",
    "\n",
    "# Load previous model parameters\n",
    "trained_model_file = os.path.join(folder, \"pytorch_mlp.pt\")\n",
    "model_cst.load_state_dict(torch.load(trained_model_file))\n",
    "print(model_cst)\n",
    "\n",
    "model_cst = model_cst.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then proceed with the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model_cst.parameters(), lr=0.001)\n",
    "\n",
    "epochs, epochs_small, loss_list, val_loss_list, stats_sum_yk, stats_A_elements = main_training_loop(X_train, X_val, Y_train, Y_val, loss_fn, optimizer, n_epochs, model_cst, log_transform, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(folder,\"pytorch_mlp_hard.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we plot the losses and the conservation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses_conservation(epochs, epochs_small, loss_list, val_loss_list, stats_sum_yk, stats_A_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the test simulations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_test_results = []\n",
    "\n",
    "fails = 0\n",
    "for i, row in df_sim_test.iterrows():\n",
    "\n",
    "    phi_ini = row['Phi']\n",
    "    temperature_ini = row['T0']\n",
    "\n",
    "    print(f\"Performing test computation for phi={phi_ini}; T0={temperature_ini}\")\n",
    "\n",
    "    df_exact, df_nn, fail = compute_nn_cantera_0D_homo(device, model_cst, Xscaler, Yscaler, phi_ini, temperature_ini, dt, dtb_params, A_element.detach().cpu().numpy())\n",
    "\n",
    "    fails += fail\n",
    "\n",
    "    list_test_results.append((df_exact, df_nn))\n",
    "\n",
    "\n",
    "print(f\"Total number of simulations which crashed: {fails}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot results for a given simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_sim = 40\n",
    "spec_to_plot = \"H2O2\"\n",
    "plot_results_sim(i_sim, list_test_results, spec_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_errors_hard_cst = compute_fitness(df_exact, df_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting gas species for labels\n",
    "gas = ct.Solution(mech_file)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.boxplot(data_errors_hard_cst, ax=ax)\n",
    "\n",
    "custom_labels = [\"T\"] + gas.species_names + [\"Total\"]\n",
    "ax.set_xticklabels(custom_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_vect = data_errors_hard_cst[:,-1]\n",
    "\n",
    "print(f\"Averaged on set of test simulations, error is M={M_vect.mean()} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_sim_max = data_errors_hard_cst[:,-1].argmax()\n",
    "print(f\"Simulation with largest error: {i_sim_max}\")\n",
    "print(f\"Error is: {data_errors_hard_cst[:,-1][i_sim_max]} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To go further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the *0D_database_generation.ipynb* and the present notebook, you can play around and do some tests on your own. Here are some ideas:\n",
    "\n",
    "+ Variation of the sampling space: extension of the $T_0$ and/or $\\phi$ range, change of the pressure.\n",
    "\n",
    "+ Change of the sampling method, using the *dt_cvode* option.\n",
    "\n",
    "+ Applying methodology with another (bigger) chemical mechanism. The mechanism *mech_ch4_lu30.yaml* is provided, it is a 30 species mechanism for methane ($CH_4$) combustion.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
