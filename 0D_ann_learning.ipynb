{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning applied to 0D reactors\n",
    "\n",
    "In this notebook, we will train neural networks to replace the CVODE solver of CANTERA. We will use the databases generated in the *0D_database_generation.ipynb* notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import cantera as ct\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(\"notebook\")\n",
    "\n",
    "from chem_ai.cantera_runs import compute_nn_cantera_0D_homo\n",
    "from chem_ai.utils import get_molar_mass_atomic_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the default pytorch precision to double. It slows down a little bit the training but it is the usual standard for CFD reacting flows applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We identify the device (CPU or GPU) available on the machine. This will be used by pytorch to identify the device on which to train and use the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda:0')\n",
    "  print('Running on the GPU')\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "  print('Running on the CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary\n",
    "\n",
    "### Loading the required data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the folder including the desired database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"./case_0D_test_case\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the parameters stored in the json file of the dabatase folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(folder, \"dtb_params.json\"), \"r\") as file:\n",
    "    dtb_params = json.load(file)\n",
    "\n",
    "fuel = dtb_params[\"fuel\"]\n",
    "mech_file = dtb_params[\"mech_file\"]\n",
    "log_transform = dtb_params[\"log_transform\"]\n",
    "threshold = dtb_params[\"threshold\"]\n",
    "p = dtb_params[\"p\"]\n",
    "dt = dtb_params[\"dt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the scalers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xscaler = joblib.load(os.path.join(folder, \"processed_database\", \"Xscaler.pkl\"))\n",
    "Yscaler = joblib.load(os.path.join(folder, \"processed_database\", \"Yscaler.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the training and validation databases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(os.path.join(folder, \"processed_database\",\"X_train.csv\"))\n",
    "X_val = pd.read_csv(os.path.join(folder, \"processed_database\",\"X_val.csv\"))\n",
    "Y_train = pd.read_csv(os.path.join(folder, \"processed_database\",\"Y_train.csv\"))\n",
    "Y_val = pd.read_csv(os.path.join(folder, \"processed_database\",\"Y_val.csv\"))\n",
    "\n",
    "Xcols = X_train.columns\n",
    "Ycols = Y_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of input and output dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in = X_train.shape[1]\n",
    "n_out = Y_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elements conservation matrix\n",
    "\n",
    "In combustion, elements (usually C, H, O, N) are preserved when a mixture undergoes chemical reactions, as there are no nuclear reactions. Therefore, the initial mass of elements of a mixture is conserved at the next time step and so on. For a 0D reactors (no mixing), elements mass fractions are constant for a given simulation.\n",
    "\n",
    "For a given element $j \\in {C, H, O, N}$, the mass fraction of this elements can be expressed as:\n",
    "\n",
    "$$\n",
    "Y_e^j = \\sum_{k=1}^{N_S} \\frac{M_j}{M_k} n_k^j Y_k\n",
    "$$\n",
    "\n",
    "where $M_j$ and $M_k$ are the molar masses of element $j$ and species $k$ respectively. $n_k^j$ is the number of atoms $j$ in species $k$. This equation can also be written in matrix form:\n",
    "\n",
    "$$\n",
    "Y_e = A Y\n",
    "$$\n",
    "\n",
    "where $Y_e \\in \\mathbb{R}^4$ is the vector of elements mass fractions and $Y \\in \\mathbb{R}^{N_S}$ the vector of species mass fractions. The matrix $A \\in \\mathbb{R}^{4 \\times N_S}$ is defined be the following coefficients:\n",
    "\n",
    "$$\n",
    "A_{jk} = \\frac{M_j}{M_k} n_k^j\n",
    "$$\n",
    "\n",
    "The matrix $A$ can be computed using the function *get_molar_mass_atomic_matrix* given in *utils.py*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gas = ct.Solution(mech_file)\n",
    "A_element = get_molar_mass_atomic_matrix(gas.species_names, fuel, True)\n",
    "print(A_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix will be helpful to analyze the conservation of elements in the training loop and at inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data loaded (scalers, training/validation sets, etc...) are in numpy format. In order to use them in a Pytorch training loop, we need to convert them to *torch* tensors. Those tensors are very similar to numpy arrays, with similar functions.\n",
    "\n",
    "We first transform training and validation datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train.values, dtype=torch.float64)\n",
    "Y_train = torch.tensor(Y_train.values, dtype=torch.float64)\n",
    "X_val = torch.tensor(X_val.values, dtype=torch.float64)\n",
    "Y_val = torch.tensor(Y_val.values, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to deal with the scaler, we decide to extract the mean and standard deviation and write the formula directly when necessary. These quantities are here converted to torch tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xscaler_mean = torch.from_numpy(Xscaler.mean.values)\n",
    "Xscaler_std = torch.from_numpy(Xscaler.std.values)\n",
    "\n",
    "Yscaler_mean = torch.from_numpy(Yscaler.mean.values)\n",
    "Yscaler_std = torch.from_numpy(Yscaler.std.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conservation matrix $A$ also needs to be converted as it will be used during the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_element = torch.tensor(A_element, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another aspect is that the data needs to be on the correct device, as pytorch will look for the data on it. As CPU and GPU memory is not shared, we will have to manually move the data to the GPU if necessary. We do it here for training/validation data, scalers and conservation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to(device)\n",
    "Y_train = Y_train.to(device)\n",
    "X_val = X_val.to(device)\n",
    "Y_val = Y_val.to(device)\n",
    "\n",
    "Xscaler_mean = Xscaler_mean.to(device)\n",
    "Xscaler_std = Xscaler_std.to(device)\n",
    "\n",
    "Yscaler_mean = Yscaler_mean.to(device)\n",
    "Yscaler_std = Yscaler_std.to(device)\n",
    "\n",
    "A_element = A_element.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now can generate the model. In this work, we will consider a simple Multi Layer Perceptron (MLP). We generate the model using Pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChemNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(n_in, 60)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.hidden2 = nn.Linear(60, 60)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.output = nn.Linear(60, n_out)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.hidden1(x))\n",
    "        x = self.act2(self.hidden2(x))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is then instantiated and transferred to the GPU if present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChemNN()\n",
    "print(model)\n",
    "\n",
    "#Put model on GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define hyperparameters of the training loop. Choices made here are standard. The following choices need to be made:\n",
    "\n",
    "+ **n_epochs**: number of passies of entire training dataset through teh algorithm.\n",
    "+ **batch_size**: size of the chunks passed to the algorithm at each parameters update.\n",
    "+ **loss_fn**: loss function. In this we choose the Mean Square Loss (MSE), which is adapted to the regression problem. Assuming that the output of the ANN is $Y_k^n$ (preprocessed mass fractions) and the true value is $Y_k^{n,*}$, the loss reads:\n",
    "\n",
    "$$ \n",
    "\\mathcal{L} = \\frac{1}{n} \\sum_{i=1}^{n} \\left( Y_k^n - Y_k^{n,*} \\right)^2\n",
    "$$\n",
    "\n",
    "+ **optimizer**: optimization method. We use here the standard Adam method with initial learnign rate $lr$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 300\n",
    "batch_size = 256\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform now the main model training loop.\n",
    "\n",
    "*Exercice 1:*\n",
    "\n",
    "We give the following hints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array to store the loss and validation loss\n",
    "loss_list = np.empty(n_epochs)\n",
    "val_loss_list = np.empty(n_epochs//10)\n",
    "\n",
    "# Array to store sum of mass fractions: mean, min and max\n",
    "mean_sum_yk = np.empty(n_epochs//10)\n",
    "max_sum_yk = np.empty(n_epochs//10)\n",
    "min_sum_yk = np.empty(n_epochs//10)\n",
    "\n",
    "# Array to store elements conservation: mean, min and max\n",
    "mean_A_elements = np.empty((n_epochs//10,4))\n",
    "max_A_elements = np.empty((n_epochs//10,4))\n",
    "min_A_elements = np.empty((n_epochs//10,4))\n",
    "\n",
    "epochs = np.arange(n_epochs)\n",
    "epochs_small = epochs[::10]\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # Training parameters\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "\n",
    "        Xbatch = X_train[i:i+batch_size]\n",
    "        y_pred = model(Xbatch)\n",
    "        ybatch = Y_train[i:i+batch_size]\n",
    "        loss = loss_fn(y_pred, ybatch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    loss_list[epoch] = loss\n",
    "\n",
    "    # Computing validation loss and mass conservation metric (only every 10 epochs as it is expensive)\n",
    "    if epoch%10==0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Validation loss\n",
    "            y_val_pred = model(X_val)\n",
    "            val_loss = loss_fn(y_val_pred, Y_val)\n",
    "\n",
    "            # Sum of mass fractions\n",
    "            #Inverse scale done by hand to stay with Torch arrays\n",
    "            yk = Yscaler_mean + (Yscaler_std + 1e-7)*y_val_pred\n",
    "            if log_transform:\n",
    "                yk = torch.exp(yk)\n",
    "            sum_yk = yk.sum(axis=1)\n",
    "            sum_yk = sum_yk.detach().cpu().numpy()\n",
    "            mean_sum_yk[epoch//10] = sum_yk.mean()\n",
    "            max_sum_yk[epoch//10] = sum_yk.max()\n",
    "            min_sum_yk[epoch//10] = sum_yk.min()\n",
    "\n",
    "            # Elements conservation\n",
    "            # De-transforming inputs also\n",
    "            yval_in = Xscaler_mean[1:] + (Xscaler_std[1:] + 1e-7)*X_val[:,1:]\n",
    "            if log_transform:\n",
    "                yval_in = torch.exp(yval_in)\n",
    "            ye_in = torch.matmul(A_element, torch.transpose(yval_in, 0, 1))\n",
    "            ye_out = torch.matmul(A_element, torch.transpose(yk, 0, 1))\n",
    "            delta_ye = ye_out - ye_in\n",
    "            delta_ye = delta_ye.detach().cpu().numpy()\n",
    "            mean_A_elements[epoch//10, :] = delta_ye.mean(axis=1)\n",
    "            min_A_elements[epoch//10, :] = delta_ye.min(axis=1)\n",
    "            max_A_elements[epoch//10, :] = delta_ye.max(axis=1)\n",
    "\n",
    "        model.train()\n",
    "        val_loss_list[epoch//10] = val_loss\n",
    "\n",
    "    print(f\"Finished epoch {epoch}\")\n",
    "    print(f\"    >> Loss: {loss}\")\n",
    "    if epoch%10==0:\n",
    "        print(f\"    >> Validation loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the training and validation losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(epochs, loss_list, color=\"k\", label=\"Training\")\n",
    "ax.plot(epochs_small, val_loss_list, color=\"r\", label = \"Validation\")\n",
    "\n",
    "ax.set_yscale('log')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the evolution of $\\sum_{k=1}^{N_S} Y_k$ over epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(epochs_small, mean_sum_yk, color=\"k\")\n",
    "ax.plot(epochs_small, max_sum_yk, color=\"k\", ls=\"--\")\n",
    "ax.plot(epochs_small, min_sum_yk, color=\"k\", ls=\"--\")\n",
    "\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(r\"$\\sum_k \\ Y_k$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing conservation of chemical elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2)\n",
    "\n",
    "# C\n",
    "ax1.plot(epochs_small, mean_A_elements[:,0], color=\"k\")\n",
    "ax1.plot(epochs_small, min_A_elements[:,0], color=\"k\", ls=\"--\")\n",
    "ax1.plot(epochs_small, max_A_elements[:,0], color=\"k\", ls=\"--\")\n",
    "\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(r\"$\\Delta Y_C$\")\n",
    "\n",
    "# H\n",
    "ax2.plot(epochs_small, mean_A_elements[:,1], color=\"k\")\n",
    "ax2.plot(epochs_small, min_A_elements[:,1], color=\"k\", ls=\"--\")\n",
    "ax2.plot(epochs_small, max_A_elements[:,1], color=\"k\", ls=\"--\")\n",
    "\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(r\"$\\Delta Y_H$\")\n",
    "\n",
    "# O\n",
    "ax3.plot(epochs_small, mean_A_elements[:,2], color=\"k\")\n",
    "ax3.plot(epochs_small, min_A_elements[:,2], color=\"k\", ls=\"--\")\n",
    "ax3.plot(epochs_small, max_A_elements[:,2], color=\"k\", ls=\"--\")\n",
    "\n",
    "ax3.set_xlabel(\"Epoch\")\n",
    "ax3.set_ylabel(r\"$\\Delta Y_O$\")\n",
    "\n",
    "# N\n",
    "ax4.plot(epochs_small, mean_A_elements[:,3], color=\"k\")\n",
    "ax4.plot(epochs_small, min_A_elements[:,3], color=\"k\", ls=\"--\")\n",
    "ax4.plot(epochs_small, max_A_elements[:,3], color=\"k\", ls=\"--\")\n",
    "\n",
    "ax4.set_xlabel(\"Epoch\")\n",
    "ax4.set_ylabel(r\"$\\Delta Y_N$\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally save the Pytorch model in the case folder for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(folder,\"pytorch_mlp.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing simulations with CVODE and NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the test simulations initial conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim_test = pd.read_csv(os.path.join(folder, \"sim_test.csv\"))\n",
    "\n",
    "n_sim = df_sim_test.shape[0]\n",
    "print(f\"There are {n_sim} test simulations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_test_results = []\n",
    "\n",
    "for i, row in df_sim_test.iterrows():\n",
    "\n",
    "    phi_ini = row['Phi']\n",
    "    temperature_ini = row['T0']\n",
    "\n",
    "    print(f\"Performing test computation for phi={phi_ini}; T0={temperature_ini}\")\n",
    "\n",
    "    df_exact, df_nn = compute_nn_cantera_0D_homo(device, model, Xscaler, Yscaler, phi_ini, temperature_ini, dt, dtb_params)\n",
    "\n",
    "    list_test_results.append((df_exact, df_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_sim = 40\n",
    "df_exact = list_test_results[i_sim][0]\n",
    "df_nn = list_test_results[i_sim][1]\n",
    "\n",
    "# Temperature \n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(df_exact['Time'], df_exact['Temperature'], color='k')\n",
    "ax.plot(df_nn['Time'], df_nn['Temperature'], color='b')\n",
    "\n",
    "\n",
    "# Species\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(df_exact['Time'], df_exact['CH4'], color='k')\n",
    "ax.plot(df_nn['Time'], df_nn['CH4'], color='b')\n",
    "\n",
    "\n",
    "# Sum of Yk\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(df_nn['Time'], df_nn['SumYk'], color='b')\n",
    "\n",
    "\n",
    "# Elements\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2)\n",
    "ax1.plot(df_nn['Time'], df_nn['Y_C'], color='b')\n",
    "ax2.plot(df_nn['Time'], df_nn['Y_H'], color='b')\n",
    "ax3.plot(df_nn['Time'], df_nn['Y_O'], color='b')\n",
    "ax4.plot(df_nn['Time'], df_nn['Y_N'], color='b')\n",
    "ax1.set_ylabel(\"$Y_C$\")\n",
    "ax2.set_ylabel(\"$Y_H$\")\n",
    "ax3.set_ylabel(\"$Y_O$\")\n",
    "ax4.set_ylabel(\"$Y_N$\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing error statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to re-scale and log transform. Loop on test simulations :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_errors = np.empty([n_sim, n_out+2])  # +2 because error on temperature is included, and mean of errors also\n",
    "\n",
    "for i_sim in range(n_sim):\n",
    "\n",
    "    df_exact = list_test_results[i_sim][0]\n",
    "    df_nn = list_test_results[i_sim][1]\n",
    "\n",
    "    # Removing undesired variables\n",
    "    df_exact = df_exact.drop('Time', axis=1)\n",
    "    df_nn = df_nn.drop([\"Time\",\"SumYk\", \"Y_C\", \"Y_H\", \"Y_O\", \"Y_N\"], axis=1)\n",
    "\n",
    "\n",
    "    # Applying log\n",
    "    if log_transform:\n",
    "\n",
    "        df_exact[df_exact < threshold] = threshold\n",
    "        df_nn[df_nn < threshold] = threshold\n",
    "\n",
    "        df_exact.iloc[:, 1:] = np.log(df_exact.iloc[:, 1:])\n",
    "        df_nn.iloc[:, 1:] = np.log(df_nn.iloc[:, 1:])\n",
    "\n",
    "    # Scaling\n",
    "    data_exact_scaled = (df_exact.values-Xscaler.mean.values)/(Xscaler.std.values+1.0e-7)\n",
    "    data_nn_scaled = (df_nn.values-Xscaler.mean.values)/(Xscaler.std.values+1.0e-7)\n",
    "\n",
    "    diff_exact_nn = np.abs((data_nn_scaled-data_exact_scaled)/data_exact_scaled)\n",
    "\n",
    "    diff_exact_nn = diff_exact_nn.mean(axis=0)\n",
    "\n",
    "    M = diff_exact_nn.mean()\n",
    "\n",
    "    print(f\"Simulation {i_sim} error M = {M}\")\n",
    "\n",
    "    data_errors[i_sim, :n_out+1] = diff_exact_nn\n",
    "    data_errors[i_sim, n_out+1] = diff_exact_nn.mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing M:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting gas species for labels\n",
    "gas = ct.Solution(mech_file)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.boxplot(data_errors, ax=ax)\n",
    "\n",
    "custom_labels = [\"T\"] + gas.species_names + [\"Total\"]\n",
    "ax.set_xticklabels(custom_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_vect = data_errors[:,-1]\n",
    "\n",
    "print(f\"Averaged on set of test simulations, error is M={M_vect.mean()} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying simulation with the largest error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_sim_max = M_vect.argmax()\n",
    "print(f\"Simulation with largest error: {i_sim_max}\")\n",
    "print(f\"Error is: {M_vect[i_sim_max]} %\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
