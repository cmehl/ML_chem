{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning applied to 0D reactors with multiple dt prediction: *dt* input method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_colab = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google colab preparation\n",
    "\n",
    "These lines are here to enable Colab running of the tools. We need to perform a git clone in order to have access to python scripts. This needs to be done at each runtime as the clone is lost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if use_colab:\n",
    "    !git clone -b feature_multi_dt https://github.com/cmehl/ML_chem.git\n",
    "    \n",
    "    !pip install cantera\n",
    "\n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Create a folder in the root directory\n",
    "    if not os.path.isdir(\"/content/drive/MyDrive/ML_chem_data\"):\n",
    "        !mkdir -p \"/content/drive/MyDrive/ML_chem_data\"\n",
    "    else:\n",
    "        print(\"Folder /content/drive/MyDrive/ML_chem_data already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import cantera as ct\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(\"notebook\")\n",
    "\n",
    "if use_colab:\n",
    "    from ML_chem.chem_ai.cantera_runs import compute_nn_cantera_0D_homo\n",
    "    from ML_chem.chem_ai.utils import get_molar_mass_atomic_matrix\n",
    "    from ML_chem.chem_ai.utils import StandardScaler\n",
    "else:\n",
    "    from chem_ai.cantera_runs import compute_nn_cantera_0D_homo\n",
    "    from chem_ai.utils import get_molar_mass_atomic_matrix\n",
    "    from chem_ai.utils import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the default pytorch precision to double. It slows down a little bit the training but it is the usual standard for CFD reacting flows applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We identify the device (CPU or GPU) available on the machine. This will be used by pytorch to identify the device on which to train and use the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda:0')\n",
    "  print('Running on the GPU')\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "  print('Running on the CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the folder including the desired database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_colab:\n",
    "    folder = \"/content/drive/MyDrive/ML_chem_data/case_0D_test_multidt\"\n",
    "else:\n",
    "    folder = \"./case_0D_multidt_baseline_dtmin_incl_preddiff\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the parameters stored in the json file of the dabatase folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(folder, \"dtb_params.json\"), \"r\") as file:\n",
    "    dtb_params = json.load(file)\n",
    "\n",
    "fuel = dtb_params[\"fuel\"]\n",
    "mech_file = dtb_params[\"mech_file\"]\n",
    "log_transform = dtb_params[\"log_transform\"]\n",
    "threshold = dtb_params[\"threshold\"]\n",
    "predict_differences = dtb_params[\"predict_differences\"]\n",
    "p = dtb_params[\"p\"]\n",
    "dt = dtb_params[\"dt\"]\n",
    "\n",
    "print(f\"fuel={fuel}\")\n",
    "print(f\"mech_file={mech_file}\")\n",
    "print(f\"log_transform={log_transform}\")\n",
    "print(f\"threshold={threshold}\")\n",
    "print(f\"predict_differences={predict_differences}\")\n",
    "print(f\"p={p}\")\n",
    "print(f\"dt={dt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the scalers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xscaler = joblib.load(os.path.join(folder, \"processed_database\", \"Xscaler.pkl\"))\n",
    "Yscaler = joblib.load(os.path.join(folder, \"processed_database\", \"Yscaler.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the training and validation databases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(os.path.join(folder, \"processed_database\", \"X_train.npy\"))\n",
    "X_val = np.load(os.path.join(folder, \"processed_database\", \"X_val.npy\"))\n",
    "Y_train = np.load(os.path.join(folder, \"processed_database\", \"Y_train.npy\"))\n",
    "Y_val = np.load(os.path.join(folder, \"processed_database\", \"Y_val.npy\"))\n",
    "\n",
    "dt_array_train = np.load(os.path.join(folder, \"dt_array_train.npy\"))\n",
    "dt_array_val = np.load(os.path.join(folder, \"dt_array_val.npy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of input and output dimensions, and number of dt values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_s_train = X_train.shape[0]\n",
    "n_s_val = X_val.shape[0]\n",
    "\n",
    "n_in = X_train.shape[1]\n",
    "n_out = Y_train.shape[1]\n",
    "nb_dt = Y_train.shape[2]\n",
    "\n",
    "\n",
    "print(f\"There are {n_s_train} train vectors\")\n",
    "print(f\"There are {n_s_val} validation vectors\")\n",
    "print(\"\")\n",
    "print(f\"Input dimension: {n_in}\")\n",
    "print(f\"Output dimension: {n_out}\")\n",
    "print(f\"Number of dt: {nb_dt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gas = ct.Solution(mech_file)\n",
    "A_element = get_molar_mass_atomic_matrix(gas.species_names, fuel, True)\n",
    "print(A_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first method, which we could qualify as brute force, we add *dt* as an input of the network. \n",
    "\n",
    "We first need to prepare datasets so that we have the list of input and corresponding outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training\n",
    "\n",
    "First we tackle inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As *dt* is used as an input of the NN, it is necessary here to scale it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It needs to be done on a flatted version of dt_array\n",
    "dt_array_train_flat = dt_array_train.flatten()\n",
    "dt_array_val_flat = dt_array_val.flatten()\n",
    "\n",
    "dt_array_train_flat = np.log(dt_array_train_flat)\n",
    "dt_array_val_flat = np.log(dt_array_val_flat)\n",
    "\n",
    "Tscaler = StandardScaler()\n",
    "Tscaler.fit(dt_array_train_flat)\n",
    "dt_array_train_flat_s = Tscaler.transform(dt_array_train_flat)\n",
    "dt_array_val_flat_s = Tscaler.transform(dt_array_val_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "X_train_1 = np.empty((nb_dt*n_s_train,n_in+1))\n",
    "\n",
    "for i_dt in range(nb_dt):\n",
    "    X_train_1[i_dt::nb_dt,:-1] = X_train\n",
    "\n",
    "X_train_1[:,-1] = dt_array_train_flat_s\n",
    "\n",
    "\n",
    "# VALIDATION\n",
    "X_val_1 = np.empty((nb_dt*n_s_val,n_in+1))\n",
    "\n",
    "for i_dt in range(nb_dt):\n",
    "    X_val_1[i_dt::nb_dt,:-1] = X_val\n",
    "\n",
    "X_val_1[:,-1] = dt_array_val_flat_s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we focus on outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "Y_train_1 = np.empty((nb_dt*n_s_train,n_out))\n",
    "\n",
    "for i_dt in range(nb_dt):\n",
    "    Y_train_1[i_dt::nb_dt,:] = Y_train[:,:,i_dt]\n",
    "\n",
    "\n",
    "# VALIDATION\n",
    "Y_val_1 = np.empty((nb_dt*n_s_val,n_out))\n",
    "\n",
    "for i_dt in range(nb_dt):\n",
    "    Y_val_1[i_dt::nb_dt,:] = Y_val[:,:,i_dt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1 = torch.tensor(X_train_1, dtype=torch.float64)\n",
    "Y_train_1 = torch.tensor(Y_train_1, dtype=torch.float64)\n",
    "X_val_1 = torch.tensor(X_val_1, dtype=torch.float64)\n",
    "Y_val_1 = torch.tensor(Y_val_1, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xscaler_mean = torch.from_numpy(Xscaler.mean)\n",
    "Xscaler_std = torch.from_numpy(Xscaler.std)\n",
    "\n",
    "Yscaler_mean = torch.from_numpy(Yscaler.mean)\n",
    "Yscaler_std = torch.from_numpy(Yscaler.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_element = torch.tensor(A_element, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1 = X_train_1.to(device)\n",
    "Y_train_1 = Y_train_1.to(device)\n",
    "X_val_1 = X_val_1.to(device)\n",
    "Y_val_1 = Y_val_1.to(device)\n",
    "\n",
    "Xscaler_mean = Xscaler_mean.to(device)\n",
    "Xscaler_std = Xscaler_std.to(device)\n",
    "\n",
    "Yscaler_mean = Yscaler_mean.to(device)\n",
    "Yscaler_std = Yscaler_std.to(device)\n",
    "\n",
    "A_element = A_element.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now can generate the model. In this work, we will consider a simple Multi Layer Perceptron (MLP). We generate the model using Pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_func = nn.GELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChemNN_multi(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(n_in + 1, 100)\n",
    "        self.act1 = act_func()\n",
    "        self.hidden2 = nn.Linear(100, 50)\n",
    "        self.act2 = act_func()\n",
    "        self.hidden3 = nn.Linear(50, 50)\n",
    "        self.act3 = act_func()\n",
    "        self.output = nn.Linear(50, n_out)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.hidden1(x))\n",
    "        x = self.act2(self.hidden2(x))\n",
    "        x = self.act3(self.hidden3(x))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is then instantiated and transferred to the GPU if present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChemNN_multi()\n",
    "print(model)\n",
    "\n",
    "# Put model on GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Number of model parameters\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of trainable parameters: {pytorch_total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1500\n",
    "batch_size = 2048\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.995)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_training_loop(X_train, X_val, Y_train, Y_val, loss_fn, optimizer, n_epochs, model, log_transform):\n",
    "\n",
    "    # Array to store the loss and validation loss\n",
    "    loss_list = np.empty(n_epochs)\n",
    "    val_loss_list = np.empty(n_epochs//10)\n",
    "\n",
    "    # Array to store sum of mass fractions: mean, min and max\n",
    "    stats_sum_yk = np.empty((n_epochs//10,3))\n",
    "\n",
    "    # Array to store elements conservation: mean, min and max\n",
    "    stats_A_elements = np.empty((n_epochs//10,4,3))\n",
    "\n",
    "    epochs = np.arange(n_epochs)\n",
    "    epochs_small = epochs[::10]\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # Training parameters\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "\n",
    "            Xbatch = X_train[i:i+batch_size]\n",
    "            y_pred = model(Xbatch)\n",
    "            ybatch = Y_train[i:i+batch_size]\n",
    "            loss = loss_fn(y_pred, ybatch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        loss_list[epoch] = loss\n",
    "\n",
    "        before_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        scheduler.step()\n",
    "        after_lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "        # Computing validation loss and mass conservation metric (only every 10 epochs as it is expensive)\n",
    "        if epoch%10==0:\n",
    "            model.eval()  # evaluation mode\n",
    "            with torch.no_grad():\n",
    "\n",
    "                # VALIDATION LOSS\n",
    "                y_val_pred = model(X_val)\n",
    "                val_loss = loss_fn(y_val_pred, Y_val)\n",
    "\n",
    "                # SUM OF MASS FRACTION\n",
    "                #Inverse scale done by hand to stay with Torch arrays\n",
    "                yk = Xscaler_mean[1:] + (Xscaler_std[1:] + 1e-7)*y_val_pred\n",
    "                if log_transform:\n",
    "                    yk = torch.exp(yk)\n",
    "                sum_yk = yk.sum(axis=1)\n",
    "                sum_yk = sum_yk.detach().cpu().numpy()\n",
    "                stats_sum_yk[epoch//10,0] = sum_yk.mean() \n",
    "                stats_sum_yk[epoch//10,1] = sum_yk.min()\n",
    "                stats_sum_yk[epoch//10,2] = sum_yk.max()\n",
    "\n",
    "                # ELEMENTS CONSERVATION\n",
    "                yval_in = Xscaler_mean[1:] + (Xscaler_std[1:] + 1e-7)*X_val[:,1:-1]\n",
    "                if log_transform:\n",
    "                    yval_in = torch.exp(yval_in)\n",
    "                ye_in = torch.matmul(A_element, torch.transpose(yval_in, 0, 1))\n",
    "                ye_out = torch.matmul(A_element, torch.transpose(yk, 0, 1))\n",
    "                delta_ye = (ye_out - ye_in)/(ye_in+1e-10)\n",
    "                delta_ye = delta_ye.detach().cpu().numpy()\n",
    "                stats_A_elements[epoch//10, :, 0] = delta_ye.mean(axis=1)\n",
    "                stats_A_elements[epoch//10, :, 1] = delta_ye.min(axis=1)\n",
    "                stats_A_elements[epoch//10, :, 2] = delta_ye.max(axis=1)\n",
    "\n",
    "            model.train()   # Back to training mode\n",
    "            val_loss_list[epoch//10] = val_loss\n",
    "\n",
    "        print(f\"Finished epoch {epoch}\")\n",
    "        print(f\"    >> lr: {before_lr} -> {after_lr}\")\n",
    "        print(f\"    >> Loss: {loss}\")\n",
    "        if epoch%10==0:\n",
    "            print(f\"    >> Validation loss: {val_loss}\")\n",
    "\n",
    "    return epochs, epochs_small, loss_list, val_loss_list, stats_sum_yk, stats_A_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.perf_counter()\n",
    "epochs, epochs_small, loss_list, val_loss_list, stats_sum_yk, stats_A_elements = main_training_loop(X_train_1, X_val_1, Y_train_1, Y_val_1, loss_fn, optimizer, n_epochs, model, log_transform)\n",
    "end_time = time.perf_counter()\n",
    "print(f\" TRAINING DURATION: {end_time-start_time} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a function to analyze the training. We plot:\n",
    "\n",
    "+ The training and validation losses\n",
    "\n",
    "+ The evolution of $\\sum_{k=1}^{N_S} Y_k$ (mean, min and max).\n",
    "\n",
    "+ The elements conservation by plotting $100\\times\\delta Y_e$ for each element (C, H, O and N). The factor $100$ enables to get an error in \\%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses_conservation(epochs, epochs_small, loss_list, val_loss_list, stats_sum_yk, stats_A_elements):\n",
    "\n",
    "    # LOSSES\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(epochs, loss_list, color=\"k\", label=\"Training\")\n",
    "    ax.plot(epochs_small, val_loss_list, color=\"r\", label = \"Validation\")\n",
    "\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "\n",
    "    # MASS CONSERVATION\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(epochs_small, stats_sum_yk[:,0], color=\"k\")\n",
    "    ax.plot(epochs_small, stats_sum_yk[:,1], color=\"k\", ls=\"--\")\n",
    "    ax.plot(epochs_small, stats_sum_yk[:,2], color=\"k\", ls=\"--\")\n",
    "\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(r\"$\\sum_k \\ Y_k$\")\n",
    "\n",
    "    # ELEMENTS CONSERVATION\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2)\n",
    "\n",
    "    # C\n",
    "    ax1.plot(epochs_small, 100*stats_A_elements[:,0,0], color=\"k\")\n",
    "    ax1.plot(epochs_small, 100*stats_A_elements[:,0,1], color=\"k\", ls=\"--\")\n",
    "    ax1.plot(epochs_small, 100*stats_A_elements[:,0,2], color=\"k\", ls=\"--\")\n",
    "\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(r\"$\\Delta Y_C$ $(\\%$)\")\n",
    "\n",
    "    # H\n",
    "    ax2.plot(epochs_small, 100*stats_A_elements[:,1,0], color=\"k\")\n",
    "    ax2.plot(epochs_small, 100*stats_A_elements[:,1,1], color=\"k\", ls=\"--\")\n",
    "    ax2.plot(epochs_small, 100*stats_A_elements[:,1,2], color=\"k\", ls=\"--\")\n",
    "\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(r\"$\\Delta Y_H$ $(\\%)$\")\n",
    "\n",
    "    # O\n",
    "    ax3.plot(epochs_small, 100*stats_A_elements[:,2,0], color=\"k\")\n",
    "    ax3.plot(epochs_small, 100*stats_A_elements[:,2,1], color=\"k\", ls=\"--\")\n",
    "    ax3.plot(epochs_small, 100*stats_A_elements[:,2,2], color=\"k\", ls=\"--\")\n",
    "\n",
    "    ax3.set_xlabel(\"Epoch\")\n",
    "    ax3.set_ylabel(r\"$\\Delta Y_O$ $(\\%)$\")\n",
    "\n",
    "    # N\n",
    "    ax4.plot(epochs_small, 100*stats_A_elements[:,3,0], color=\"k\")\n",
    "    ax4.plot(epochs_small, 100*stats_A_elements[:,3,1], color=\"k\", ls=\"--\")\n",
    "    ax4.plot(epochs_small, 100*stats_A_elements[:,3,2], color=\"k\", ls=\"--\")\n",
    "\n",
    "    ax4.set_xlabel(\"Epoch\")\n",
    "    ax4.set_ylabel(r\"$\\Delta Y_N$ $(\\%)$\")\n",
    "\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses_conservation(epochs, epochs_small, loss_list, val_loss_list, stats_sum_yk, stats_A_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the Pytorch model in the case folder for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(folder,\"pytorch_mlp.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(os.path.join(folder,\"pytorch_mlp.pt\"), weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the test initial conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim_test = pd.read_csv(os.path.join(folder, \"sim_test.csv\"))\n",
    "\n",
    "n_sim = df_sim_test.shape[0]\n",
    "print(f\"There are {n_sim} test simulations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_simulations(dt):\n",
    "\n",
    "    list_test_results = []\n",
    "\n",
    "    fails = 0\n",
    "    for i, row in df_sim_test.iterrows():\n",
    "\n",
    "        phi_ini = row['Phi']\n",
    "        temperature_ini = row['T0']\n",
    "\n",
    "        print(f\"Performing test computation for phi={phi_ini}; T0={temperature_ini}\")\n",
    "\n",
    "        df_exact, df_nn, fail = compute_nn_cantera_0D_homo(device, model, Xscaler, Yscaler, phi_ini, temperature_ini, dt, dtb_params, A_element.detach().cpu().numpy(), 1, Tscaler, False)\n",
    "\n",
    "        fails += fail\n",
    "\n",
    "        list_test_results.append((df_exact, df_nn))\n",
    "\n",
    "\n",
    "    print(f\"dt={dt}:Total number of simulations which crashed: {fails}\")\n",
    "\n",
    "    return list_test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_list = [0.1e-6, 0.2e-6, 0.3e-6, 0.4e-6, 0.5e-6, 0.6e-6, 0.7e-6, 0.8e-6, 0.9e-6, 1e-6]\n",
    "dict_test_res = {}\n",
    "\n",
    "for dt in dt_list:\n",
    "\n",
    "     print(f\"RUNNING SIMULATIONS FOR DT={dt}\")\n",
    "     dict_test_res[dt] = run_test_simulations(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a function to plot a given simulation, for a given dt: (in *dt_list*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results_sim(i_sim, dt, dict_test_res, spec_to_plot):\n",
    "\n",
    "    df_exact =  dict_test_res[dt][i_sim][0]\n",
    "    df_nn =  dict_test_res[dt][i_sim][1]\n",
    "\n",
    "    # Temperature \n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(df_exact['Time'], df_exact['Temperature'], color='k')\n",
    "    ax.plot(df_nn['Time'], df_nn['Temperature'], color='b')\n",
    "    ax.set_xlabel(\"Time [s]\")\n",
    "    ax.set_ylabel(\"T [K]\")\n",
    "\n",
    "    # Species (normal)\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(df_exact['Time'], df_exact[spec_to_plot], color='k')\n",
    "    ax.plot(df_nn['Time'], df_nn[spec_to_plot], color='b')\n",
    "    ax.set_xlabel(\"Time [s]\")\n",
    "    ax.set_ylabel(f\"{spec_to_plot} [-]\")\n",
    "\n",
    "    # Species (log)\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(df_exact['Time'], np.log(df_exact[spec_to_plot]), color='k')\n",
    "    ax.plot(df_nn['Time'], np.log(df_nn[spec_to_plot]), color='b')\n",
    "    ax.set_xlabel(\"Time [s]\")\n",
    "    ax.set_ylabel(f\"{spec_to_plot} [-]\")\n",
    "\n",
    "    # Sum of Yk\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(df_nn['Time'], df_nn['SumYk'], color='b')\n",
    "    ax.set_xlabel(\"Time [s]\")\n",
    "    ax.set_ylabel(\"$\\sum Y_k$ [-]\")\n",
    "\n",
    "    # Elements\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2)\n",
    "    ax1.plot(df_nn['Time'], df_nn['Y_C'], color='b')\n",
    "    ax2.plot(df_nn['Time'], df_nn['Y_H'], color='b')\n",
    "    ax3.plot(df_nn['Time'], df_nn['Y_O'], color='b')\n",
    "    ax4.plot(df_nn['Time'], df_nn['Y_N'], color='b')\n",
    "    ax1.set_ylabel(\"$Y_C$\")\n",
    "    ax2.set_ylabel(\"$Y_H$\")\n",
    "    ax3.set_ylabel(\"$Y_O$\")\n",
    "    ax4.set_ylabel(\"$Y_N$\")\n",
    "    ax3.set_xlabel(\"Time [s]\")\n",
    "    ax4.set_xlabel(\"Time [s]\")\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 2e-7\n",
    "i_sim = 20\n",
    "spec_to_plot = \"H2O2\"\n",
    "plot_results_sim(i_sim, dt, dict_test_res, spec_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to compute fitness between two simulations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fitness(list_test_results):\n",
    "\n",
    "    # Results will be stored in data_errors array.\n",
    "    # The first column corresponds to errors on temperature\n",
    "    # The next n_out columns correspond to errors on species mass fractions\n",
    "    # The last column corresponds to the mean error\n",
    "    data_errors = np.empty([n_sim, n_out+2]) \n",
    "\n",
    "    for i_sim in range(n_sim):\n",
    "\n",
    "        df_exact = list_test_results[i_sim][0]\n",
    "        df_nn = list_test_results[i_sim][1]\n",
    "\n",
    "        # Removing undesired variables\n",
    "        df_exact = df_exact.drop('Time', axis=1)\n",
    "        df_nn = df_nn.drop([\"Time\",\"SumYk\", \"Y_C\", \"Y_H\", \"Y_O\", \"Y_N\"], axis=1)\n",
    "\n",
    "        # Applying log\n",
    "        if log_transform:\n",
    "\n",
    "            df_exact[df_exact < threshold] = threshold\n",
    "            df_nn[df_nn < threshold] = threshold\n",
    "\n",
    "            df_exact.iloc[:, 1:] = np.log(df_exact.iloc[:, 1:])\n",
    "            df_nn.iloc[:, 1:] = np.log(df_nn.iloc[:, 1:])\n",
    "\n",
    "        # Scaling\n",
    "        data_exact_scaled = (df_exact-Xscaler.mean)/(Xscaler.std+1.0e-7)\n",
    "        data_nn_scaled = (df_nn-Xscaler.mean)/(Xscaler.std+1.0e-7)\n",
    "\n",
    "        diff_exact_nn = np.abs((data_nn_scaled-data_exact_scaled)/data_exact_scaled)\n",
    "\n",
    "        diff_exact_nn = diff_exact_nn.mean(axis=0)\n",
    "\n",
    "        M = diff_exact_nn.mean()\n",
    "\n",
    "        print(f\"Simulation {i_sim} error M = {M}\")\n",
    "\n",
    "        data_errors[i_sim, :n_out+1] = diff_exact_nn\n",
    "        data_errors[i_sim, n_out+1] = M\n",
    "\n",
    "\n",
    "    return data_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_errors = {}\n",
    "for dt in dt_list:\n",
    "    data_errors[dt] = compute_fitness(dict_test_res[dt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the mean and std error for each dt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_errors_mean = np.empty(len(dt_list))\n",
    "data_errors_std = np.empty(len(dt_list))\n",
    "\n",
    "for i, dt in enumerate(dt_list):\n",
    "    data_errors_mean[i] = data_errors[dt][:,-1].mean()\n",
    "    data_errors_std[i] = data_errors[dt][:,-1].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(dt_list, data_errors_mean, color=\"k\", marker=\"o\")\n",
    "\n",
    "ax.set_xlabel(\"dt [s]\", fontsize=14)\n",
    "ax.set_ylabel(\"Error [%]\", fontsize=14)\n",
    "\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
