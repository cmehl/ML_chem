{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OD reactors database generation\n",
    "\n",
    "In this notebook we will generate the database which will be used to train the neural networks. In this case, we consider 0D homogeneous reactors which, given an initial state, will ignite and reach a thermodynamical equilibrium (burned gas state). Each reactor is here parametrized by:\n",
    "\n",
    "+ Initial temperature $T_0$\n",
    "+ Equivalence ratio $\\phi$ (constant throughout the simulation as there is no mixing)\n",
    "\n",
    "The pressure will be assumed constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from chem_ai.database_homo import Database_HomoReac\n",
    "from chem_ai.utils import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0D reactors simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform first the 0D reactors simulations in order to populate the database. A series of reactors are computed and results are stored in a dataframe. These reactors are solved here using the [CANTERA software](https://cantera.org/).\n",
    "\n",
    "Some parameters are first set:\n",
    "\n",
    "+ **folder**: folder path where the databases and models will be stored.\n",
    "+ **p**: pressure, constant in this exercice.\n",
    "+ **phi_bounds**: equivalence ratio bounds (min/max) for the initial conditions sampling.\n",
    "+ **T0_bounds**: temperature bounds (min/max) for the initial conditions sampling.\n",
    "+ **n_samples**: number of initial conditions sampled in the given bounds.\n",
    "+ **fuel**: fuel species.\n",
    "+ **mech_file**: yaml file for the CANTERA mechanism. Some mechanism are provided in *data/mechanisms*.\n",
    "+ **solve_mode**: sets the chemical space sampling method, either using fixed time increments (*dt_cfd*) or increments based on solver (*dt_cvode*). In the second case, values $Y_k(t+dt)$ will need to be recomputed later for each point.\n",
    "+ **max_sim_time**: limit time for a given 0D reactor to avoid problems with stopping criterion.\n",
    "+ **dt**: time step value used for the ML database; and if *dt_cfd* method is chosen it corresponds to the actual *dt* in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"case_0D_\" + \"highT\"\n",
    "\n",
    "#Sampling parameters\n",
    "p = 101325.0\n",
    "phi_bounds = (0.8, 1.0)\n",
    "T0_bounds = (2100.0, 2200.0)\n",
    "n_samples = 500\n",
    "\n",
    "# Chemistry parameters\n",
    "fuel = \"H2\"\n",
    "mech_file = \"/work/mehlc/Lecture_IA_chem_accel/chem_AI_project/data/mechanisms/mech_h2.yaml\"\n",
    "\n",
    "# Simulations parameters\n",
    "solve_mode = \"dt_cfd\"   # dt_cvode or dt_cfd\n",
    "max_sim_time = 10.0e-3\n",
    "dt = 1.0e-6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class *Database_HomoReac* includes functions to compute the reactors and to build the training and validation datasets $(X_{train}, Y_{train})$ and $(X_{val}, Y_{val})$.\n",
    "\n",
    "We instantiate the database using the parameters defined above. At instantation (in the *init* function of the class), several operations are performed:\n",
    "\n",
    "+ A design of Experiment is done: a set of *n_samples* $(T_0,\\phi)$ couples is selected based on Latin Hypercube Sampling (LHS)\n",
    "+ The corresponding 0D reactors are run and the results are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtb = Database_HomoReac(mech_file, fuel, folder, p, phi_bounds, T0_bounds, n_samples, dt, max_sim_time, solve_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The design of experiments is stored in the *df_ODE* pandas dataframe, and each conditions is assinged a simulation number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtb.df_ODE.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the 0D reactors simulations are concatenated in a pandas dataframe named *data_simu*. The necessary data for characterizing states ($T$, $p$ and $Y_k$) are included in the table, as well as the simulation number it sorrespond to and the simulation time. Attribution of states to simulation will be used to generate the train/validation/test databases.\n",
    "\n",
    "This dataframe is solved in the **folder** ad *0D_runs.csv* and may be read in a separate notebook if we need.\n",
    "\n",
    "We can visualize the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtb.data_simu.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercice 1*: In this exercice we will get more familiar with the functions we want to reproduce with AI.\n",
    "\n",
    "1) plot all the species mass fractions (on the same figure) for a given simulation. You can withdraw $N_2$ as it is constant.\n",
    "\n",
    "2) Repeat the same task but by taking the logarithm of species mass fractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation number\n",
    "i = 10\n",
    "\n",
    "simu_i = dtb.data_simu[dtb.data_simu[\"Simulation number\"]==i]\n",
    "\n",
    "cols_species = simu_i.columns[2:-2]\n",
    "cols_species = cols_species.drop(\"N2\")\n",
    "simu_i.set_index(\"Time\")[cols_species].plot()\n",
    "plt.xlim([0, 2e-5])\n",
    "\n",
    "simu_i_log = simu_i.copy()\n",
    "simu_i_log[cols_species] = np.log(simu_i[cols_species])\n",
    "simu_i_log.set_index(\"Time\")[cols_species].plot()\n",
    "plt.xlim([0, 2e-5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/validation/test datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the simulations dataset, we need to split it into training/validation/test datasets to be able to carry on with ML training. A first option would be to take the *data_simu* array and select training/validation/test pointwise. In this specific 0D reactor case however, we prefer to split the dataset on a simulation basis. That is, we split the *n_samples* simulations into *n_training*, *n_validation* and *n_test* simulations for training, validation and test respectively.\n",
    "\n",
    "We first set the ratios of simulations selected for validation and training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_valid = 0.15\n",
    "ratio_test = 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function *generate_train_valid_test*, part of the *Database_HomoReac* class, performs the following operations:\n",
    "\n",
    "+ It splits the *n_samples* simulations into training, validation and test, as explained above.\n",
    "\n",
    "+ It generates arrays of chemical states for training and validation: $(X_{train}, Y_{train})$ and $(X_{val}, Y_{val})$. The arrays for testing are not generated, we only save the initial conditions for the testing simulations and rerun them at testing time.\n",
    "\n",
    "In the case of the *dt_cfd* solving, the ML time step corresponds to the time step in the *data_simu* array. Therefore, building the *Y* array can be done by simply shifting *Y* by one step. Conversely, for the *dt_cvode* sampling, the states corresponding to *t+dt* must be computed for each point, and the routine might take a bit longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtb.generate_train_valid_test(ratio_valid, ratio_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selected simulations for train/validation/test can be accessed through *dtb.df_ODE_train*, *dtb.df_ODE_val*, *dtb.df_ODE_test*; and are also saved in csv files in the case folder. Test simulations conditions will in particular be reused later for testing the ML model.\n",
    "\n",
    "The database are also stored in arrayx *dtb.X_train*, *dtb.Y_train*, *dtb.X_val* and *dtb.X_val*; and save in files for later use if necessary. We can have a look at *X_train* and *Y_train* for instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtb.X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtb.Y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only the variables used as inputs/outputs are present in these dataframes. Unused variables, such as pressure, simulation time and number have been discarded. Note that the temperature is present as an input but not as an output. This is because here we will estimate the output temperature based on the conservation of enthalpy. It is a cheap and straightforward way to guarantee at least that energy is preserved and not let the ML algorithm choose for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML database preprocessing\n",
    "\n",
    "By analyzing the profiles of $Y_k$ for 0D reactors we can spot at least two issues (see exercice 1 above):\n",
    "\n",
    "1. The order of magnitude of species is very different: major species are of the order of $0.1$ while minor species can be $10^{-3}$-$10^{-2}$ and even much less for larger mechanisms. This will pose problems for learning, as the loss function will be dominated by major species.\n",
    "2. The evolution of species mass fractions is very slow before ignition timing (sudden increase of species and temperature). Predicting such low reaction rate can also be an issue for ML algorithms.\n",
    "\n",
    "To tackles these issues, two methods can be considered:\n",
    "\n",
    "1. **Data scaling/normalization**: using knowledge of the data, we can normalize so that each species has an order of magnitude $\\approx 1$. This will eliminate differences in order of magnitude.\n",
    "2. **Data transformation**: as shown in exercice 1, applying a logarithm function ot the data leads to a linear evolution of $Y_k$'s in the first instants. This is more suitable for learning.\n",
    "\n",
    "For the current database, we will first apply an (optional) logarithm transform and then a standard scaler.\n",
    "\n",
    "We set a flag to decide if we apply the logarithm or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_transform = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We devine new arrays which will contain the processed database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_p = dtb.X_train.copy()\n",
    "Y_train_p = dtb.Y_train.copy()\n",
    "X_val_p = dtb.X_val.copy()\n",
    "Y_val_p = dtb.Y_val.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue with the logarithm is that we need to apply it to strictly positive data. We then need to clip the data to a certain threshold in order to guarantee that no NaN's will appear and corrupt the data. This threshold value is arbitrary and certainly has an impact on the learning. \n",
    "\n",
    "*Exercice 2:* Set the threshold on the trainign and validation databases and apply the logarithm. For $X$, the logarithm should be applied on species mass fractions only, not on temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1.0e-10\n",
    "if log_transform:\n",
    "    X_train_p[X_train_p < threshold] = threshold\n",
    "    X_val_p[X_val_p < threshold] = threshold\n",
    "    #\n",
    "    Y_train_p[Y_train_p < threshold] = threshold\n",
    "    Y_val_p[Y_val_p < threshold] = threshold\n",
    "\n",
    "    # Apply log\n",
    "    X_train_p.iloc[:, 1:] = np.log(X_train_p.iloc[:, 1:])\n",
    "    X_val_p.iloc[:, 1:] = np.log(X_val_p.iloc[:, 1:])\n",
    "    #\n",
    "    Y_train_p = np.log(Y_train_p)\n",
    "    Y_val_p = np.log(Y_val_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the present work, we select the standard scaler. For a given variable $\\psi$, the normalization is:\n",
    "\n",
    "$$\n",
    "\\psi^n = \\frac{\\psi - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "where $\\mu$ is the mean of $\\psi$ and $\\sigma$ its standard deviation. The inverse transform reads:\n",
    "\n",
    "$$\n",
    "\\psi = \\sigma \\psi^n + \\mu\n",
    "$$\n",
    "\n",
    "A function *StandardScaler* is given in the *utils.py* script. It features three functions:\n",
    "\n",
    "+ *fit*: compute mean and standard deviation.\n",
    "+ *transform*: performs the transform.\n",
    "+ *inverse_transform*: performes the inverse transform.\n",
    "\n",
    "*Exercice 3*: Perform the scaling of the data, by using training data to set the parameters and applying it on both train and validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xscaler = StandardScaler()\n",
    "Xscaler.fit(X_train_p)\n",
    "X_train_p = Xscaler.transform(X_train_p)\n",
    "X_val_p = Xscaler.transform(X_val_p)\n",
    "\n",
    "Yscaler = StandardScaler()\n",
    "Yscaler.fit(Y_train_p)\n",
    "Y_train_p = Yscaler.transform(Y_train_p)\n",
    "Y_val_p = Yscaler.transform(Y_val_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a look at the training database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_p.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the scalers for later use in training routine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_p = os.path.join(folder,\"processed_database\")\n",
    "if not os.path.isdir(folder_p):\n",
    "    os.mkdir(folder_p)\n",
    "\n",
    "joblib.dump(Xscaler, os.path.join(folder_p,'Xscaler.pkl'))\n",
    "joblib.dump(Yscaler, os.path.join(folder_p,'Yscaler.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the processed training and validation databases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_p.to_csv(os.path.join(folder_p,\"X_train.csv\"), index=False)\n",
    "Y_train_p.to_csv(os.path.join(folder_p,\"Y_train.csv\"), index=False)\n",
    "X_val_p.to_csv(os.path.join(folder_p,\"X_val.csv\"), index=False)\n",
    "Y_val_p.to_csv(os.path.join(folder_p,\"Y_val.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save some parameters in a json file for later use in the ML learning notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        \"fuel\": fuel,\n",
    "        \"mech_file\": mech_file,\n",
    "        \"log_transform\": log_transform,\n",
    "        \"threshold\": threshold,\n",
    "        \"p\": p,\n",
    "        \"dt\": dt,\n",
    "        }\n",
    "\n",
    "# Save to file\n",
    "with open(os.path.join(folder, \"dtb_params.json\"), \"w\") as file:\n",
    "    json.dump(params, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
