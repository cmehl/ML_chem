{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning applied to HFRD flamelet database\n",
    "\n",
    "In this notebook, we will train neural networks to replace the CVODE solver of CANTERA. We will use the databases generated in the *Flamelet_database_generation.ipynb* notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_colab = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google colab preparation\n",
    "\n",
    "These lines are here to enable Colab running of the tools. We need to perform a git clone in order to have access to python scripts. This needs to be done at each runtime as the clone is lost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if use_colab:\n",
    "    !git clone -b cost_course_exercices https://github.com/cmehl/ML_chem.git\n",
    "    \n",
    "    !pip install cantera\n",
    "\n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Create a folder in the root directory\n",
    "    if not os.path.isdir(\"/content/drive/MyDrive/ML_chem_data\"):\n",
    "        !mkdir -p \"/content/drive/MyDrive/ML_chem_data\"\n",
    "    else:\n",
    "        print(\"Folder /content/drive/MyDrive/ML_chem_data already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import joblib\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import cantera as ct\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(\"notebook\")\n",
    "\n",
    "if use_colab:\n",
    "    from ML_chem.chem_ai.cantera_runs_clustering import compute_nn_cantera_0D_homo, compute_nn_cantera_1D_prem, compute_nn_cantera_1D_diff\n",
    "    from ML_chem.chem_ai.utils import get_molar_mass_atomic_matrix\n",
    "    from ML_chem.chem_ai.utils import StandardScaler\n",
    "else:\n",
    "    from chem_ai.cantera_runs_clustering import compute_nn_cantera_0D_homo, compute_nn_cantera_1D_prem, compute_nn_cantera_1D_diff\n",
    "    from chem_ai.utils import get_molar_mass_atomic_matrix\n",
    "    from chem_ai.utils import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the default pytorch precision to double. It slows down a little bit the training but it is the usual standard for CFD reacting flows applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We identify the device (CPU or GPU) available on the machine. This will be used by pytorch to identify the device on which to train and use the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda:0')\n",
    "  print('Running on the GPU')\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "  print('Running on the CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary\n",
    "\n",
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_colab:\n",
    "    folder = \"/content/drive/MyDrive/ML_chem_data/case_0D_highT\"\n",
    "else:\n",
    "    folder = \"./case_multi_test_case_flamelets\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(folder, \"dtb_params.json\"), \"r\") as file:\n",
    "    dtb_params = json.load(file)\n",
    "\n",
    "fuel = dtb_params[\"fuel\"]\n",
    "mech_file = dtb_params[\"mech_file\"]\n",
    "log_transform = dtb_params[\"log_transform\"]\n",
    "threshold = dtb_params[\"threshold\"]\n",
    "p = dtb_params[\"p\"]\n",
    "dt = dtb_params[\"dt\"]\n",
    "n_clusters = dtb_params[\"n_clusters\"]\n",
    "\n",
    "\n",
    "print(f\"fuel = {fuel}\")\n",
    "print(f\"mech_file = {mech_file}\")\n",
    "print(f\"log_transform = {log_transform}\")\n",
    "print(f\"threshold = {threshold}\")\n",
    "print(f\"p = {p}\")\n",
    "print(f\"dt = {dt}\")\n",
    "print(f\"n_clusters = {n_clusters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(i_cluster):\n",
    "\n",
    "    Xscaler = joblib.load(os.path.join(folder, f\"processed_database_cluster/cluster_{i_cluster}\", \"Xscaler.pkl\"))\n",
    "    Yscaler = joblib.load(os.path.join(folder, f\"processed_database_cluster/cluster_{i_cluster}\", \"Yscaler.pkl\"))\n",
    "\n",
    "    X_train = pd.read_csv(os.path.join(folder, f\"processed_database_cluster/cluster_{i_cluster}\",\"X_train.csv\"))\n",
    "    X_val = pd.read_csv(os.path.join(folder, f\"processed_database_cluster/cluster_{i_cluster}\",\"X_val.csv\"))\n",
    "    Y_train = pd.read_csv(os.path.join(folder, f\"processed_database_cluster/cluster_{i_cluster}\",\"Y_train.csv\"))\n",
    "    Y_val = pd.read_csv(os.path.join(folder, f\"processed_database_cluster/cluster_{i_cluster}\",\"Y_val.csv\"))\n",
    "\n",
    "    return Xscaler, Yscaler, X_train, X_val, Y_train, Y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the first cluster (necessarily present) to get some needed information for designing the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, X_train, _, Y_train, _ = load_data(0)\n",
    "\n",
    "# Additional variable needed\n",
    "Xcols = X_train.columns\n",
    "Ycols = Y_train.columns\n",
    "\n",
    "n_in = X_train.shape[1]\n",
    "n_out = Y_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elements conservation matrix\n",
    "\n",
    "See notebook *0D_ann_learning.ipynb* to get details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gas = ct.Solution(mech_file)\n",
    "A_element = get_molar_mass_atomic_matrix(gas.species_names, fuel, True)\n",
    "print(A_element)\n",
    "\n",
    "A_element = torch.tensor(A_element, dtype=torch.float64)\n",
    "A_element = A_element.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN models training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_training_loop(X_train, X_val, Y_train, Y_val, loss_fn, optimizer, n_epochs, batch_size, model, log_transform, Xscaler_mean, Yscaler_mean, Xscaler_std, Yscaler_std):\n",
    "\n",
    "    # Array to store the loss and validation loss\n",
    "    loss_list = np.empty(n_epochs)\n",
    "    val_loss_list = np.empty(n_epochs//10)\n",
    "\n",
    "    # Array to store sum of mass fractions: mean, min and max\n",
    "    stats_sum_yk = np.empty((n_epochs//10,3))\n",
    "\n",
    "    # Array to store elements conservation: mean, min and max\n",
    "    stats_A_elements = np.empty((n_epochs//10,4,3))\n",
    "\n",
    "    epochs = np.arange(n_epochs)\n",
    "    epochs_small = epochs[::10]\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # Training parameters\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "\n",
    "            Xbatch = X_train[i:i+batch_size]\n",
    "            y_pred = model(Xbatch)\n",
    "            ybatch = Y_train[i:i+batch_size]\n",
    "            loss = loss_fn(y_pred, ybatch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        loss_list[epoch] = loss\n",
    "\n",
    "        # Computing validation loss and mass conservation metric (only every 10 epochs as it is expensive)\n",
    "        if epoch%10==0:\n",
    "            model.eval()  # evaluation mode\n",
    "            with torch.no_grad():\n",
    "\n",
    "                # VALIDATION LOSS\n",
    "                y_val_pred = model(X_val)\n",
    "                val_loss = loss_fn(y_val_pred, Y_val)\n",
    "\n",
    "                # SUM OF MASS FRACTION\n",
    "                #Inverse scale done by hand to stay with Torch arrays\n",
    "                yk = Yscaler_mean + (Yscaler_std + 1e-7)*y_val_pred\n",
    "                if log_transform:\n",
    "                    yk = torch.exp(yk)\n",
    "                sum_yk = yk.sum(axis=1)\n",
    "                sum_yk = sum_yk.detach().cpu().numpy()\n",
    "                stats_sum_yk[epoch//10,0] = sum_yk.mean() \n",
    "                stats_sum_yk[epoch//10,1] = sum_yk.min()\n",
    "                stats_sum_yk[epoch//10,2] = sum_yk.max()\n",
    "\n",
    "                # ELEMENTS CONSERVATION\n",
    "                yval_in = Xscaler_mean[1:] + (Xscaler_std[1:] + 1e-7)*X_val[:,1:]\n",
    "                if log_transform:\n",
    "                    yval_in = torch.exp(yval_in)\n",
    "                ye_in = torch.matmul(A_element, torch.transpose(yval_in, 0, 1))\n",
    "                ye_out = torch.matmul(A_element, torch.transpose(yk, 0, 1))\n",
    "                delta_ye = (ye_out - ye_in)/(ye_in+1e-10)\n",
    "                delta_ye = delta_ye.detach().cpu().numpy()\n",
    "                stats_A_elements[epoch//10, :, 0] = delta_ye.mean(axis=1)\n",
    "                stats_A_elements[epoch//10, :, 1] = delta_ye.min(axis=1)\n",
    "                stats_A_elements[epoch//10, :, 2] = delta_ye.max(axis=1)\n",
    "\n",
    "            model.train()   # Back to training mode\n",
    "            val_loss_list[epoch//10] = val_loss\n",
    "\n",
    "        print(f\"Finished epoch {epoch}\")\n",
    "        print(f\"    >> Loss: {loss}\")\n",
    "        if epoch%10==0:\n",
    "            print(f\"    >> Validation loss: {val_loss}\")\n",
    "\n",
    "    return epochs, epochs_small, loss_list, val_loss_list, stats_sum_yk, stats_A_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Routine to plot training & validation errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses_conservation(model_folder_i, epochs, epochs_small, loss_list, val_loss_list, stats_sum_yk, stats_A_elements):\n",
    "\n",
    "    # LOSSES\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(epochs, loss_list, color=\"k\", label=\"Training\")\n",
    "    ax.plot(epochs_small, val_loss_list, color=\"r\", label = \"Validation\")\n",
    "\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    fig.savefig(os.path.join(model_folder_i, \"loss.png\"))\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "    # MASS CONSERVATION\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(epochs_small, stats_sum_yk[:,0], color=\"k\")\n",
    "    ax.plot(epochs_small, stats_sum_yk[:,1], color=\"k\", ls=\"--\")\n",
    "    ax.plot(epochs_small, stats_sum_yk[:,2], color=\"k\", ls=\"--\")\n",
    "\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(r\"$\\sum_k \\ Y_k$\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    fig.savefig(os.path.join(model_folder_i, \"sumYk.png\"))\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "    # ELEMENTS CONSERVATION\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2)\n",
    "\n",
    "    # C\n",
    "    ax1.plot(epochs_small, 100*stats_A_elements[:,0,0], color=\"k\")\n",
    "    ax1.plot(epochs_small, 100*stats_A_elements[:,0,1], color=\"k\", ls=\"--\")\n",
    "    ax1.plot(epochs_small, 100*stats_A_elements[:,0,2], color=\"k\", ls=\"--\")\n",
    "\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(r\"$\\Delta Y_C$ $(\\%$)\")\n",
    "\n",
    "    # H\n",
    "    ax2.plot(epochs_small, 100*stats_A_elements[:,1,0], color=\"k\")\n",
    "    ax2.plot(epochs_small, 100*stats_A_elements[:,1,1], color=\"k\", ls=\"--\")\n",
    "    ax2.plot(epochs_small, 100*stats_A_elements[:,1,2], color=\"k\", ls=\"--\")\n",
    "\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(r\"$\\Delta Y_H$ $(\\%)$\")\n",
    "\n",
    "    # O\n",
    "    ax3.plot(epochs_small, 100*stats_A_elements[:,2,0], color=\"k\")\n",
    "    ax3.plot(epochs_small, 100*stats_A_elements[:,2,1], color=\"k\", ls=\"--\")\n",
    "    ax3.plot(epochs_small, 100*stats_A_elements[:,2,2], color=\"k\", ls=\"--\")\n",
    "\n",
    "    ax3.set_xlabel(\"Epoch\")\n",
    "    ax3.set_ylabel(r\"$\\Delta Y_O$ $(\\%)$\")\n",
    "\n",
    "    # N\n",
    "    ax4.plot(epochs_small, 100*stats_A_elements[:,3,0], color=\"k\")\n",
    "    ax4.plot(epochs_small, 100*stats_A_elements[:,3,1], color=\"k\", ls=\"--\")\n",
    "    ax4.plot(epochs_small, 100*stats_A_elements[:,3,2], color=\"k\", ls=\"--\")\n",
    "\n",
    "    ax4.set_xlabel(\"Epoch\")\n",
    "    ax4.set_ylabel(r\"$\\Delta Y_N$ $(\\%)$\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    fig.savefig(os.path.join(model_folder_i, \"elements.png\"))\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training hyperparameters\n",
    "\n",
    "The hyper-parameters are given in lists. Here we set everything manually, we need thus to know in advance the number of clusters and their size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs_list = [1000, 1000, 200, 200]\n",
    "batch_size_list = [256, 256, 256, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChemNN_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(n_in, 60)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.hidden2 = nn.Linear(60, 60)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.output = nn.Linear(60, n_out)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.hidden1(x))\n",
    "        x = self.act2(self.hidden2(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class ChemNN_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(n_in, 60)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.hidden2 = nn.Linear(60, 60)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.output = nn.Linear(60, n_out)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.hidden1(x))\n",
    "        x = self.act2(self.hidden2(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class ChemNN_3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(n_in, 60)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.hidden2 = nn.Linear(60, 60)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.output = nn.Linear(60, n_out)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.hidden1(x))\n",
    "        x = self.act2(self.hidden2(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class ChemNN_4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(n_in, 60)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.hidden2 = nn.Linear(60, 60)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.output = nn.Linear(60, n_out)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.hidden1(x))\n",
    "        x = self.act2(self.hidden2(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "model_list = [ChemNN_1(), ChemNN_2(), ChemNN_3(), ChemNN_4()]\n",
    "\n",
    "for model in model_list:\n",
    "    model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main training loop on clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating main folder to store models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = os.path.join(folder, \"ann_models\")\n",
    "if not os.path.isdir(model_folder):\n",
    "    os.mkdir(model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_cluster in range(n_clusters):\n",
    "\n",
    "    print(\"-------------------------------------------\")\n",
    "    print(f\"         TRAINING MODEL {i_cluster}       \")\n",
    "    print(\"-------------------------------------------\")\n",
    "\n",
    "    model_folder_i = os.path.join(folder, f\"ann_models/cluster_{i_cluster}\")\n",
    "    if not os.path.isdir(model_folder_i):\n",
    "        os.mkdir(model_folder_i)\n",
    "\n",
    "\n",
    "    # Cluster hyperparameters\n",
    "    model = model_list[i_cluster]\n",
    "    n_epochs = n_epochs_list[i_cluster]\n",
    "    batch_size = batch_size_list[i_cluster]\n",
    "    \n",
    "    # Load the data\n",
    "    Xscaler, Yscaler, X_train, X_val, Y_train, Y_val = load_data(i_cluster)\n",
    "\n",
    "    X_train = torch.tensor(X_train.values, dtype=torch.float64)\n",
    "    Y_train = torch.tensor(Y_train.values, dtype=torch.float64)\n",
    "    X_val = torch.tensor(X_val.values, dtype=torch.float64)\n",
    "    Y_val = torch.tensor(Y_val.values, dtype=torch.float64)\n",
    "\n",
    "    # Converting to torch arrays\n",
    "    Xscaler_mean = torch.from_numpy(Xscaler.mean.values)\n",
    "    Xscaler_std = torch.from_numpy(Xscaler.std.values)\n",
    "\n",
    "    Yscaler_mean = torch.from_numpy(Yscaler.mean.values)\n",
    "    Yscaler_std = torch.from_numpy(Yscaler.std.values)\n",
    "\n",
    "    # Moving to correct device\n",
    "    X_train = X_train.to(device)\n",
    "    Y_train = Y_train.to(device)\n",
    "    X_val = X_val.to(device)\n",
    "    Y_val = Y_val.to(device)\n",
    "\n",
    "    Xscaler_mean = Xscaler_mean.to(device)\n",
    "    Xscaler_std = Xscaler_std.to(device)\n",
    "\n",
    "    Yscaler_mean = Yscaler_mean.to(device)\n",
    "    Yscaler_std = Yscaler_std.to(device)\n",
    "\n",
    "    # Loss \n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    epochs, epochs_small, loss_list, val_loss_list, stats_sum_yk, stats_A_elements = main_training_loop(X_train, X_val, Y_train, Y_val, loss_fn, optimizer, \n",
    "                                                                                                        n_epochs, batch_size, model, log_transform, \n",
    "                                                                                                        Xscaler_mean, Yscaler_mean, Xscaler_std, Yscaler_std)\n",
    "\n",
    "\n",
    "    # Plotting losses and conservation metrics\n",
    "    plot_losses_conservation(model_folder_i, epochs, epochs_small, loss_list, val_loss_list, stats_sum_yk, stats_A_elements)\n",
    "\n",
    "    # Saving model\n",
    "    torch.save(model.state_dict(), os.path.join(model_folder_i,\"pytorch_mlp.pt\"))                                                                                                \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN models testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading k-means algorithm and scaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(folder, \"processed_database_cluster/kmeans_model.pkl\"), \"rb\") as f:\n",
    "    kmeans = pickle.load(f)\n",
    "\n",
    "kmeans_scaler = joblib.load(os.path.join(folder, \"processed_database_cluster/Xscaler_kmeans.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to plot individual functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results_sim_0D(i_sim, list_test_results, spec_to_plot):\n",
    "\n",
    "    df_exact = list_test_results[i_sim][0]\n",
    "    df_nn = list_test_results[i_sim][1]\n",
    "\n",
    "    # Temperature \n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(df_exact['Time'], df_exact['Temperature'], color='k')\n",
    "    ax.plot(df_nn['Time'], df_nn['Temperature'], color='b')\n",
    "    ax.set_xlabel(\"Time [s]\")\n",
    "    ax.set_ylabel(\"T [K]\")\n",
    "\n",
    "    # Species (normal)\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(df_exact['Time'], df_exact[spec_to_plot], color='k')\n",
    "    ax.plot(df_nn['Time'], df_nn[spec_to_plot], color='b')\n",
    "    ax.set_xlabel(\"Time [s]\")\n",
    "    ax.set_ylabel(f\"{spec_to_plot} [-]\")\n",
    "\n",
    "    # Species (log)\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(df_exact['Time'], np.log(df_exact[spec_to_plot]), color='k')\n",
    "    ax.plot(df_nn['Time'], np.log(df_nn[spec_to_plot]), color='b')\n",
    "    ax.set_xlabel(\"Time [s]\")\n",
    "    ax.set_ylabel(f\"{spec_to_plot} [-]\")\n",
    "\n",
    "    # Sum of Yk\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(df_nn['Time'], df_nn['SumYk'], color='b')\n",
    "    ax.set_xlabel(\"Time [s]\")\n",
    "    ax.set_ylabel(\"$\\sum Y_k$ [-]\")\n",
    "\n",
    "    # Elements\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2)\n",
    "    ax1.plot(df_nn['Time'], df_nn['Y_C'], color='b')\n",
    "    ax2.plot(df_nn['Time'], df_nn['Y_H'], color='b')\n",
    "    ax3.plot(df_nn['Time'], df_nn['Y_O'], color='b')\n",
    "    ax4.plot(df_nn['Time'], df_nn['Y_N'], color='b')\n",
    "    ax1.set_ylabel(\"$Y_C$\")\n",
    "    ax2.set_ylabel(\"$Y_H$\")\n",
    "    ax3.set_ylabel(\"$Y_O$\")\n",
    "    ax4.set_ylabel(\"$Y_N$\")\n",
    "    ax3.set_xlabel(\"Time [s]\")\n",
    "    ax4.set_xlabel(\"Time [s]\")\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results_sim_1D(i_sim, list_test_results, spec_to_plot):\n",
    "\n",
    "    df_exact = list_test_results[i_sim][0]\n",
    "    df_nn = list_test_results[i_sim][1]\n",
    "\n",
    "    # Species (normal)\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(df_exact['X'], df_exact[spec_to_plot], color='k')\n",
    "    ax.plot(df_nn['X'], df_nn[spec_to_plot], color='b')\n",
    "    ax.set_xlabel(\"x [m]\")\n",
    "    ax.set_ylabel(f\"{spec_to_plot} [-]\")\n",
    "\n",
    "    # Sum of Yk\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(df_nn['X'], df_nn['SumOmegak'], color='b')\n",
    "    ax.set_xlabel(\"x [m]\")\n",
    "    ax.set_ylabel(\"$\\sum \\dot{\\omega}_k$ [-]\")\n",
    "\n",
    "    # Elements\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2)\n",
    "    ax1.plot(df_nn['X'], df_nn['Omega_C'], color='b')\n",
    "    ax2.plot(df_nn['X'], df_nn['Omega_H'], color='b')\n",
    "    ax3.plot(df_nn['X'], df_nn['Omega_O'], color='b')\n",
    "    ax4.plot(df_nn['X'], df_nn['Omega_N'], color='b')\n",
    "    ax1.set_ylabel(\"$\\dot{\\omega}_C$\")\n",
    "    ax2.set_ylabel(\"$\\dot{\\omega}_H$\")\n",
    "    ax3.set_ylabel(\"$\\dot{\\omega}_O$\")\n",
    "    ax4.set_ylabel(\"$\\dot{\\omega}_N$\")\n",
    "    ax3.set_xlabel(\"x [m]\")\n",
    "    ax4.set_xlabel(\"x [m]\")\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on 0D reactors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the test initial conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim_test_0D = pd.read_csv(os.path.join(folder, \"sim_test_0D.csv\"))\n",
    "\n",
    "n_sim_0D = df_sim_test_0D.shape[0]\n",
    "print(f\"There are {n_sim_0D} 0D test simulations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_test_results_0D = []\n",
    "\n",
    "fails = 0\n",
    "for i, row in df_sim_test_0D.iterrows():\n",
    "\n",
    "    phi_ini = row['Phi']\n",
    "    temperature_ini = row['T0']\n",
    "\n",
    "    print(f\"Performing test computation for phi={phi_ini}; T0={temperature_ini}\")\n",
    "\n",
    "    df_exact, df_nn, fail = compute_nn_cantera_0D_homo(device, kmeans, kmeans_scaler, model_list, Xscaler, Yscaler, phi_ini, temperature_ini, dt, dtb_params, A_element.detach().cpu().numpy())\n",
    "\n",
    "    fails += fail\n",
    "\n",
    "    list_test_results_0D.append((df_exact, df_nn))\n",
    "\n",
    "\n",
    "print(f\"Total number of simulations which crashed: {fails}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_sim = 9\n",
    "spec_to_plot = \"H2O2\"\n",
    "plot_results_sim_0D(i_sim, list_test_results_0D, spec_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on 1D freely propagating premixed flames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim_test_1D_prem = pd.read_csv(os.path.join(folder, \"sim_test_1D_prem.csv\"))\n",
    "\n",
    "n_sim_1D_prem = df_sim_test_1D_prem.shape[0]\n",
    "print(f\"There are {n_sim_1D_prem} 1D premixed test simulations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_test_results_1D_prem = []\n",
    "\n",
    "for i, row in df_sim_test_1D_prem.iterrows():\n",
    "\n",
    "    phi_ini = row['Phi']\n",
    "    temperature_ini = row['T0']\n",
    "\n",
    "    print(f\"Performing test computation for phi={phi_ini}; T0={temperature_ini}\")\n",
    "\n",
    "    df_exact, df_nn = compute_nn_cantera_1D_prem(device, kmeans, kmeans_scaler, model_list, Xscaler, Yscaler, phi_ini, temperature_ini, dt, dtb_params, A_element.detach().cpu().numpy())\n",
    "\n",
    "    list_test_results_1D_prem.append((df_exact, df_nn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_sim = 9\n",
    "spec_to_plot = \"OH\"\n",
    "plot_results_sim_1D(i_sim, list_test_results_1D_prem, spec_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on 1D counterflow diffusion flames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim_test_1D_diff = pd.read_csv(os.path.join(folder, \"sim_test_1D_diff.csv\"))\n",
    "\n",
    "n_sim_1D_diff = df_sim_test_1D_diff.shape[0]\n",
    "print(f\"There are {n_sim_1D_diff} 1D CF diffusion test simulations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_test_results_1D_diff = []\n",
    "\n",
    "width = 0.02\n",
    "\n",
    "for i, row in df_sim_test_1D_diff.iterrows():\n",
    "\n",
    "    strain = row['Strain']\n",
    "    T0 = row['T0']\n",
    "\n",
    "    print(f\"Performing test computation for Strain={strain}; T0={T0}\")\n",
    "\n",
    "    df_exact, df_nn = compute_nn_cantera_1D_diff(device, kmeans, kmeans_scaler, model_list, Xscaler, Yscaler, strain, T0, width, dt, dtb_params, A_element.detach().cpu().numpy())\n",
    "\n",
    "    list_test_results_1D_diff.append((df_exact, df_nn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_sim = 10\n",
    "spec_to_plot = \"H2O\"\n",
    "plot_results_sim_1D(i_sim, list_test_results_1D_diff, spec_to_plot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
