{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HFRD data generation method\n",
    "\n",
    "This method is an hybrid Flamelet / random data approach to generate training data for neural networks. The idea is to compute standard flamelets (0D, 1D premixed, 1D diffusion) and augment them using a random based technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Google Colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_colab = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google colab preparation\n",
    "\n",
    "These lines are here to enable Colab running of the tools. We need to perform a git clone in order to have access to python scripts. This needs to be done at each runtime as the clone is lost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if use_colab:\n",
    "    !git clone -b cost_course_exercices https://github.com/cmehl/ML_chem.git\n",
    "    !pip install PyDOE\n",
    "    !pip install cantera\n",
    "\n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Create a folder in the root directory\n",
    "    if not os.path.isdir(\"/content/drive/MyDrive/ML_chem_data\"):\n",
    "        !mkdir -p \"/content/drive/MyDrive/ML_chem_data\"\n",
    "    else:\n",
    "        print(\"Folder /content/drive/MyDrive/ML_chem_data already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if use_colab:\n",
    "    from ML_chem.database_flamelets import DatabaseFlamelets\n",
    "    from ML_chem.chem_ai.utils import StandardScaler\n",
    "else:\n",
    "    from chem_ai.database_flamelets import DatabaseFlamelets\n",
    "    from chem_ai.utils import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flames computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define common parameters to all flames computations, such as fuel, chemical mechanism, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 101325.0\n",
    "\n",
    "fuel = \"H2\"\n",
    "mech_file = \"/work/mehlc/Lecture_IA_chem_accel/chem_AI_project/data/mechanisms/mech_h2.yaml\"\n",
    "\n",
    "folder = \"case_multi_\" + \"test_case_flamelets\"\n",
    "\n",
    "dt_CFD = 1.0e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtb = DatabaseFlamelets(mech_file, fuel, folder, p, dt_CFD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0D reactors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_bounds = (0.8, 1.0)\n",
    "T0_bounds = (1500.0, 1600.0)\n",
    "\n",
    "n_samples = 200\n",
    "\n",
    "max_sim_time = 10.0e-3\n",
    "\n",
    "solve_mode = \"dt_cfd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtb.compute_0d_reactors(phi_bounds, T0_bounds, n_samples, max_sim_time, solve_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D PREMIXED FLAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_bounds = (0.5, 1.0)\n",
    "T0_bounds = (300.0, 400.0)\n",
    "\n",
    "n_samples = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtb.compute_1d_premixed(phi_bounds, T0_bounds, n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D DIFFUSION FLAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strain_bounds = (0., 1000.0)\n",
    "T0_bounds = (300.0, 500.0)\n",
    "\n",
    "n_samples = 200\n",
    "\n",
    "width = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtb.compute_1d_diffusion(strain_bounds, T0_bounds, n_samples, width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing simulations database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtb.df.plot.scatter(x=\"Temperature\", y=\"OH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtb.augment_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "x_var = \"Temperature\"\n",
    "y_var = \"H2O\"\n",
    "\n",
    "if dtb.includes_0d_reactors:\n",
    "    ax.scatter(dtb.df_augmented[x_var][dtb.df_augmented[\"reactor_type\"]==0], dtb.df_augmented[y_var][dtb.df_augmented[\"reactor_type\"]==0], color=\"blue\", alpha=0.2,  s=3)\n",
    "if dtb.includes_1d_prem:\n",
    "    ax.scatter(dtb.df_augmented[x_var][dtb.df_augmented[\"reactor_type\"]==1], dtb.df_augmented[y_var][dtb.df_augmented[\"reactor_type\"]==1], color=\"green\", alpha=0.2,  s=3)\n",
    "if dtb.includes_1d_diff:\n",
    "    ax.scatter(dtb.df_augmented[x_var][dtb.df_augmented[\"reactor_type\"]==2], dtb.df_augmented[y_var][dtb.df_augmented[\"reactor_type\"]==2], color=\"purple\", alpha=0.2,  s=3)\n",
    "\n",
    "if dtb.includes_0d_reactors:\n",
    "    ax.scatter(dtb.df_flamelet[x_var][dtb.df_flamelet[\"reactor_type\"]==0], dtb.df_flamelet[y_var][dtb.df_flamelet[\"reactor_type\"]==0], color=\"blue\", s=3, label=\"0D\")\n",
    "if dtb.includes_1d_prem:\n",
    "    ax.scatter(dtb.df_flamelet[x_var][dtb.df_flamelet[\"reactor_type\"]==1], dtb.df_flamelet[y_var][dtb.df_flamelet[\"reactor_type\"]==1], color=\"green\", s=3, label=\"1D premixed\")\n",
    "if dtb.includes_1d_diff:\n",
    "    ax.scatter(dtb.df_flamelet[x_var][dtb.df_flamelet[\"reactor_type\"]==2], dtb.df_flamelet[y_var][dtb.df_flamelet[\"reactor_type\"]==2], color=\"purple\", s=3, label=\"1D diffusion\")\n",
    "\n",
    "ax.set_xlabel(x_var, fontsize=14)\n",
    "ax.set_ylabel(y_var, fontsize=14)\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtb.save_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of train and test databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "dtb.generate_train_valid_test(valid_ratio, test_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtb.X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtb.Y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing of database\n",
    "\n",
    "Pre-processing is by default made using K-means clustering. If no clustering is needed, we need to set *n_clusters=1*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set a flag to decide if we apply the logarithm or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_transform = True\n",
    "threshold = 1.0e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_p_2 = os.path.join(folder,\"processed_database_cluster\")\n",
    "if not os.path.isdir(folder_p_2):\n",
    "    os.mkdir(folder_p_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform k-means clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_kmeans = dtb.X_train.copy()\n",
    "\n",
    "# We apply log and normalization\n",
    "# Apply threshold if log\n",
    "if log_transform:\n",
    "    X_kmeans[X_kmeans < threshold] = threshold\n",
    "\n",
    "    # Apply log\n",
    "    X_kmeans.iloc[:, 1:] = np.log(X_kmeans.iloc[:, 1:])\n",
    "\n",
    "# Apply scaling\n",
    "Xscaler = StandardScaler()\n",
    "Xscaler.fit(X_kmeans)\n",
    "X_kmeans = Xscaler.transform(X_kmeans)\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(X_kmeans)\n",
    "\n",
    "kmeans_clusters_train = kmeans.labels_\n",
    "\n",
    "# Saving K-means model\n",
    "with open(os.path.join(folder_p_2, \"kmeans_model.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(kmeans, f)\n",
    "\n",
    "# Saving scaler\n",
    "joblib.dump(Xscaler, os.path.join(folder_p_2,\"Xscaler_kmeans.pkl\"))\n",
    "\n",
    "# Saving normalization parameters and centroids\n",
    "np.savetxt(os.path.join(folder_p_2, 'kmeans_norm.dat'), np.vstack([Xscaler.mean, Xscaler.std]).T)\n",
    "np.savetxt(os.path.join(folder_p_2, 'km_centroids.dat'), kmeans.cluster_centers_.T)\n",
    "\n",
    "\n",
    "# Validation data Kmeans\n",
    "X_kmeans_val = dtb.X_val.copy()\n",
    "# Apply threshold if log\n",
    "if log_transform:\n",
    "    X_kmeans_val[X_kmeans_val < threshold] = threshold\n",
    "\n",
    "    # Apply log\n",
    "    X_kmeans_val.iloc[:, 1:] = np.log(X_kmeans_val.iloc[:, 1:])\n",
    "\n",
    "# Apply scaling\n",
    "X_kmeans_val = Xscaler.transform(X_kmeans_val)\n",
    "\n",
    "kmeans_clusters_val = kmeans.predict(X_kmeans_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the log transformation and scaling for each cluster separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_cluster in range(n_clusters):\n",
    "\n",
    "    dtb_folder_i = os.path.join(folder_p_2, f\"cluster_{i_cluster}\")\n",
    "    if not os.path.isdir(dtb_folder_i):\n",
    "        os.mkdir(dtb_folder_i)\n",
    "\n",
    "    Xcols = dtb.X_train.columns\n",
    "    Ycols = dtb.Y_train.columns\n",
    "\n",
    "    # Getting data for cluster\n",
    "    X_train_i = dtb.X_train[kmeans_clusters_train==i_cluster].copy()\n",
    "    Y_train_i = dtb.Y_train[kmeans_clusters_train==i_cluster].copy()\n",
    "    #\n",
    "    X_val_i = dtb.X_val[kmeans_clusters_val==i_cluster].copy()\n",
    "    Y_val_i = dtb.Y_val[kmeans_clusters_val==i_cluster].copy()\n",
    "\n",
    "    print(f\"CLUSTER {i_cluster}\")\n",
    "    print(f\" >> {X_train_i.shape[0]} points in training set\")\n",
    "    print(f\" >> {X_val_i.shape[0]} points in validation set \\n\")\n",
    "\n",
    "    # Apply threshold if log\n",
    "    if log_transform:\n",
    "        X_train_i[X_train_i < threshold] = threshold\n",
    "        X_val_i[X_val_i < threshold] = threshold\n",
    "        #\n",
    "        Y_train_i[Y_train_i < threshold] = threshold\n",
    "        Y_val_i[Y_val_i < threshold] = threshold\n",
    "\n",
    "        # Apply log\n",
    "        X_train_i.iloc[:, 1:] = np.log(X_train_i.iloc[:, 1:])\n",
    "        X_val_i.iloc[:, 1:] = np.log(X_val_i.iloc[:, 1:])\n",
    "        #\n",
    "        Y_train_i = np.log(Y_train_i)\n",
    "        Y_val_i = np.log(Y_val_i)\n",
    "\n",
    "\n",
    "    # Apply scaling\n",
    "    Xscaler = StandardScaler()\n",
    "    Xscaler.fit(X_train_i)\n",
    "    X_train_i = Xscaler.transform(X_train_i)\n",
    "    X_val_i = Xscaler.transform(X_val_i)\n",
    "\n",
    "    Yscaler = StandardScaler()\n",
    "    Yscaler.fit(Y_train_i)\n",
    "    Y_train_i = Yscaler.transform(Y_train_i)\n",
    "    Y_val_i = Yscaler.transform(Y_val_i)\n",
    "\n",
    "    # Saving scalers for later use\n",
    "    joblib.dump(Xscaler, os.path.join(dtb_folder_i,'Xscaler.pkl'))\n",
    "    joblib.dump(Yscaler, os.path.join(dtb_folder_i,'Yscaler.pkl'))\n",
    "\n",
    "\n",
    "    # Saving data (transformed)\n",
    "    X_train_i.to_csv(os.path.join(dtb_folder_i,\"X_train.csv\"), index=False)\n",
    "    Y_train_i.to_csv(os.path.join(dtb_folder_i,\"Y_train.csv\"), index=False)\n",
    "    X_val_i.to_csv(os.path.join(dtb_folder_i,\"X_val.csv\"), index=False)\n",
    "    Y_val_i.to_csv(os.path.join(dtb_folder_i,\"Y_val.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        \"fuel\": fuel,\n",
    "        \"mech_file\": mech_file,\n",
    "        \"log_transform\": log_transform,\n",
    "        \"threshold\": threshold,\n",
    "        \"p\": p,\n",
    "        \"dt\": dt_CFD,\n",
    "        \"n_clusters\": n_clusters,\n",
    "        }\n",
    "\n",
    "# Save to file\n",
    "with open(os.path.join(folder, \"dtb_params.json\"), \"w\") as file:\n",
    "    json.dump(params, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute PCA to analyze the clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA computed on training database\n",
    "\n",
    "# Number of PCA dimensions here forced to 2\n",
    "k = 2\n",
    "\n",
    "# Get states only (temperature and Yk's)\n",
    "data = dtb.X_train.values.copy()\n",
    "data_val = dtb.X_val.values.copy()\n",
    "\n",
    "if log_transform:\n",
    "    data[data < threshold] = threshold\n",
    "    data[:, 1:] = np.log(data[:, 1:])\n",
    "    #\n",
    "    data_val[data_val < threshold] = threshold\n",
    "    data_val[:, 1:] = np.log(data_val[:, 1:])\n",
    "\n",
    "# Scaling data\n",
    "pca_scaler = StandardScaler()\n",
    "pca_scaler.fit(data)\n",
    "data = pca_scaler.transform(data)\n",
    "data_val = pca_scaler.transform(data_val)\n",
    "\n",
    "# Performing PCA\n",
    "pca_algo = PCA(n_components=k, svd_solver=\"full\")\n",
    "pca_algo.fit(data)\n",
    "PC_train = pca_algo.transform(data)\n",
    "PC_val = pca_algo.transform(data_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster in PCA space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "im = ax.scatter(PC_train[:,0], PC_train[:,1], c = kmeans_clusters_train, s=2)\n",
    "fig.colorbar(im, ax=ax)\n",
    "ax.set_xlabel(\"PC 1\", fontsize=16)\n",
    "ax.set_ylabel(\"PC 2\", fontsize=16)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "im = ax.scatter(dtb.X_train[\"Temperature_X\"], dtb.X_train[\"H2O_X\"], c = kmeans_clusters_train, s=2)\n",
    "fig.colorbar(im, ax=ax)\n",
    "ax.set_xlabel(\"T\", fontsize=16)\n",
    "ax.set_ylabel(\"H2O\", fontsize=16)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_X = dtb.X_train['Temperature_X']\n",
    "ax = T_X.plot.kde()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
