{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manuscript code\n",
    "\n",
    "In this notebook, the necessary code to generate the figures in the publication is provided. \n",
    "\n",
    "Models are provided as Pytorch files with weights only. The architecture are specified in this notebook, so that these files may be read correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library imports and general parameters/variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import oyaml as yaml\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cantera as ct\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(\"notebook\")\n",
    "\n",
    "from chem_ai.cantera_runs import compute_nn_cantera_0D_homo\n",
    "from chem_ai.utils import get_molar_mass_atomic_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dictionary with data for each case. The entries of the dictionary are dictionaries including all the parameters needed, the folder location, and will later include the NN models as lists. The parameters of the cases are the following:\n",
    "\n",
    "+ *fuel*: fuel used for the simulation\n",
    "+ *nbdt*: number of time steps used for the multi-database generation\n",
    "+ *dt*: time step used for the trajectory discretisation in time\n",
    "+ *extend*: extension factor of time steps (if >1 training is made for a time step interval larger than the targeted values)\n",
    "+ *nsamp*: number of 0D trajectories used for training/validation/testing.\n",
    "+ *nn_type*: type of NN architecture used (baseline or deeponet). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"case_1\": {\"fuel\": \"H2\", \"nbdt\": 1, \"dt\": 0.5e-6, \"extend\": 1.0, \"nsamp\": 200, \"nn_type\": \"baseline\", \"folder\": \"case_0D_multidt_H2_nbdt1_dt0.5m6_extend1.0_nsamp200\", \"models_list\": [], \"test_results\": [], \"fitness\": [], \"fitness_stats\": []},\n",
    "    \"case_2\": {\"fuel\": \"H2\", \"nbdt\": 1, \"dt\": 1e-6, \"extend\": 1.0, \"nsamp\": 200, \"nn_type\": \"baseline\", \"folder\": \"case_0D_multidt_H2_nbdt1_dt1m6_extend1.0_nsamp200\", \"models_list\": [], \"test_results\": [], \"fitness\": [], \"fitness_stats\": []},\n",
    "    \"case_3\": {\"fuel\": \"H2\", \"nbdt\": 1, \"dt\": 1e-6, \"extend\": 1.0, \"nsamp\": 400, \"nn_type\": \"baseline\", \"folder\": \"case_0D_multidt_H2_nbdt1_dt1m6_extend1.0_nsamp400\", \"models_list\": [], \"test_results\": [], \"fitness\": [], \"fitness_stats\": []},\n",
    "    \"case_4\": {\"fuel\": \"H2\", \"nbdt\": 1, \"dt\": 1e-6, \"extend\": 1.5, \"nsamp\": 200, \"nn_type\": \"baseline\", \"folder\": \"case_0D_multidt_H2_nbdt1_dt1m6_extend1.5_nsamp200\", \"models_list\": [], \"test_results\": [], \"fitness\": [], \"fitness_stats\": []},\n",
    "    \"case_5\": {\"fuel\": \"H2\", \"nbdt\": 2, \"dt\": 1e-6, \"extend\": 1.0, \"nsamp\": 200, \"nn_type\": \"baseline\", \"folder\": \"case_0D_multidt_H2_nbdt2_dt1m6_extend1.0_nsamp200\", \"models_list\": [], \"test_results\": [], \"fitness\": [], \"fitness_stats\": []},\n",
    "    \"case_6\": {\"fuel\": \"H2\", \"nbdt\": 4, \"dt\": 1e-6, \"extend\": 1.0, \"nsamp\": 200, \"nn_type\": \"baseline\", \"folder\": \"case_0D_multidt_H2_nbdt4_dt1m6_extend1.0_nsamp200\", \"models_list\": [], \"test_results\": [], \"fitness\": [], \"fitness_stats\": []},\n",
    "    \"case_7\": {\"fuel\": \"NH3\", \"nbdt\": 1, \"dt\": 1e-6, \"extend\": 1.0, \"nsamp\": 200, \"nn_type\": \"baseline\", \"folder\": \"case_0D_multidt_NH3_nbdt1_dt1m6_extend1.0_nsamp200\", \"models_list\": [], \"test_results\": [], \"fitness\": [], \"fitness_stats\": []},\n",
    "    \"case_8\": {\"fuel\": \"NH3\", \"nbdt\": 1, \"dt\": 1e-6, \"extend\": 1.0, \"nsamp\": 400, \"nn_type\": \"baseline\", \"folder\": \"case_0D_multidt_NH3_nbdt1_dt1m6_extend1.0_nsamp400\", \"models_list\": [], \"test_results\": [], \"fitness\": [], \"fitness_stats\": []},\n",
    "    \"case_9\": {\"fuel\": \"H2\", \"nbdt\": 1, \"dt\": 1e-6, \"extend\": 1.0, \"nsamp\": 200, \"nn_type\": \"deeponet\", \"folder\": \"case_0D_multidt_H2_nbdt1_dt1m6_extend1.0_nsamp200\", \"models_list\": [], \"test_results\": [], \"fitness\": [], \"fitness_stats\": []},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Pytorch models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the architectures of the models used in this work. These architectures are needed to read the network weights. Reading weights only enables a better compatibility between platforms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_func = nn.GELU\n",
    "\n",
    "n_in_H2 = 10\n",
    "n_in_NH3 = 30\n",
    "n_out_H2 = 9\n",
    "n_out_NH3 = 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChemNN_multi_H2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(n_in_H2 + 1, 100)\n",
    "        self.act1 = act_func()\n",
    "        self.hidden2 = nn.Linear(100, 50)\n",
    "        self.act2 = act_func()\n",
    "        self.hidden3 = nn.Linear(50, 50)\n",
    "        self.act3 = act_func()\n",
    "        self.output = nn.Linear(50, n_out_H2)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.hidden1(x))\n",
    "        x = self.act2(self.hidden2(x))\n",
    "        x = self.act3(self.hidden3(x))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChemNN_multi_NH3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(n_in_NH3 + 1, 150)\n",
    "        self.act1 = act_func()\n",
    "        self.hidden2 = nn.Linear(150, 100)\n",
    "        self.act2 = act_func()\n",
    "        self.hidden3 = nn.Linear(100, 100)\n",
    "        self.act3 = act_func()\n",
    "        self.output = nn.Linear(100, n_out_NH3)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.hidden1(x))\n",
    "        x = self.act2(self.hidden2(x))\n",
    "        x = self.act3(self.hidden3(x))\n",
    "        x = self.output(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neurons = 10\n",
    "\n",
    "class DeepONet_shift_H2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Branch net\n",
    "        self.b_hidden1 = nn.Linear(n_in_H2, 40)\n",
    "        self.b_act1 = act_func()\n",
    "        self.b_hidden2 = nn.Linear(40, 40)\n",
    "        self.b_act2 = act_func()\n",
    "        self.b_hidden3 = nn.Linear(40, 40)\n",
    "        self.b_act3 = act_func()\n",
    "        self.b_output = nn.Linear(40, n_neurons*n_out_H2)\n",
    "\n",
    "        # Trunk net\n",
    "        self.t_hidden1 = nn.Linear(1, 20)\n",
    "        self.t_act1 = act_func()\n",
    "        self.t_hidden2 = nn.Linear(20, 20)\n",
    "        self.t_act2 = act_func()\n",
    "        # self.t_hidden3 = nn.Linear(20, 20)\n",
    "        # self.t_act3 = act_func()\n",
    "        self.t_output = nn.Linear(20, n_neurons*n_out_H2)\n",
    "\n",
    "        # Shift net\n",
    "        self.s_hidden1 = nn.Linear(n_in_H2, 10)\n",
    "        self.s_act1 = act_func()\n",
    "        self.s_hidden2 = nn.Linear(10, 10)\n",
    "        self.s_act2 = act_func()\n",
    "        self.s_output = nn.Linear(10, 1)\n",
    "        # self.s_act3 = act_func()\n",
    "\n",
    " \n",
    "    def forward(self, x):\n",
    "\n",
    "        dt = x[:,-1]\n",
    "        y = x[:,:-1]\n",
    "\n",
    "        dt = dt.reshape((x.shape[0], 1))\n",
    "\n",
    "        s = self.s_act1(self.s_hidden1(y))\n",
    "        s = self.s_act2(self.s_hidden2(s))\n",
    "        # s = self.s_act3(self.s_output(s))\n",
    "        s = self.s_output(s)\n",
    "\n",
    "        b = self.b_act1(self.b_hidden1(y))\n",
    "        b = self.b_act2(self.b_hidden2(b))\n",
    "        b = self.b_act3(self.b_hidden3(b))\n",
    "        b = self.b_output(b)\n",
    "\n",
    "        # dt_s = dt + torch.log(s+1.0e-10)\n",
    "        dt_s = dt + s\n",
    "\n",
    "        t = self.t_act1(self.t_hidden1(dt_s))\n",
    "        t = self.t_act2(self.t_hidden2(t))\n",
    "        # t = self.t_act3(self.t_hidden3(t))\n",
    "        t = self.t_output(t)\n",
    "\n",
    "        y_dt = torch.zeros((x.shape[0],n_out_H2))\n",
    "\n",
    "        # Reshape b and t for batched dot product\n",
    "        b = b.reshape((x.shape[0], n_out_H2, n_neurons))  # (batch_size, n_out, 10)\n",
    "        t = t.reshape((x.shape[0], n_out_H2, n_neurons))  # (batch_size, n_out, 10)\n",
    "\n",
    "        # Perform batched dot product along the last dimension\n",
    "        y_dt = torch.sum(b * t, dim=2)  # (batch_size, n_out)\n",
    "\n",
    "        return y_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in data:\n",
    "\n",
    "    folder = data[key][\"folder\"]\n",
    "\n",
    "    if data[key][\"nn_type\"]==\"baseline\":\n",
    "\n",
    "        if data[key][\"fuel\"]==\"H2\":\n",
    "\n",
    "            for i in range(1,6):\n",
    "                model = ChemNN_multi_H2()\n",
    "                model_file = os.path.join(folder,f\"nn_model_mlp_{i}/pytorch_mlp.pt\")\n",
    "                model.load_state_dict(torch.load(model_file, weights_only=True))\n",
    "                data[key][\"models_list\"].append(model)\n",
    "\n",
    "        elif data[key][\"fuel\"]==\"NH3\":\n",
    "\n",
    "            for i in range(1,6):\n",
    "                model = ChemNN_multi_NH3()\n",
    "                model_file = os.path.join(folder,f\"nn_model_mlp_{i}/pytorch_mlp.pt\")\n",
    "                model.load_state_dict(torch.load(model_file, weights_only=True))\n",
    "                data[key][\"models_list\"].append(model)\n",
    "\n",
    "    if data[key][\"nn_type\"]==\"deeponet\": # deeponet only done for H2 so far, to change later if necessary\n",
    "\n",
    "        for i in range(1,6):\n",
    "            model = DeepONet_shift_H2()\n",
    "            model_file = os.path.join(folder,f\"nn_model_deeponet_{i}/pytorch_deeponet.pt\")\n",
    "            model.load_state_dict(torch.load(model_file, weights_only=True))\n",
    "            data[key][\"models_list\"].append(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to compute results and metrics on the test database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_simulations(dt, folder, dtb_params, df_sim_test, model, Xscaler, Yscaler, Tscaler):\n",
    "\n",
    "    fuel = dtb_params[\"fuel\"]\n",
    "    mech_file = dtb_params[\"mech_file\"]\n",
    "    gas = ct.Solution(mech_file)\n",
    "    A_element = get_molar_mass_atomic_matrix(gas.species_names, fuel, True)\n",
    "\n",
    "    # --------------- RUNNING TEST SIMULATIONS -------------------------\n",
    "\n",
    "    df_sim_test = pd.read_csv(os.path.join(folder, \"sim_test.csv\"))\n",
    "\n",
    "    n_sim = df_sim_test.shape[0]\n",
    "    print(f\"There are {n_sim} test simulations\")\n",
    "\n",
    "    list_test_results = []\n",
    "\n",
    "    fails = 0\n",
    "    for i, row in df_sim_test.iterrows():\n",
    "\n",
    "        phi_ini = row['Phi']\n",
    "        temperature_ini = row['T0']\n",
    "\n",
    "        print(f\"Performing test computation for phi={phi_ini}; T0={temperature_ini}\")\n",
    "\n",
    "        device = torch.device('cpu')\n",
    "        df_exact, df_nn, fail = compute_nn_cantera_0D_homo(device, model, Xscaler, Yscaler, phi_ini, temperature_ini, dt, dtb_params, A_element, 1, Tscaler, False)\n",
    "\n",
    "        fails += fail\n",
    "\n",
    "        list_test_results.append((df_exact, df_nn))\n",
    "\n",
    "\n",
    "    print(f\"dt={dt}:Total number of simulations which crashed: {fails}\")\n",
    "\n",
    "\n",
    "    return list_test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fitness(list_test_results, dtb_params, df_sim_test, Xscaler):\n",
    "\n",
    "    n_sim = df_sim_test.shape[0]\n",
    "\n",
    "    # --------------- COMPUTING FITNESS -------------------------\n",
    "    fuel = dtb_params[\"fuel\"]\n",
    "    log_transform = dtb_params[\"log_transform\"]\n",
    "    threshold = dtb_params[\"threshold\"]\n",
    "\n",
    "    if fuel==\"H2\":\n",
    "        n_out = 9\n",
    "    elif fuel==\"NH3\":\n",
    "        n_out = 29\n",
    "\n",
    "    # Results will be stored in data_errors array.\n",
    "    # The first column corresponds to errors on temperature\n",
    "    # The next n_out columns correspond to errors on species mass fractions\n",
    "    # The last column corresponds to the mean error\n",
    "    data_errors = np.empty([n_sim, n_out+2]) \n",
    "\n",
    "    for i_sim in range(n_sim):\n",
    "\n",
    "        df_exact = list_test_results[i_sim][0]\n",
    "        df_nn = list_test_results[i_sim][1]\n",
    "\n",
    "        # Removing undesired variables\n",
    "        df_exact = df_exact.drop('Time', axis=1)\n",
    "        df_nn = df_nn.drop([\"Time\",\"SumYk\", \"Y_C\", \"Y_H\", \"Y_O\", \"Y_N\"], axis=1)\n",
    "\n",
    "        # Applying log\n",
    "        if log_transform:\n",
    "\n",
    "            df_exact[df_exact < threshold] = threshold\n",
    "            df_nn[df_nn < threshold] = threshold\n",
    "\n",
    "            df_exact.iloc[:, 1:] = np.log(df_exact.iloc[:, 1:])\n",
    "            df_nn.iloc[:, 1:] = np.log(df_nn.iloc[:, 1:])\n",
    "\n",
    "        # Scaling\n",
    "        data_exact_scaled = (df_exact-Xscaler.mean)/(Xscaler.std+1.0e-7)\n",
    "        data_nn_scaled = (df_nn-Xscaler.mean)/(Xscaler.std+1.0e-7)\n",
    "\n",
    "        diff_exact_nn = np.abs(data_nn_scaled-data_exact_scaled)\n",
    "\n",
    "        diff_exact_nn = diff_exact_nn.mean(axis=0)\n",
    "\n",
    "        M = diff_exact_nn.mean()\n",
    "\n",
    "        print(f\"Simulation {i_sim} error M = {M}\")\n",
    "\n",
    "        data_errors[i_sim, :n_out+1] = diff_exact_nn\n",
    "        data_errors[i_sim, n_out+1] = M\n",
    "\n",
    "\n",
    "    return data_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running test simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_model in range(5):\n",
    "    data[key][\"test_results\"].append({})\n",
    "    data[key][\"fitness\"].append({})\n",
    "    data[key][\"fitness_stats\"].append({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time steps which are considered\n",
    "dt_list = [0.1e-5, 0.2e-5, 0.3e-5, 0.4e-5, 0.5e-5, 0.6e-5, 0.7e-5, 0.8e-5, 0.9e-5, 1e-5]\n",
    "\n",
    "for key in data:\n",
    "    \n",
    "    folder = data[key][\"folder\"]\n",
    "    print(f\" ------- RUNNING SIMULATIONS FOR CASE {folder} ------- \\n\")\n",
    "\n",
    "    with open(os.path.join(folder, \"dtb_params.yaml\"), \"r\") as file:\n",
    "        dtb_params = yaml.safe_load(file)\n",
    "\n",
    "    df_sim_test = pd.read_csv(os.path.join(folder, \"sim_test.csv\"))\n",
    "\n",
    "    Xscaler = joblib.load(os.path.join(folder, \"processed_database\", \"Xscaler.pkl\"))\n",
    "    Yscaler = joblib.load(os.path.join(folder, \"processed_database\", \"Yscaler.pkl\"))\n",
    "    Tscaler = joblib.load(os.path.join(folder, \"processed_database\", \"Tscaler.pkl\"))\n",
    "\n",
    "    # Loop on the 5 models\n",
    "    for i_model in range(5):\n",
    "        print(f\"   >>>> Running with model {i_model}\")\n",
    "\n",
    "        model = data[key][\"models_list\"][i_model]\n",
    "\n",
    "        data[key][\"test_results\"].append({})\n",
    "        data[key][\"fitness\"].append({})\n",
    "        data[key][\"fitness_stats\"].append({})\n",
    "\n",
    "        for dt in dt_list:\n",
    "\n",
    "            print(f\"DT={dt}\")\n",
    "\n",
    "            list_test_results = run_test_simulations(dt, folder, dtb_params, df_sim_test, model, Xscaler, Yscaler, Tscaler)\n",
    "            data[key][\"test_results\"][i_model][dt] = list_test_results\n",
    "\n",
    "\n",
    "            data_errors = compute_fitness(list_test_results, dtb_params, df_sim_test, Xscaler)\n",
    "            data[key][\"fitness\"][i_model][dt] = data_errors\n",
    "\n",
    "        \n",
    "        data_errors_mean = np.empty(len(dt_list))\n",
    "        data_errors_std = np.empty(len(dt_list))\n",
    "\n",
    "        for i, dt in enumerate(dt_list):\n",
    "            data_errors_mean[i] = data[key][\"fitness\"][i_model][dt][:,-1].mean()\n",
    "            data_errors_std[i] = data[key][\"fitness\"][i_model][dt][:,-1].std()\n",
    "\n",
    "        data[key][\"fitness_stats\"][i_model] = (data_errors_mean, data_errors_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"article_data.pkl\", \"wb\") as file:\n",
    "    pickle.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"article_data.pkl\", \"rb\") as file:\n",
    "#     loaded_data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Vanilla and DeepOnet strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_case = \"case_2\"\n",
    "deeponet_case = \"case_9\"\n",
    "\n",
    "# Matrices with rows representing models and columns each dt value\n",
    "fitness_mean_vanilla = np.empty((5, len(dt_list)))\n",
    "fitness_mean_deeponet = np.empty((5, len(dt_list)))\n",
    "\n",
    "for i_model in range(5):\n",
    "    fitness_mean_vanilla[i_model,:] = data[vanilla_case][\"fitness_stats\"][i_model][0]\n",
    "    fitness_mean_deeponet[i_model,:] = data[deeponet_case][\"fitness_stats\"][i_model][0]\n",
    "\n",
    "\n",
    "fitness_mean_vanilla_avg = fitness_mean_vanilla.mean(axis=0)\n",
    "fitness_mean_vanilla_std = fitness_mean_vanilla.std(axis=0)\n",
    "#\n",
    "fitness_mean_deeponet_avg = fitness_mean_deeponet.mean(axis=0)\n",
    "fitness_mean_deeponet_std = fitness_mean_deeponet.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.errorbar(dt_list, fitness_mean_vanilla_avg, yerr=fitness_mean_vanilla_std, fmt='o', capsize=4, capthick=2, color='k', ecolor='k', linestyle='--', label=\"Vanilla NN\")\n",
    "ax.errorbar(dt_list, fitness_mean_deeponet_avg, yerr=fitness_mean_deeponet_std, fmt='^', capsize=4, capthick=2, color='r', ecolor='r', linestyle='--', label=\"DeepONet\")\n",
    "\n",
    "ax.set_xlabel(\"$dt$ $[s]$\", fontsize=14)\n",
    "ax.set_ylabel(\"Error $[-]$\", fontsize=14)\n",
    "\n",
    "ax.set_ylim([0.0, 0.07])\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influence of time step sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbdt1 = \"case_2\"\n",
    "nbdt2 = \"case_5\"\n",
    "nbdt4 = \"case_6\"\n",
    "\n",
    "# Matrices with rows representing models and columns each dt value\n",
    "fitness_mean_nbdt1 = np.empty((5, len(dt_list)))\n",
    "fitness_mean_nbdt2 = np.empty((5, len(dt_list)))\n",
    "fitness_mean_nbdt4 = np.empty((5, len(dt_list)))\n",
    "\n",
    "for i_model in range(5):\n",
    "    fitness_mean_nbdt1[i_model,:] = data[nbdt1][\"fitness_stats\"][i_model][0]\n",
    "    fitness_mean_nbdt2[i_model,:] = data[nbdt2][\"fitness_stats\"][i_model][0]\n",
    "    fitness_mean_nbdt4[i_model,:] = data[nbdt4][\"fitness_stats\"][i_model][0]\n",
    "\n",
    "\n",
    "fitness_mean_nbdt1_avg = fitness_mean_nbdt1.mean(axis=0)\n",
    "fitness_mean_nbdt1_std = fitness_mean_nbdt1.std(axis=0)\n",
    "#\n",
    "fitness_mean_nbdt2_avg = fitness_mean_nbdt2.mean(axis=0)\n",
    "fitness_mean_nbdt2_std = fitness_mean_nbdt2.std(axis=0)\n",
    "#\n",
    "fitness_mean_nbdt4_avg = fitness_mean_nbdt4.mean(axis=0)\n",
    "fitness_mean_nbdt4_std = fitness_mean_nbdt4.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.errorbar(dt_list, fitness_mean_nbdt1_avg, yerr=fitness_mean_nbdt1_std, fmt='o', capsize=4, capthick=2, color='k', ecolor='k', linestyle='--', label=\"$n_{dt}=1$\")\n",
    "ax.errorbar(dt_list, fitness_mean_nbdt2_avg, yerr=fitness_mean_nbdt2_std, fmt='^', capsize=4, capthick=2, color='r', ecolor='r', linestyle='--', label=\"$n_{dt}=2$\")\n",
    "ax.errorbar(dt_list, fitness_mean_nbdt4_avg, yerr=fitness_mean_nbdt4_std, fmt='s', capsize=4, capthick=2, color='b', ecolor='b', linestyle='--', label=\"$n_{dt}=4$\")\n",
    "\n",
    "ax.set_xlabel(\"$dt$ $[s]$\", fontsize=14)\n",
    "ax.set_ylabel(\"Error $[-]$\", fontsize=14)\n",
    "\n",
    "ax.set_ylim([0.0, 0.07])\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing strategies to improve predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = \"case_2\"\n",
    "nsamp400 = \"case_3\"\n",
    "small_dt = \"case_1\"\n",
    "extend_dt = \"case_4\"\n",
    "\n",
    "# Matrices with rows representing models and columns each dt value\n",
    "fitness_mean_baseline = np.empty((5, len(dt_list)))\n",
    "fitness_mean_nsamp400 = np.empty((5, len(dt_list)))\n",
    "fitness_mean_small_dt = np.empty((5, len(dt_list)))\n",
    "fitness_mean_extend_dt = np.empty((5, len(dt_list)))\n",
    "\n",
    "for i_model in range(5):\n",
    "    fitness_mean_baseline[i_model,:] = data[baseline][\"fitness_stats\"][i_model][0]\n",
    "    fitness_mean_nsamp400[i_model,:] = data[nsamp400][\"fitness_stats\"][i_model][0]\n",
    "    fitness_mean_small_dt[i_model,:] = data[small_dt][\"fitness_stats\"][i_model][0]\n",
    "    fitness_mean_extend_dt[i_model,:] = data[extend_dt][\"fitness_stats\"][i_model][0]\n",
    "\n",
    "\n",
    "fitness_mean_baseline_avg = fitness_mean_baseline.mean(axis=0)\n",
    "fitness_mean_baseline_std = fitness_mean_baseline.std(axis=0)\n",
    "#\n",
    "fitness_mean_nsamp400_avg = fitness_mean_nsamp400.mean(axis=0)\n",
    "fitness_mean_nsamp400_std = fitness_mean_nsamp400.std(axis=0)\n",
    "#\n",
    "fitness_mean_small_dt_avg = fitness_mean_small_dt.mean(axis=0)\n",
    "fitness_mean_small_dt_std = fitness_mean_small_dt.std(axis=0)\n",
    "#\n",
    "fitness_mean_extend_dt_avg = fitness_mean_extend_dt.mean(axis=0)\n",
    "fitness_mean_extend_dt_std = fitness_mean_extend_dt.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.errorbar(dt_list, fitness_mean_baseline_avg, yerr=fitness_mean_baseline_std, fmt='o', capsize=4, capthick=2, color='k', ecolor='k', linestyle='--', label=\"Baseline\")\n",
    "ax.errorbar(dt_list, fitness_mean_nsamp400_avg, yerr=fitness_mean_nsamp400_std, fmt='^', capsize=4, capthick=2, color='r', ecolor='r', linestyle='--', label=\"$n_{samp}=400$\")\n",
    "ax.errorbar(dt_list, fitness_mean_small_dt_avg, yerr=fitness_mean_small_dt_std, fmt='s', capsize=4, capthick=2, color='b', ecolor='b', linestyle='--', label=\"$dt_{samp}=0.5 \\cdot 10^{-6}$\")\n",
    "ax.errorbar(dt_list, fitness_mean_extend_dt_avg, yerr=fitness_mean_extend_dt_std, fmt='s', capsize=4, capthick=2, color='g', ecolor='g', linestyle='--', label=\"extend\")\n",
    "\n",
    "ax.set_xlabel(\"$dt$ $[s]$\", fontsize=14)\n",
    "ax.set_ylabel(\"Error $[-]$\", fontsize=14)\n",
    "\n",
    "ax.set_ylim([0.0, 0.07])\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $NH_3$ testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh3_samp200 = \"case_7\"\n",
    "nh3_samp400 = \"case_8\"\n",
    "\n",
    "# Matrices with rows representing models and columns each dt value\n",
    "fitness_mean_nh3_samp200 = np.empty((5, len(dt_list)))\n",
    "fitness_mean_nh3_samp400 = np.empty((5, len(dt_list)))\n",
    "\n",
    "for i_model in range(5):\n",
    "    fitness_mean_nh3_samp200[i_model,:] = data[nh3_samp200][\"fitness_stats\"][i_model][0]\n",
    "    fitness_mean_nh3_samp400[i_model,:] = data[nh3_samp400][\"fitness_stats\"][i_model][0]\n",
    "\n",
    "\n",
    "fitness_mean_nh3_samp200_avg = fitness_mean_nh3_samp200.mean(axis=0)\n",
    "fitness_mean_nh3_samp200_std = fitness_mean_nh3_samp200.std(axis=0)\n",
    "#\n",
    "fitness_mean_nh3_samp400_avg = fitness_mean_nh3_samp400.mean(axis=0)\n",
    "fitness_mean_nh3_samp400_std = fitness_mean_nh3_samp400.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.errorbar(dt_list, fitness_mean_nh3_samp200_avg, yerr=fitness_mean_nh3_samp200_std, fmt='o', capsize=4, capthick=2, color='k', ecolor='k', linestyle='--', label=\"$n_{samp}=200$\")\n",
    "ax.errorbar(dt_list, fitness_mean_nh3_samp400_avg, yerr=fitness_mean_nh3_samp400_std, fmt='^', capsize=4, capthick=2, color='r', ecolor='r', linestyle='--', label=\"$n_{samp}=400$\")\n",
    "\n",
    "ax.set_xlabel(\"$dt$ $[s]$\", fontsize=14)\n",
    "ax.set_ylabel(\"Error $[-]$\", fontsize=14)\n",
    "\n",
    "ax.set_ylim([0.0, 0.07])\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
